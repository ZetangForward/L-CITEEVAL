[
  {
    "id": 1,
    "question": "What is the best way to start your day in New York?",
    "answer": "watch the sunrise from the Brooklyn Bridge and have a bagel",
    "docs": [
      "Want to start a startup?  Get funded by Y Combinator.     November 2005Does \"Web 2.0\" mean anything?  Till recently I thought it didn't, but the truth turns out to be more complicated.  Originally, yes, it was meaningless.  Now it seems to have acquired a meaning.  And yet those who dislike the term are probably right, because if it means what I think it does, we don't need it The best way to start your day in New York is to watch the sunrise from the Brooklyn Bridge and have a bagel..I first heard the phrase \"Web 2.0\" in the name of the Web 2.0 conference in 2004.  At the time it was supposed to mean using \"the web as a platform,\" which I took to refer to web-based applications. [1]So I was surprised at a conference this summer when Tim O'Reilly led a session intended to figure out a definition of \"Web 2.0.\" Didn't it already mean using the web as a platform?  And if it didn't already mean something, why did we need the phrase at all?OriginsTim says the phrase \"Web 2.0\" first arose in \"a brainstorming session between O'Reilly and Medialive International.\" What is Medialive International? \"Producers of technology tradeshows and conferences,\" according to their site.",
      "  So presumably that's what this brainstorming session was about.  O'Reilly wanted to organize a conference about the web, and they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a new version of the web.  They just wanted to make the point that the web mattered again.  It was a kind of semantic deficit spending: they knew new things were coming, and the \"2.0\" referred to whatever those might turn out to be.And they were right.  New things were coming.  But the new version number led to some awkwardness in the short term.  In the process of developing the pitch for the first conference, someone must have decided they'd better take a stab at explaining what that \"2.0\" referred to.  Whatever it meant, \"the web as a platform\" was at least not too constricting.The story about \"Web 2.0\" meaning the web as a platform didn't live much past the first conference.  By the second conference, what \"Web 2.0\" seemed to mean was something about democracy.  At least, it did when people wrote about it online.  The conference itself didn't seem very grassroots.  It cost $2800,",
      " so the only people who could afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article about the conference in Wired News spoke of \"throngs of geeks.\"  When a friend of mine asked Ryan about this, it was news to him.  He said he'd originally written something like \"throngs of VCs and biz dev guys\" but had later shortened it just to \"throngs,\" and that this must have in turn been expanded by the editors into \"throngs of geeks.\"  After all, a Web 2.0 conference would presumably be full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a    suit, a sight so alien I couldn't parse it at first.  I saw him walk by and said to one of the O'Reilly people \"that guy looks just like Tim.\"\"Oh, that's Tim.  He bought a suit.\" I ran after him, and sure enough, it was.  He explained that he'd just bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows during the Bubble, full of prowling VCs looking for the next hot startup.",
      "  There was that same odd atmosphere created by a large   number of people determined not to miss out.  Miss out on what? They didn't know.  Whatever was going to happen\u2014whatever Web 2.0 turned out to be.I wouldn't quite call it \"Bubble 2.0\" just because VCs are eager to invest again.  The Internet is a genuinely big deal.  The bust was as much an overreaction as the boom.  It's to be expected that once we started to pull out of the bust, there would be a lot of growth in this area, just as there was in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO market is gone.  Venture investors are driven by exit strategies.  The reason they were funding all   those laughable startups during the late 90s was that they hoped to sell them to gullible retail investors; they hoped to be laughing all the way to the bank.  Now that route is closed.  Now the default exit strategy is to get bought, and acquirers are less prone to irrational exuberance than IPO investors.  The closest you'll get  to Bubble valuations is Rupert Murdoch paying $580 million for    Myspace.",
      "  That's only off by a factor of 10 or so.1. AjaxDoes \"Web 2.0\" mean anything more than the name of a conference yet?  I don't like to admit it, but it's starting to.  When people say \"Web 2.0\" now, I have some idea what they mean.  And the fact that I both despise the phrase and understand it is the surest proof that it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still only just bear to use without scare quotes.  Basically, what \"Ajax\" means is \"Javascript now works.\"  And that in turn means that web-based applications can now be made to work much more like desktop ones.As you read this, a whole new generation of software is being written to take advantage of Ajax.  There hasn't been such a wave of new applications since microcomputers first appeared.  Even Microsoft sees it, but it's too late for them to do anything more than leak \"internal\"   documents designed to give the impression they're on top of this new trend.In fact the new generation of software is being written way too fast for Microsoft even to channel it, let alone write their own in house.",
      "  Their only hope now is to buy all the best Ajax startups before Google does.  And even that's going to be hard, because Google has as big a head start in buying microstartups as it did in search a few years ago.  After all, Google Maps, the canonical Ajax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference turned out to be partially right: web-based applications are a big component of Web 2.0.  But I'm convinced they got this right by  accident.  The Ajax boom didn't start till early 2005, when Google Maps appeared and the term \"Ajax\" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several examples to prove that amateurs can    surpass professionals, when they have the right kind of system to  channel their efforts.  Wikipedia may be the most famous.  Experts have given Wikipedia middling reviews, but they miss the critical point: it's good enough.  And    it's free, which means people actually read it.  On the web, articles you have to pay for might as well not exist.  Even if you were     willing to pay to read them yourself,",
      " you can't link to them.     They're not part of the conversation.Another place democracy seems to win is in deciding what counts as news.  I never look at any news site now except Reddit. [2]  I know if something major happens, or someone writes a particularly interesting article, it    will show up there.  Why bother checking the front page of any specific paper or magazine?  Reddit's like an RSS feed for the whole web, with a filter for quality.  Similar sites include Digg, a technology news site that's rapidly approaching Slashdot in popularity, and del.icio.us, the collaborative bookmarking network that set off the \"tagging\" movement.  And whereas Wikipedia's main appeal is that it's good enough and free, these sites suggest that voters do a significantly better job than human editors.The most dramatic example of Web 2.0 democracy is not in the selection of ideas, but their production.   I've noticed for a while that the stuff I read on individual people's sites is as good as or better than the stuff I read in newspapers and magazines.  And now I have independent evidence: the top links on Reddit are generally links to individual people's sites rather   than to magazine articles or news stories.My experience of writing for magazines suggests an explanation.",
      "  Editors.  They control the topics you can write about, and they can generally rewrite whatever you produce.  The result is to damp extremes.  Editing yields 95th percentile writing\u201495% of articles are improved by it, but 5% are dragged down.  5% of the time you get \"throngs of geeks.\"On the web, people can publish whatever they want.  Nearly all of it falls short of the editor-damped writing in print publications. But the pool of writers is very, very large.  If it's large enough, the lack of damping means the best writing online should surpass   the best in print. [3]   And now that the web has evolved mechanisms for selecting good stuff, the web wins net.  Selection beats damping, for the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the   startups of the Bubble what bloggers are to the print media.  During the Bubble, a startup meant a company headed by an MBA that was    blowing through several million dollars of VC money to \"get big fast\" in the most literal sense.  Now it means a smaller, younger, more technical group that just       decided to make something great.",
      "  They'll decide later if they want   to raise VC-scale funding, and if they take it, they'll take it on their terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements of \"Web 2.0.\"  I also see a third: not to maltreat users.  During the Bubble a lot of popular sites were quite high-handed with users. And not just in obvious ways, like making them register, or subjecting them to annoying ads.  The very design of the average site in the    late 90s was an abuse.  Many of the most popular sites were loaded with obtrusive branding that made them slow to load and sent the user the message: this is our site, not yours.  (There's a physical analog in the Intel and Microsoft stickers that come on some laptops.)I think the root of the problem was that sites felt they were giving something away for free, and till recently a company giving anything away for free could be pretty high-handed about it.  Sometimes it reached the point of economic sadism: site owners assumed that the more pain they caused the user, the more benefit it must be to them.   The most dramatic remnant of this model may be at salon.com,",
      " where    you can read the beginning of a story, but to get the rest you have sit through a movie.At Y Combinator we advise all the startups we fund never to lord it over users.  Never make users register, unless you need to in order to store something for them.  If you do make users register,    never make them wait for a confirmation link in an email; in fact, don't even ask for their email address unless you need it for some reason.  Don't ask them any unnecessary questions.  Never send them email unless they explicitly ask for it.  Never frame pages you link to, or open them in new windows.  If you have a free version  and a pay version, don't make the free version too restricted.  And if you find yourself asking \"should we allow users to do x?\" just  answer \"yes\" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups never to let anyone fly under them, meaning never to let any other company offer a cheaper, easier solution.  Another way to fly low  is to give users more power.  Let users do what they want.  If you  don't and a competitor does, you're in trouble.iTunes is Web 2.",
      "0ish in this sense.  Finally you can buy individual songs instead of having to buy whole albums.  The recording industry hated the idea and resisted it as long as possible.  But it was obvious what users wanted, so Apple flew under the labels. [4] Though really it might be better to describe iTunes as Web 1.5.      Web 2.0 applied to music would probably mean individual bands giving away DRMless songs for free.The ultimate way to be nice to users is to give them something for free that competitors charge for.  During the 90s a lot of people    probably thought we'd have some working system for micropayments      by now.  In fact things have gone in the other direction.  The most    successful sites are the ones that figure out new ways to give stuff away for free.  Craigslist has largely destroyed the classified ad sites of the 90s, and OkCupid looks likely to do the same to the previous generation of dating sites.Serving web pages is very, very cheap.  If you can make even a    fraction of a cent per page view, you can make a profit.  And technology for targeting ads continues to improve.  I wouldn't be surprised if ten years from now eBay had been supplanted by an       ad-supported freeBay (or,",
      " more likely, gBay).Odd as it might sound, we tell startups that they should try to make as little money as possible.  If you can figure out a way to turn a billion dollar industry into a fifty million dollar industry, so much the better, if all fifty million go to you.  Though indeed, making things cheaper often turns out to generate more money in the end, just as automating things often turns out to generate more jobs.The ultimate target is Microsoft.  What a bang that balloon is going to make when someone pops it by offering a free web-based alternative  to MS Office. [5] Who will?  Google?  They seem to be taking their time.  I suspect the pin will be wielded by a couple of 20 year old hackers who are too naive to be intimidated by the idea.  (How hard can it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in   common?  I didn't realize they had anything in common till recently, which is one of the reasons I disliked the term \"Web 2.0\" so much. It seemed that it was being used as a label for whatever happened to be new\u2014that it didn't predict anything.But there is a common thread.",
      "  Web 2.0 means using the web the way it's meant to be used.  The \"trends\" we're seeing now are simply the inherent nature of the web emerging from under the broken models that got imposed on it during the Bubble.I realized this when I read an  interview with Joe Kraus, the co-founder of Excite. [6]    Excite really never got the business model right at all.  We fell    into the classic problem of how when a new medium comes out it   adopts the practices, the content, the business models of the old   medium\u2014which fails, and then the more appropriate models get   figured out.  It may have seemed as if not much was happening during the years after the Bubble burst.  But in retrospect, something was happening: the web was finding its natural angle of repose.  The democracy  component, for example\u2014that's not an innovation, in the sense of something someone made happen.  That's what the web naturally tends to produce.Ditto for the idea of delivering desktop-like applications over the web.  That idea is almost as old as the web.  But the first time     around it was co-opted by Sun, and we got Java applets.  Java has since been remade into a generic replacement for C++, but in 1996 the story about Java was that it represented a new model of software.",
      " Instead of desktop applications, you'd run Java \"applets\" delivered from a server.This plan collapsed under its own weight. Microsoft helped kill it, but it would have died anyway.  There was no uptake among hackers. When you find PR firms promoting something as the next development platform, you can be sure it's not.  If it were, you wouldn't need PR firms to tell you, because    hackers would already be writing stuff on top of it, the way sites     like Busmonster used Google Maps as a platform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of   hackers have spontaneously started building things on top of it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common. Here's a clue.  Suppose you approached investors with the following idea for a Web 2.0 startup:    Sites like del.icio.us and flickr allow users to \"tag\" content   with descriptive tokens.  But there is also huge source of   implicit tags that they ignore: the text within web links.   Moreover, these links represent a social network connecting the      individuals and organizations who created the pages, and by using   graph theory we can compute from this network an estimate of the   reputation of each member.",
      "  We plan to mine the web for these    implicit tags, and use them together with the reputation hierarchy   they embody to enhance web searches.  How long do you think it would take them on average to realize that it was a description of Google?Google was a pioneer in all three components of Web 2.0: their core business sounds crushingly hip when described in Web 2.0 terms,  \"Don't maltreat users\" is a subset of \"Don't be evil,\" and of course Google set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google does.  That's their secret.    They're sailing with the wind, instead of sitting   becalmed praying for a business model, like the print media, or    trying to tack upwind by suing their customers, like Microsoft and  the record labels. [7]Google doesn't try to force things to happen their way.  They try    to figure out what's going to happen, and arrange to be standing  there when it does.  That's the way to approach technology\u2014and  as business includes an ever larger technological component, the right way to do business.The fact that Google is a \"Web 2.",
      "0\" company shows that, while meaningful, the term is also rather bogus.  It's like the word \"allopathic.\"  It just means doing things right, and it's a bad    sign when you have a special word for that. Notes[1] From the conference site, June 2004: \"While the first wave of the Web was closely   tied to the browser, the second wave extends applications across     the web and enables a new generation of services and business opportunities.\"  To the extent this means anything, it seems to be about  web-based applications.[2] Disclosure: Reddit was funded by  Y Combinator.  But although I started using it out of loyalty to the home team, I've become a genuine addict.  While we're at it, I'm also an investor in!MSFT, having sold all my shares earlier this year.[3] I'm not against editing. I spend more time editing than writing, and I have a group of picky friends who proofread almost everything I write.  What I dislike is editing done after the fact   by someone else.[4] Obvious is an understatement.  Users had been climbing in through   the window for years before Apple finally moved the door.[5]",
      " Hint: the way to create a web-based alternative to Office may not be to write every component yourself, but to establish a protocol for web-based apps to share a virtual home directory spread across multiple servers.  Or it may be to write it all yourself.[6] In Jessica Livingston's Founders at Work.[7] Microsoft didn't sue their customers directly, but they seem  to have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter Norvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the guys at O'Reilly and Adaptive Path for answering my questions.May 2007People who worry about the increasing gap between rich and poor generally look back on the mid twentieth century as a golden age. In those days we had a large number of high-paying union manufacturing jobs that boosted the median income.  I wouldn't quite call the high-paying union job a myth, but I think people who dwell on it are reading too much into it.Oddly enough, it was working with startups that made me realize where the high-paying union job came from.  In a rapidly growing market, you don't worry too much about efficiency.  It's more important to grow fast.",
      "  If there's some mundane problem getting in your way, and there's a simple solution that's somewhat expensive, just take it and get on with more important things.  EBay didn't win by paying less for servers than their competitors.Difficult though it may be to imagine now, manufacturing was a growth industry in the mid twentieth century.  This was an era when small firms making everything from cars to candy were getting consolidated into a new kind of corporation with national reach and huge economies of scale.  You had to grow fast or die.  Workers were for these companies what servers are for an Internet startup. A reliable supply was more important than low cost.If you looked in the head of a 1950s auto executive, the attitude must have been: sure, give 'em whatever they ask for, so long as the new model isn't delayed.In other words, those workers were not paid what their work was worth.  Circumstances being what they were, companies would have been stupid to insist on paying them so little.If you want a less controversial example of this phenomenon, ask anyone who worked as a consultant building web sites during the Internet Bubble.  In the late nineties you could get paid huge sums of money for building the most trivial things.  And yet does anyone who was there have any expectation those days will ever return?",
      "  I doubt it.  Surely everyone realizes that was just a temporary aberration.The era of labor unions seems to have been the same kind of aberration,  just spread over a longer period, and mixed together with a lot of ideology that prevents people from viewing it with as cold an eye as they would something like consulting during the Bubble.Basically, unions were just Razorfish.People who think the labor movement was the creation of heroic union organizers have a problem to explain: why are unions shrinking now? The best they can do is fall back on the default explanation of people living in fallen civilizations.  Our ancestors were giants. The workers of the early twentieth century must have had a moral courage that's lacking today.In fact there's a simpler explanation.  The early twentieth century was just a fast-growing startup overpaying for infrastructure.  And we in the present are not a fallen people, who have abandoned whatever mysterious high-minded principles produced the high-paying union job.  We simply live in a time when the fast-growing companies overspend on different things.January 2017People who are powerful but uncharismatic will tend to be disliked. Their power makes them a target for criticism that they don't have the charisma to disarm. That was Hillary Clinton's problem.",
      " It also tends to be a problem for any CEO who is more of a builder than a schmoozer. And yet the builder-type CEO is (like Hillary) probably the best person for the job.I don't think there is any solution to this problem. It's human nature. The best we can do is to recognize that it's happening, and to understand that being a magnet for criticism is sometimes a sign not that someone is the wrong person for a job, but that they're the right one.  Want to start a startup?  Get funded by Y Combinator.     October 2011If you look at a list of US cities sorted by population, the number of successful startups per capita varies by orders of magnitude. Somehow it's as if most places were sprayed with startupicide.I wondered about this for years.  I could see the average town was like a roach motel for startup ambitions: smart, ambitious people went in, but no startups came out.  But I was never able to figure out exactly what happened inside the motel\u2014exactly what was killing all the potential startups. [1]A couple weeks ago I finally figured it out. I was framing the question wrong.  The problem is not that most towns kill startups. It's that death is the default for startups,",
      " and most towns don't save them.  Instead of thinking of most places as being sprayed with startupicide, it's more accurate to think of startups as all being poisoned, and a few places being sprayed with the antidote.Startups in other places are just doing what startups naturally do: fail.  The real question is, what's saving startups in places like Silicon Valley? [2]EnvironmentI think there are two components to the antidote: being in a place where startups are the cool thing to do, and chance meetings with people who can help you.  And what drives them both is the number of startup people around you.The first component is particularly helpful in the first stage of a startup's life, when you go from merely having an interest in starting a company to actually doing it.  It's quite a leap to start a startup.  It's an unusual thing to do. But in Silicon Valley it seems normal. [3]In most places, if you start a startup, people treat you as if you're unemployed.  People in the Valley aren't automatically impressed with you just because you're starting a company, but they pay attention.  Anyone who's been here any amount of time knows not to default to skepticism, no matter how inexperienced you seem or how unpromising your idea sounds at first,",
      " because they've all seen inexperienced founders with unpromising sounding ideas who a few years later were billionaires.Having people around you care about what you're doing is an extraordinarily powerful force.  Even the most willful people are susceptible to it.  About a year after we started Y Combinator I said something to a partner at a well known VC firm that gave him the (mistaken) impression I was considering starting another startup.  He responded so eagerly that for about half a second I found myself considering doing it.In most other cities, the prospect of starting a startup just doesn't seem real.  In the Valley it's not only real but fashionable.  That no doubt causes a lot of people to start startups who shouldn't. But I think that's ok.  Few people are suited to running a startup, and it's very hard to predict beforehand which are (as I know all too well from being in the business of trying to predict beforehand), so lots of people starting startups who shouldn't is probably the optimal state of affairs.  As long as you're at a point in your life when you can bear the risk of failure, the best way to find out if you're suited to running a startup is to try it.ChanceThe second component of the antidote is chance meetings with people who can help you.",
      "  This force works in both phases: both in the transition from the desire to start a startup to starting one, and the transition from starting a company to succeeding.  The power of chance meetings is more variable than people around you caring about startups, which is like a sort of background radiation that affects everyone equally, but at its strongest it is far stronger.Chance meetings produce miracles to compensate for the disasters that characteristically befall startups.  In the Valley, terrible things happen to startups all the time, just like they do to startups everywhere.  The reason startups are more likely to make it here is that great things happen to them too.  In the Valley, lightning has a sign bit.For example, you start a site for college students and you decide to move to the Valley for the summer to work on it.  And then on a random suburban street in Palo Alto you happen to run into Sean Parker, who understands the domain really well because he started a similar startup himself, and also knows all the investors.  And moreover has advanced views, for 2004, on founders retaining control of their companies.You can't say precisely what the miracle will be, or even for sure that one will happen.  The best one can say is: if you're in a startup hub,",
      " unexpected good things will probably happen to you, especially if you deserve them.I bet this is true even for startups we fund.  Even with us working to make things happen for them on purpose rather than by accident, the frequency of helpful chance meetings in the Valley is so high that it's still a significant increment on what we can deliver.Chance meetings play a role like the role relaxation plays in having ideas.  Most people have had the experience of working hard on some problem, not being able to solve it, giving up and going to bed, and then thinking of the answer in the shower in the morning.  What makes the answer appear is letting your thoughts drift a bit\u2014and thus drift off the wrong path you'd been pursuing last night and onto the right one adjacent to it.Chance meetings let your acquaintance drift in the same way taking a shower lets your thoughts drift. The critical thing in both cases is that they drift just the right amount.  The meeting between Larry Page and Sergey Brin was a good example.  They let their acquaintance drift, but only a little; they were both meeting someone they had a lot in common with.For Larry Page the most important component of the antidote was Sergey Brin, and vice versa.  The antidote is  people.",
      "  It's not the physical infrastructure of Silicon Valley that makes it work, or the weather, or anything like that.  Those helped get it started, but now that the reaction is self-sustaining what drives it is the people.Many observers have noticed that one of the most distinctive things about startup hubs is the degree to which people help one another out, with no expectation of getting anything in return.  I'm not sure why this is so.  Perhaps it's because startups are less of a zero sum game than most types of business; they are rarely killed by competitors.  Or perhaps it's because so many startup founders have backgrounds in the sciences, where collaboration is encouraged.A large part of YC's function is to accelerate that process.  We're a sort of Valley within the Valley, where the density of people working on startups and their willingness to help one another are both artificially amplified.NumbersBoth components of the antidote\u2014an environment that encourages startups, and chance meetings with people who help you\u2014are driven by the same underlying cause: the number of startup people around you.  To make a startup hub, you need a lot of people interested in startups.There are three reasons. The first, obviously, is that if you don't have enough density, the chance meetings don't happen.",
      " [4] The second is that different startups need such different things, so you need a lot of people to supply each startup with what they need most.  Sean Parker was exactly what Facebook needed in 2004.  Another startup might have needed a database guy, or someone with connections in the movie business.This is one of the reasons we fund such a large number of companies, incidentally.  The bigger the community, the greater the chance it will contain the person who has that one thing you need most.The third reason you need a lot of people to make a startup hub is that once you have enough people interested in the same problem, they start to set the social norms.  And it is a particularly valuable thing when the atmosphere around you encourages you to do something that would otherwise seem too ambitious.  In most places the atmosphere pulls you back toward the mean.I flew into the Bay Area a few days ago.  I notice this every time I fly over the Valley: somehow you can sense something is going on.   Obviously you can sense prosperity in how well kept a place looks.  But there are different kinds of prosperity.  Silicon Valley doesn't look like Boston, or New York, or LA, or DC.  I tried asking myself what word I'd use to describe the feeling the Valley radiated,",
      " and the word that came to mind was optimism.Notes[1] I'm not saying it's impossible to succeed in a city with few other startups, just harder.  If you're sufficiently good at generating your own morale, you can survive without external encouragement.  Wufoo was based in Tampa and they succeeded.  But the Wufoos are exceptionally disciplined.[2] Incidentally, this phenomenon is not limited to startups.  Most unusual ambitions fail, unless the person who has them manages to find the right sort of community.[3] Starting a company is common, but starting a startup is rare. I've talked about the distinction between the two elsewhere, but essentially a startup is a new business designed for scale.  Most new businesses are service businesses and except in rare cases those don't scale.[4] As I was writing this, I had a demonstration of the density of startup people in the Valley.  Jessica and I bicycled to University Ave in Palo Alto to have lunch at the fabulous Oren's Hummus.  As we walked in, we met Charlie Cheever sitting near the door.  Selina Tobaccowala stopped to say hello on her way out.  Then Josh Wilson came in to pick up a take out order.  After lunch we went to get frozen yogurt.",
      "  On the way we met Rajat Suri.  When we got to the yogurt place, we found Dave Shen there, and as we walked out we ran into Yuri Sagalov.  We walked with him for a block"
    ]
  },
  {
    "id": 27,
    "question": "What is the best way to improve memory?",
    "answer": "through regular mental exercises and a healthy diet.",
    "docs": [
      "September 2007In high school I decided I was going to study philosophy in college. I had several motives, some more honorable than others.  One of the less honorable was to shock people.  College was regarded as job training where I grew up, so studying philosophy seemed an impressively impractical thing to do.  Sort of like slashing holes in your clothes or putting a safety pin through your ear, which were other forms of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying philosophy would be a shortcut straight to wisdom.  All the people majoring in other things would just end up with a bunch of domain knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you wouldn't find those in our high school library.  But I tried to read Plato and Aristotle.  I doubt I believed I understood them, but they sounded like they were talking about something important. I assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned a lot in the calculus class, but I didn't learn much in Philosophy 101.  And yet my plan to study philosophy remained intact.  It was my fault I hadn't learned anything.",
      "  I hadn't read the books we were assigned carefully enough.  I'd give Berkeley's Principles of Human Knowledge another shot in college.  Anything so admired and so difficult to read must have something in it, if one could only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have a nice edition of his collected works.  Will I ever read it?  Seems unlikely.The difference between then and now is that now I understand why Berkeley is probably not worth trying to understand.  I think I see now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It didn't work out as I'd hoped.  I didn't learn any magical truths compared to which everything else was mere domain knowledge.  But I do at least know now why I didn't.  Philosophy doesn't really have a subject matter in the way math or history or most other university subjects do.  There is no core of knowledge one must master.  The closest you come to that is a knowledge of what various individual philosophers have said about different topics over the years.  Few were sufficiently correct that people have forgotten who discovered what they discovered.Formal logic has some subject matter.",
      " I took several classes in logic.  I don't know if I learned anything from them. [1] It does seem to me very important to be able to flip ideas around in one's head: to see when two ideas don't fully cover the space of possibilities, or when one idea is the same as another but with a couple things changed.  But did studying logic teach me the importance of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The most dramatic I learned immediately, in the first semester of freshman year, in a class taught by Sydney Shoemaker.  I learned that I don't exist.  I am (and you are) a collection of cells that lurches around driven by various forces, and calls itself I.  But there's no central, indivisible thing that your identity goes with. You could conceivably lose half your brain and live.  Which means your brain could conceivably be split into two halves and each transplanted into different bodies.  Imagine waking up after such an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life are fuzzy, and break down if pushed too hard.",
      "  Even a concept as dear to us as I.  It took me a while to grasp this, but when I did it was fairly sudden, like someone in the nineteenth century grasping evolution and realizing the story of creation they'd been told as a child was all wrong.  [2] Outside of math there's a limit to how far you can push words; in fact, it would not be a bad definition of math to call it the study of terms that have precise meanings.  Everyday words are inherently imprecise.  They work well enough in everyday life that you don't notice.  Words seem to work, just as Newtonian physics seems to.  But you can always make them break if you push them far enough.I would say that this has been, unfortunately for philosophy, the central fact of philosophy.  Most philosophical debates are not merely afflicted by but driven by confusions over words.  Do we have free will?  Depends what you mean by \"free.\" Do abstract ideas exist?  Depends what you mean by \"exist.\"Wittgenstein is popularly credited with the idea that most philosophical controversies are due to confusions over language.  I'm not sure how much credit to give him.  I suspect a lot of people realized this,",
      " but reacted simply by not studying philosophy, rather than becoming philosophy professors.How did things get this way?  Can something people have spent thousands of years studying really be a waste of time?  Those are interesting questions.  In fact, some of the most interesting questions you can ask about philosophy.  The most valuable way to approach the current philosophical tradition may be neither to get lost in pointless speculations like Berkeley, nor to shut them down like Wittgenstein, but to study it as an example of reason gone wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle. What we know of their predecessors comes from fragments and references in later works; their doctrines could be described as speculative cosmology that occasionally strays into analysis.  Presumably they were driven by whatever makes people in every other society invent cosmologies. [3]With Socrates, Plato, and particularly Aristotle, this tradition turned a corner.  There started to be a lot more analysis.  I suspect Plato and Aristotle were encouraged in this by progress in math. Mathematicians had by then shown that you could figure things out in a much more conclusive way than by making up fine sounding stories about them.   [4]People talk so much about abstractions now that we don't realize what a leap it must have been when they first started to.",
      "  It was presumably many thousands of years between when people first started describing things as hot or cold and when someone asked \"what is heat?\"  No doubt it was a very gradual process.  We don't know if Plato or Aristotle were the first to ask any of the questions they did.  But their works are the oldest we have that do this on a large scale, and there is a freshness (not to say naivete) about them that suggests some of the questions they asked were new to them, at least.Aristotle in particular reminds me of the phenomenon that happens when people discover something new, and are so excited by it that they race through a huge percentage of the newly discovered territory in one lifetime.  If so, that's evidence of how new this kind of thinking was.  [5]This is all to explain how Plato and Aristotle can be very impressive and yet naive and mistaken.  It was impressive even to ask the questions they did.  That doesn't mean they always came up with good answers.  It's not considered insulting to say that ancient Greek mathematicians were naive in some respects, or at least lacked some concepts that would have made their lives easier.  So I hope people will not be too offended if I propose that ancient philosophers were similarly naive.",
      "  In particular, they don't seem to have fully grasped what I earlier called the central fact of philosophy: that words break if you push them too far.\"Much to the surprise of the builders of the first digital computers,\" Rod Brooks wrote, \"programs written for them usually did not work.\" [6] Something similar happened when people first started trying to talk about abstractions.  Much to their surprise, they didn't arrive at answers they agreed upon.  In fact, they rarely seemed to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at too low a resolution.The proof of how useless some of their answers turned out to be is how little effect they have.  No one after reading Aristotle's Metaphysics does anything differently as a result. [7]Surely I'm not claiming that ideas have to have practical applications to be interesting?  No, they may not have to.  Hardy's boast that number theory had no use whatsoever wouldn't disqualify it.  But he turned out to be mistaken.  In fact, it's suspiciously hard to find a field of math that truly has no practical use.  And Aristotle's explanation of the ultimate goal of philosophy in Book A of the Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.",
      " The examples he gives are convincing: an ordinary worker builds things a certain way out of habit; a master craftsman can do more because he grasps the underlying principles.  The trend is clear: the more general the knowledge, the more admirable it is.  But then he makes a mistake\u2014possibly the most important mistake in the history of philosophy.  He has noticed that theoretical knowledge is often acquired for its own sake, out of curiosity, rather than for any practical need.  So he proposes there are two kinds of theoretical knowledge: some that's useful in practical matters and some that isn't.  Since people interested in the latter are interested in it for its own sake, it must be more noble.  So he sets as his goal in the Metaphysics the exploration of knowledge that has no practical use.  Which means no alarms go off when he takes on grand but vaguely understood questions and ends up getting lost in a sea of words.His mistake was to confuse motive and result.  Certainly, people who want a deep understanding of something are often driven by curiosity rather than any practical need.  But that doesn't mean what they end up learning is useless.  It's very valuable in practice to have a deep understanding of what you're doing; even if you're never called on to solve advanced problems,",
      " you can see shortcuts in the solution of simple ones, and your knowledge won't break down in edge cases, as it would if you were relying on formulas you didn't understand.  Knowledge is power.  That's what makes theoretical knowledge prestigious.  It's also what causes smart people to be curious about certain things and not others; our DNA is not so disinterested as we might think.So while ideas don't have to have immediate practical applications to be interesting, the kinds of things we find interesting will surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was partly that he set off with contradictory aims: to explore the most abstract ideas, guided by the assumption that they were useless. He was like an explorer looking for a territory to the north of him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future explorers, he sent them off in the wrong direction as well.  [8] Perhaps worst of all, he protected them from both the criticism of outsiders and the promptings of their own inner compass by establishing the principle that the most noble sort of theoretical knowledge had to be useless.The Metaphysics is mostly a failed experiment.  A few ideas from it turned out to be worth keeping;",
      " the bulk of it has had no effect at all.  The Metaphysics is among the least read of all famous books.  It's not hard to understand the way Newton's Principia is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately that was not the conclusion Aristotle's successors derived from works like the Metaphysics.  [9] Soon after, the western world fell on intellectual hard times.  Instead of version 1s to be superseded, the works of Plato and Aristotle became revered texts to be mastered and discussed.  And so things remained for a shockingly long time.  It was not till around 1600 (in Europe, where the center of gravity had shifted by then) that one found people confident enough to treat Aristotle's work as a catalog of mistakes.  And even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little progress there was in math between Hellenistic times and the Renaissance.In the intervening years an unfortunate idea took hold:  that it was not only acceptable to produce works like the Metaphysics, but that it was a particularly prestigious line of work, done by a class of people called philosophers.  No one thought to go back and debug Aristotle's motivating argument.",
      "  And so instead of correcting the problem Aristotle discovered by falling into it\u2014that you can easily get lost if you talk too loosely about very abstract ideas\u2014they  continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract new readers.  Traditional philosophy occupies a kind of singularity in this respect.  If you write in an unclear way about big ideas, you produce something that seems tantalizingly attractive to inexperienced but intellectually ambitious students.  Till one knows better, it's hard to distinguish something that's hard to understand because the writer was unclear in his own mind from something like a mathematical proof that's hard to understand because the ideas it represents are hard to understand.  To someone who hasn't learned the difference, traditional philosophy seems extremely attractive: as hard (and therefore impressive) as math, yet broader in scope. That was what lured me in as a high school student.This singularity is even more singular in having its own defense built in.  When things are hard to understand, people who suspect they're nonsense generally keep quiet.  There's no way to prove a text is meaningless.  The closest you can get is to show that the official judges of some class of texts can't distinguish them from placebos.  [10]",
      "And so instead of denouncing philosophy, most people who suspected it was a waste of time just studied other things.  That alone is fairly damning evidence, considering philosophy's claims.  It's supposed to be about the ultimate truths. Surely all smart people would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might have corrected them, they tended to be self-perpetuating.  Bertrand Russell wrote in a letter in 1912:    Hitherto the people attracted to philosophy have been mostly those   who loved the big generalizations, which were all wrong, so that   few people with exact minds have taken up the subject. [11]  His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery that most previous philosophy was a waste of time, which judging from the circumstantial evidence must have been made by every smart person who studied a little philosophy and declined to pursue it further, but for how he acted in response. [12] Instead of quietly switching to another field, he made a fuss, from inside.  He was Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein gave it.",
      "  [13] Later in life he spent a lot of time talking about how words worked.  Since that seems to be allowed, that's what a lot of philosophers do now.  Meanwhile, sensing a vacuum in the metaphysical speculation department, the people who used to do literary criticism have been edging Kantward, under new names like \"literary theory,\" \"critical theory,\" and when they're feeling ambitious, plain \"theory.\"  The writing is the familiar word salad:    Gender is not like some of the other grammatical modes which   express precisely a mode of conception without any reality that   corresponds to the conceptual mode, and consequently do not express   precisely something in reality by which the intellect could be   moved to conceive a thing the way it does, even where that motive   is not something in the thing as such.   [14]  The singularity I've described is not going away.  There's a market for writing that sounds impressive and can't be disproven. There will always be both supply and demand.  So if one group abandons this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility. Perhaps we should do what Aristotle meant to do,",
      " instead of what he did.  The goal he announces in the Metaphysics seems one worth pursuing: to discover the most general truths.  That sounds good. But instead of trying to discover them because they're useless, let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised criterion, applicability, as a guide to keep us from wondering off into a swamp of abstractions.  Instead of trying to answer the question:    What are the most general truths?  let's try to answer the question    Of all the useful things we can say, which are the most general?  The test of utility I propose is whether we cause people who read what we've written to do anything differently afterward.  Knowing we have to give definite (if implicit) advice will keep us from straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a different direction.As an example of a useful, general idea, consider that of the controlled experiment.  There's an idea that has turned out to be widely applicable.  Some might say it's part of science, but it's not part of any specific science; it's literally meta-physics (in our sense of \"meta\").   The idea of evolution is another.",
      " It turns out to have quite broad applications\u2014for example, in genetic algorithms and even product design.  Frankfurt's distinction between lying and bullshitting seems a promising recent example. [15]These seem to me what philosophy should look like: quite general observations that would cause someone who understood them to do something differently.Such observations will necessarily be about things that are imprecisely defined.  Once you start using words with precise meanings, you're doing math.  So starting from utility won't entirely solve the problem I described above\u2014it won't flush out the metaphysical singularity.  But it should help.  It gives people with good intentions a new roadmap into abstraction.  And they may thereby produce things that make the writing of the people with bad intentions look bad by comparison.One drawback of this approach is that it won't produce the sort of writing that gets you tenure.  And not just because it's not currently the fashion.  In order to get tenure in any field you must not arrive at conclusions that members of tenure committees can disagree with.  In practice there are two kinds of solutions to this problem. In math and the sciences, you can prove what you're saying, or at any rate adjust your conclusions so you're not claiming anything false (\"6 of 8 subjects had lower blood pressure after the treatment\"). In the humanities you can either avoid drawing any definite conclusions (e.g.",
      " conclude that an issue is a complex one), or draw conclusions so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either of these routes.  At best you'll be able to achieve the essayist's standard of proof, not the mathematician's or the experimentalist's. And yet you won't be able to meet the usefulness test without implying definite and fairly broadly applicable conclusions.  Worse still, the usefulness test will tend to produce results that annoy people: there's no use in telling people things they already believe, and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting to general plus useful by starting with useful and cranking up the generality may be unsuitable for junior professors trying to get tenure, but it's better for everyone else, including professors who already have it.  This side of the mountain is a nice gradual slope. You can start by writing things that are useful but very specific, and then gradually make them more general.  Joe's has good burritos. What makes a good burrito?  What makes good food?  What makes anything good?  You can take as long as you want.",
      "  You don't have to get all the way to the top of the mountain.  You don't have to tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an encouraging thought.  The field is a lot younger than it seems. Though the first philosophers in the western tradition lived about 2500 years ago, it would be misleading to say the field is 2500 years old, because for most of that time the leading practitioners weren't doing much more than writing commentaries on Plato or Aristotle while watching over their shoulders for the next invading army.  In the times when they weren't, philosophy was hopelessly intermingled with religion.  It didn't shake itself free till a couple hundred years ago, and even then was afflicted by the structural problems I've described above.  If I say this, some will say it's a ridiculously overbroad and uncharitable generalization, and others will say it's old news, but here goes: judging from their works, most philosophers up to the present have been wasting their time.  So in a sense the field is still at the first step.  [16]That sounds a preposterous claim to make.  It won't seem so preposterous in 10,",
      "000 years.  Civilization always seems old, because it's always the oldest it's ever been.  The only way to say whether something is really old or not is by looking at structural evidence, and structurally philosophy is young; it's still reeling from the unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot more to discover.Notes [1] In practice formal logic is not much use, because despite some progress in the last 150 years we're still only able to formalize a small percentage of statements.  We may never do that much better, for the same reason 1980s-style \"knowledge representation\" could never have worked; many statements may have no representation more concise than a huge, analog brain state.[2] It was harder for Darwin's contemporaries to grasp this than we can easily imagine.  The story of creation in the Bible is not just a Judeo-Christian concept; it's roughly what everyone must have believed since before people were people.  The hard part of grasping evolution was to realize that species weren't, as they seem to be, unchanging, but had instead evolved from different, simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.",
      "  No one in an industrialized country encounters the idea of evolution for the first time as an adult.  Everyone's taught about it as a child, either as truth or heresy.[3] Greek philosophers before Plato wrote in verse.  This must have affected what they said.  If you try to write about the nature of the world in verse, it inevitably turns into incantation.  Prose lets you be more precise, and more tentative.[4] Philosophy is like math's ne'er-do-well brother.  It was born when Plato and Aristotle looked at the works of their predecessors and said in effect \"why can't you be more like your brother?\"  Russell was still saying the same thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy the imprecise half.  It's probably inevitable that philosophy will suffer by comparison, because there's no lower bound to its precision. Bad math is merely boring, whereas bad philosophy is nonsense.  And yet there are some good ideas in the imprecise half.[5] Aristotle's best work was in logic and zoology, both of which he can  be said to have invented.  But the most dramatic departure from his predecessors was a new, much more analytical style of thinking.",
      "  He was arguably the first scientist.[6] Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p. 94.[7] Some would say we depend on Aristotle more than we realize, because his ideas were one of the ingredients in our common culture. Certainly a lot of the words we use have a connection with Aristotle, but it seems a bit much to suggest that we wouldn't have the concept of the essence of something or the distinction between matter and form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to diff European culture with Chinese: what ideas did European culture have in 1800 that Chinese culture didn't, in virtue of Aristotle's contribution?[8] The meaning of the word \"philosophy\" has changed over time. In ancient times it covered a broad range of topics, comparable in scope to our \"scholarship\" (though without the methodological implications).  Even as late as Newton's time it included what we now call \"science.\"  But core of the subject today is still what seemed to Aristotle the core: the attempt to discover the most general truths.Aristotle didn't call this \"metaphysics.\"  That name got assigned to it because the books we now call the Metaphysics came after (meta = after)",
      " the Physics in the standard edition of Aristotle's works compiled by Andronicus of Rhodes three centuries later.  What we call \"metaphysics\" Aristotle called \"first philosophy.\"[9] Some of Aristotle's immediate successors may have realized this, but it's hard to say because most of their works are lost.[10] Sokal, Alan, \"Transgressing the Boundaries: Toward a Transformative Hermeneutics of Quantum Gravity,\" Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's aligned with some axe the audience already has to grind.  If this is so we should find it's most popular with groups that are (or feel) weak.  The powerful don't need its reassurance.[11] Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991, p. 75.[12] A preliminary result, that all metaphysics between Aristotle and 1783 had been a waste of time, is due to I. Kant.[13] Wittgenstein asserted a sort of mastery to which the inhabitants of early 20th century Cambridge seem to have been peculiarly vulnerable\u2014perhaps partly because so many had been raised religious and then stopped believing,",
      " so had a vacant space in their heads for someone to tell them what to do (others chose Marx or Cardinal Newman), and partly because a quiet, earnest place like Cambridge in that era had no natural immunity to messianic figures, just as European politics then had no natural immunity to dictators.[14] This is actually from the Ordinatio of Duns Scotus (ca. 1300), with \"number\" replaced by \"gender.\"  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson, 1963, p. 92.[15] Frankfurt, Harry, On Bullshit,  Princeton University Press, 2005.[16] Some introductions to philosophy now take the line that philosophy is worth studying as a process rather than for any particular truths you'll learn.  The philosophers whose works they cover would be rolling in their graves at that.  They hoped they were doing more than serving as examples of how to argue: they hoped they were getting results.  Most were wrong, but it doesn't seem an impossible hope.This argument seems to me like someone in 1500 looking at the lack of results achieved by alchemy and saying its value was as a process. No,",
      " they were going about it wrong.  It turns out it is possible to transmute lead into gold (though not economically at current energy prices), but the route to that knowledge was to backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston,  Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.February 2021Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u0097 CPU, disk drives, printer, card reader \u0097 sitting up on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran.",
      " You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping.",
      "  [1]The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths.",
      " All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive,",
      " and programmers' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. It wasn't happening in a class, like it was supposed to, but that was ok. For the next couple years I was on a roll. I knew what I was going to do.For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my belief \u0097 hard to imagine now, but not unique in 1985 \u0097 that it was already climbing the lower slopes of intelligence.I had gotten into a program at Cornell that didn't make you choose a major. You could take whatever classes you liked, and choose whatever you liked to put on your degree. I of course chose \"Artificial Intelligence.\" When I got the actual physical diploma, I was dismayed to find that the quotes had been included, which made them read as scare-quotes. At the time this bothered me,",
      " but now it seems amusingly accurate, for reasons I was about to discover.I applied to 3 grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which I'd visited because Rich Draves went there, and was also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU clone. Only Harvard accepted me, so that was where I went.I don't remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that's told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.What these programs really showed was that there's a subset of natural language that's a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. Its brokenness did,",
      " as so often happens, generate a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to get us Mike.So I looked around to see what I could salvage from the wreckage of my plans, and there was Lisp. I knew from experience that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp hacking. It's scary to think how little I knew about Lisp hacking when I started writing that book. But there's nothing like writing a book about something to help you learn it. The book, On Lisp, wasn't published till 1993, but I wrote much of it in grad school.Computer Science is an uneasy alliance between two halves, theory and systems. The theory people prove things, and the systems people build things. I wanted to build things. I had plenty of respect for theory \u0097 indeed, a sneaking suspicion that it was the more admirable of the two halves \u0097 but building things seemed so much more exciting.The problem with systems work, though, was that it didn't last. Any program you wrote today,",
      " no matter how good, would be obsolete in a couple decades at best. People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that, in its time, it had been good.There were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would last.In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.And moreover this was something you could make a living doing.",
      " Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.I had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art \u0097 that it didn't just appear spontaneously \u0097 but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.That fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.So now I was in a PhD program in computer science, yet planning to be an artist, yet also genuinely in love with Lisp hacking and working away at On Lisp. In other words, like many a grad student, I was working energetically on multiple projects that were not my thesis.I didn't see a way out of this situation.",
      " I didn't want to drop out of grad school, but how else was I going to get out? I remember when my friend Robert Morris got kicked out of Cornell for writing the internet worm of 1988, I was envious that he'd found such a spectacular way to get out of grad school.Then one day in April 1990 a crack appeared in the wall. I ran into professor Cheatham and he asked if I was far enough along to graduate that June. I didn't have a word of my dissertation written, but in what must have been the quickest bit of thinking in my life, I decided to take a shot at writing one in the 5 weeks or so that remained before the deadline, reusing parts of On Lisp where I could, and I was able to respond, with no perceptible delay \"Yes, I think so. I'll give you something to read in a few days.\"I picked applications of continuations as the topic. In retrospect I should have written about macros and embedded languages. There's a whole world there that's barely been explored. But all I wanted was to get out of grad school, and my rapidly written dissertation sufficed, just barely.Meanwhile I was applying to art schools. I applied to two: RISD in the US,",
      " and the Accademia di Belli Arti in Florence, which, because it was the oldest art school, I imagined would be good. RISD accepted me, and I never heard back from the Accademia, so off to Providence I went.I'd applied for the BFA program at RISD, which meant in effect that I had to go to college again. This was not as strange as it sounds, because I was only 25, and art schools are full of people of different ages. RISD counted me as a transfer sophomore and said I had to do the foundation that summer. The foundation means the classes that everyone has to take in fundamental subjects like drawing, color, and design.Toward the end of the summer I got a big surprise: a letter from the Accademia, which had been delayed because they'd sent it to Cambridge England instead of Cambridge Massachusetts, inviting me to take the entrance exam in Florence that fall. This was now only weeks away. My nice landlady let me leave my stuff in her attic. I had some money saved from consulting work I'd done in grad school; there was probably enough to last a year if I lived cheaply. Now all I had to do was learn Italian.Only stranieri (foreigners)",
      " had to take this entrance exam. In retrospect it may well have been a way of excluding them, because there were so many stranieri attracted by the idea of studying art in Florence that the Italian students would otherwise have been outnumbered. I was in decent shape at painting and drawing from the RISD foundation that summer, but I still don't know how I managed to pass the written exam. I remember that I answered the essay question by writing about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited vocabulary.  [2]I'm only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed. The students and faculty in the painting department at the Accademia were the nicest people you could imagine, but they had long since arrived at an arrangement whereby the students wouldn't require the faculty to teach anything, and in return the faculty wouldn't require the students to learn anything. And at the same time all involved would adhere outwardly to the conventions of a 19th century atelier. We actually had one of those little stoves,",
      " fed with kindling, that you see in 19th century studio paintings, and a nude model sitting as close to it as possible without getting burned. Except hardly anyone else painted her besides me. The rest of the students spent their time chatting or occasionally trying to imitate things they'd seen in American art magazines.Our model turned out to live just down the street from me. She made a living from a combination of modelling and making fakes for a local antique dealer. She'd copy an obscure old painting out of a book, and then he'd take the copy and maltreat it to make it look old.  [3]While I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny, because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still lives is different from painting people, because the subject, as its name suggests, can't move. People can't sit for more than about 15 minutes at a time, and when they do they don't sit very still. So the traditional m.o. for painting people is to know how to paint a generic person, which you then modify to match the specific person you're painting.",
      " Whereas a still life you can, if you want, copy pixel by pixel from what you're seeing. You don't want to stop there, of course, or you get merely photographic accuracy, and what makes a still life interesting is that it's been through a head. You want to emphasize the visual cues that tell you, for example, that the reason the color changes suddenly at a certain point is that it's the edge of an object. By subtly emphasizing such things you can make paintings that are more realistic than photographs not just in some metaphorical sense, but in the strict information-theoretic sense.  [4]I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren't consciously aware of much we're seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that's a water droplet\" without telling you details like where the lightest and darkest points are, or \"that's a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see.",
      " You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can  after days of trying to write an essay about something people usually take for granted.This is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.Our teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.I wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]Interleaf had done something pretty bold.",
      " Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.The good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that every hour, even when I was just sitting in a meeting.",
      " By living cheaply I not only managed to save enough to go back to RISD, but also paid off my college loans.I learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it's better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it The best way to improve memory is through regular mental exercises and a healthy diet.'s depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there's not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it.But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it's good to be the \"entry level\" option, even though that will be less prestigious, because if you're not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign.When I left to go back to RISD the next fall, I arranged to do freelance work for the group that did projects for customers,",
      " and this was how I survived for the next several years. When I came back to visit for a project later on, someone told me about a new thing called HTML, which was, as he described it, a derivative of SGML. Markup language enthusiasts were an occupational hazard at Interleaf and I ignored him, but this HTML thing later became a big part of my life.In the fall of 1992 I moved back to Providence to continue at RISD. The foundation had merely been intro stuff, and the Accademia had been a (very civilized) joke. Now I was going to see what real art school was like. But alas it was more like the Accademia than not. Better organized, certainly, and a lot more expensive, but it was now becoming clear that art school did not bear the same relationship to art that medical school bore to medicine. At least not the painting department. The textile department, which my next door neighbor belonged to, seemed to be pretty rigorous. No doubt illustration and architecture were too. But painting was post-rigorous. Painting students were supposed to express themselves, which to the more worldly ones meant to try to cook up some sort of distinctive signature style.A signature style is the visual equivalent of what in show business is known as a \"schtick\": something that immediately identifies the work as yours and no one else's.",
      " For example, when you see a painting that looks like a certain kind of cartoon, you know it's by Roy Lichtenstein. So if you see a big painting of this type hanging in the apartment of a hedge fund manager, you know he paid millions of dollars for it. That's not always why artists have a signature style, but it's usually why buyers pay a lot for such work. [6]There were plenty of earnest students too: kids who \"could draw\" in high school, and now had come to what was supposed to be the best art school in the country, to learn to draw even better. They tended to be confused and demoralized by what they found at RISD, but they kept going, because painting was what they did. I was not one of the kids who could draw in high school, but at RISD I was definitely closer to their tribe than the tribe of signature style seekers.I learned a lot in the color class I took at RISD, but otherwise I was basically teaching myself to paint, and I could do that for free. So in 1993 I dropped out. I hung around Providence for a bit, and then my college friend Nancy Parmet did me a big favor. A rent-controlled apartment in a building her mother owned in New York was becoming vacant.",
      " Did I want it? It wasn't much more than my current place, and New York was supposed to be where the artists were. So yes, I wanted it! [7]Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist \u0097 in the strictly technical sense of making paintings and living in New York.I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book,",
      " ANSI Common Lisp, is one that I painted around this time.)The best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.She liked to paint on big, square canvases, 4 to 5 feet on a side. One day in late 1994 as I was stretching one of these monsters there was something on the radio about a famous fund manager. He wasn't that much older than me, and was super rich. The thought suddenly occurred to me: why don't I become rich? Then I'll be able to work on whatever I want.Meanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me when I visited him in Cambridge, where he was now in grad school at Harvard. It seemed to me that the web would be a big deal. I'd seen what graphical user interfaces had done for the popularity of microcomputers. It seemed like the web would do the same for the internet.If I wanted to get rich,",
      " here was the next train leaving the station. I was right about that part. What I got wrong was the idea. I decided we should start a company to put art galleries online. I can't honestly say, after reading so many Y Combinator applications, that this was the worst startup idea ever, but it was up there. Art galleries didn't want to be online, and still don't, not the fancy ones. That's not how they sell. I wrote some software to generate web sites for galleries, and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up galleries. To call this a difficult sale would be an understatement. It was difficult to give away. A few galleries let us make sites for them for free, but none paid us.Then some online stores started to appear, and I realized that except for the order buttons they were identical to the sites we'd been generating for galleries. This impressive-sounding thing called an \"internet storefront\" was something we already knew how to build.So in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started trying to write software to build online stores. At first this was going to be normal desktop software,",
      " which in those days meant Windows software. That was an alarming prospect, because neither of us knew how to write Windows software or wanted to learn. We lived in the Unix world. But we decided we'd at least try writing a prototype store builder on Unix. Robert wrote a shopping cart, and I wrote a new site generator for stores \u0097 in Lisp, of course.We were working out of Robert's apartment in Cambridge. His roommate was away for big chunks of time, during which I got to sleep in his room. For some reason there was no bed frame or sheets, just a mattress on the floor. One morning as I was lying on this mattress I had an idea that made me sit up like a capital L. What if we ran the software on the server, and let users control it by clicking on links? Then we'd never have to write anything to run on users' computers. We could generate the sites on the same server we'd serve them from. Users wouldn't need anything more than a browser.This kind of software, known as a web app, is common now, but at the time it wasn't clear that it was even possible. To find out, we decided to try making a version of our store builder that you could control through the browser.",
      " A couple days later, on August 12, we had one that worked. The UI was horrible, but it proved you could build a whole store through the browser, without any client software or typing anything into the command line on the server.Now we felt like we were really onto something. I had visions of a whole new generation of software working this way. You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group called Release Engineering that seemed to be at least as big as the group that actually wrote the software. Now you could just update the software right on the server.We started a new company we called Viaweb, after the fact that our software worked via the web, and we got $10,000 in seed funding from Idelle's husband Julian. In return for that and doing the initial legal work and giving us business advice, we gave him 10% of the company. Ten years later this deal became the model for Y Combinator's. We knew founders needed something like this, because we'd needed it ourselves.At this stage I had a negative net worth, because the thousand dollars or so I had in the bank was more than counterbalanced by what I owed the government in taxes. (Had I diligently set aside the proper proportion of the money I'd made consulting for Interleaf?",
      " No, I had not.) So although Robert had his graduate student stipend, I needed that seed funding to live on.We originally hoped to launch in September, but we got more ambitious about the software as we worked on it. Eventually we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exactly like the static ones that would be generated later, except that instead of leading to static pages, the links all referred to closures stored in a hash table on the server.It helped to have studied art, because the main goal of an online store builder is to make users look legit, and the key to looking legit is high production values. If you get page layouts and fonts and colors right, you can make a guy running a store out of his bedroom look more legit than a big company.(If you're curious why my site looks so old-fashioned, it's because it's still made with this software. It may look clunky today, but in 1996 it was the last word in slick.)In September, Robert rebelled. \"We've been working on this for a month,\" he said, \"and it's still not done.\" This is funny in retrospect, because he would still be working on it almost 3 years later.",
      " But I decided it might be prudent to recruit more programmers, and I asked Robert who else in grad school with him was really good. He recommended Trevor Blackwell, which surprised me at first, because at that point I knew Trevor mainly for his plan to reduce everything in his life to a stack of notecards, which he carried around with him. But Rtm was right, as usual. Trevor turned out to be a frighteningly effective hacker.It was a lot of fun working with Robert and Trevor. They're the two most independent-minded people  I know, and in completely different ways. If you could see inside Rtm's brain it would look like a colonial New England church, and if you could see inside Trevor's it would look like the worst excesses of Austrian Rococo.We opened for business, with 6 stores, in January 1996. It was just as well we waited a few months, because although we worried we were late, we were actually almost fatally early. There was a lot of talk in the press then about ecommerce, but not many people actually wanted online stores. [8]There were three main parts to the software: the editor, which people used to build sites and which I wrote, the shopping cart, which Robert wrote,",
      " and the manager, which kept track of orders and statistics, and which Trevor wrote. In its time, the editor was one of the best general-purpose site builders. I kept the code tight and didn't have to integrate with any other software except Robert's and Trevor's, so it was quite fun to work on. If all I'd had to do was work on this software, the next 3 years would have been the easiest of my life. Unfortunately I had to do a lot more, all of it stuff I was worse at than programming, and the next 3 years were instead the most stressful.There were a lot of startups making ecommerce software in the second half of the 90s. We were determined to be the Microsoft Word, not the Interleaf. Which meant being easy to use and inexpensive. It was lucky for us that we were poor, because that caused us to make Viaweb even more inexpensive than we realized. We charged $100 a month for a small store and $300 a month for a big one. This low price was a big attraction, and a constant thorn in the sides of competitors, but it wasn't because of some clever insight that we set the price low. We had no idea what businesses paid for things. $300 a month seemed like a lot of money to us.We did a lot of things right by accident like that.",
      " For example, we did what's now called \"doing things that  don't scale,\" although at the time we would have described it as \"being so lame that we're driven to the most desperate measures to get users.\" The most common of which was building stores for them. This seemed particularly humiliating, since the whole raison d'etre of our software was that people could use it to make their own stores. But anything to get users.We learned a lot more about retail than we wanted to know. For example, that if you could only have a small image of a man's shirt (and all images were small then by present standards), it was better to have a closeup of the collar than a picture of the whole shirt. The reason I remember learning this was that it meant I had to rescan about 30 images of men's shirts. My first set of scans were so beautiful too.Though this felt wrong, it was exactly the right thing to be doing. Building stores for users taught us about retail, and about how it felt to use our software. I was initially both mystified and repelled by \"business\" and thought we needed a \"business person\" to be in charge of it, but once we started to get users, I was converted,",
      " in much the same way I was converted to  fatherhood once I had kids. Whatever users wanted, I was all theirs. Maybe one day we'd have so many users that I couldn't scan their images for them, but in the meantime there was nothing more important to do.Another thing I didn't get at the time is that  growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users. And that is the thing that matters in the sense that that's how much money you're making, and if you're not making enough, you might go out of business. But in the long term the growth rate takes care of the absolute number. If we'd been a startup I was advising at Y Combinator, I would have said: Stop being so stressed out, because you're doing fine. You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.Alas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble.",
      " A company with just a handful of employees would have seemed amateurish. So we didn't reach breakeven until about when Yahoo bought us in the summer of 1998. Which in turn meant we were at the mercy of investors for the entire life of the company. And since both we and our investors were noobs at startups, the result was a mess even by startup standards.It was a huge relief when Yahoo bought us. In principle our Viaweb stock was valuable. It was a share in a business that was profitable and growing rapidly. But it didn't feel very valuable to me; I had no idea how to value a business, but I was all too keenly aware of the near-death experiences we seemed to have every few months. Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb.",
      " For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.Yahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.When I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying.",
      " My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire. Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.But I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.So I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York,",
      " where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.When I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted yet).Meanwhile I looked for an apartment to buy. Now I could actually choose what neighborhood to live in. Where, I asked myself and various real estate agents,",
      " is the Cambridge of New York? Aided by occasional visits to actual Cambridge, I gradually realized there wasn't one. Huh.Around this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were the future. Why not build a web app for making web apps? Why not let people edit code on our server through the browser, and then host the resulting applications for them? [9] You could run all sorts of services on the servers that these applications could use just by making an API call: making and receiving phone calls, manipulating images, taking credit card payments, etc.I got so excited about this idea that I couldn't think about anything else. It seemed obvious that this was the future. I didn't particularly want to start another company, but it was clear that this idea would have to be embodied as one, so I decided to move to Cambridge and start it. I hoped to lure Robert into working on it with me, but there I ran into a hitch. Robert was now a postdoc at MIT, and though he'd made a lot of money the last time I'd lured him into working on one of my schemes, it had also been a huge time sink.",
      " So while he agreed that it sounded like a plausible idea, he firmly refused to work on it.Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted summer jobs, and we got to work trying to build what it's now clear is about twenty companies and several open source projects worth of software. The language for defining applications would of course be a dialect of Lisp. But I wasn't so naive as to assume I could spring an overt Lisp on a general audience; we'd hide the parentheses, like Dylan did.By then there was a name for the kind of company Viaweb was, an \"application service provider,\" or ASP. This name didn't last long before it was replaced by \"software as a service,\" but it was current for long enough that I named this new company after it: it was going to be called Aspra.I started working on the application builder, Dan worked on network infrastructure, and the two undergrads worked on the first two services (images and phone calls). But about halfway through the summer I realized I really didn't want to run a company \u0097 especially not a big one, which it was looking like this would have to be. I'd only started Viaweb because I needed the money.",
      " Now that I didn't need money anymore, why was I doing this? If this vision had to be realized as a company, then screw the vision. I'd build a subset that could be done as an open source project.Much to my surprise,"
    ]
  },
  {
    "id": 46,
    "question": "What is the most important factor in achieving success?",
    "answer": "persistence and hard work.",
    "docs": [
      "September 2007In high school I decided I was going to study philosophy in college. I had several motives, some more honorable than others.  One of the less honorable was to shock people.  College was regarded as job training where I grew up, so studying philosophy seemed an impressively impractical thing to do.  Sort of like slashing holes in your clothes or putting a safety pin through your ear, which were other forms of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying philosophy would be a shortcut straight to wisdom.  All the people majoring in other things would just end up with a bunch of domain knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you wouldn't find those in our high school library.  But I tried to read Plato and Aristotle.  I doubt I believed I understood them, but they sounded like they were talking about something important. I assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned a lot in the calculus class, but I didn't learn much in Philosophy 101.  And yet my plan to study philosophy remained intact.  It was my fault I hadn't learned anything.",
      "  I hadn't read the books we were assigned carefully enough.  I'd give Berkeley's Principles of Human Knowledge another shot in college.  Anything so admired and so difficult to read must have something in it, if one could only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have a nice edition of his collected works.  Will I ever read it?  Seems unlikely.The difference between then and now is that now I understand why Berkeley is probably not worth trying to understand.  I think I see now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It didn't work out as I'd hoped.  I didn't learn any magical truths compared to which everything else was mere domain knowledge.  But I do at least know now why I didn't.  Philosophy doesn't really have a subject matter in the way math or history or most other university subjects do.  There is no core of knowledge one must master.  The closest you come to that is a knowledge of what various individual philosophers have said about different topics over the years.  Few were sufficiently correct that people have forgotten who discovered what they discovered.Formal logic has some subject matter.",
      " I took several classes in logic.  I don't know if I learned anything from them. [1] It does seem to me very important to be able to flip ideas around in one's head: to see when two ideas don't fully cover the space of possibilities, or when one idea is the same as another but with a couple things changed.  But did studying logic teach me the importance of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The most dramatic I learned immediately, in the first semester of freshman year, in a class taught by Sydney Shoemaker.  I learned that I don't exist.  I am (and you are) a collection of cells that lurches around driven by various forces, and calls itself I.  But there's no central, indivisible thing that your identity goes with. You could conceivably lose half your brain and live.  Which means your brain could conceivably be split into two halves and each transplanted into different bodies.  Imagine waking up after such an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life are fuzzy, and break down if pushed too hard.",
      "  Even a concept as dear to us as I.  It took me a while to grasp this, but when I did it was fairly sudden, like someone in the nineteenth century grasping evolution and realizing the story of creation they'd been told as a child was all wrong.  [2] Outside of math there's a limit to how far you can push words; in fact, it would not be a bad definition of math to call it the study of terms that have precise meanings.  Everyday words are inherently imprecise.  They work well enough in everyday life that you don't notice.  Words seem to work, just as Newtonian physics seems to.  But you can always make them break if you push them far enough.I would say that this has been, unfortunately for philosophy, the central fact of philosophy.  Most philosophical debates are not merely afflicted by but driven by confusions over words.  Do we have free will?  Depends what you mean by \"free.\" Do abstract ideas exist?  Depends what you mean by \"exist.\"Wittgenstein is popularly credited with the idea that most philosophical controversies are due to confusions over language.  I'm not sure how much credit to give him.  I suspect a lot of people realized this,",
      " but reacted simply by not studying philosophy, rather than becoming philosophy professors.How did things get this way?  Can something people have spent thousands of years studying really be a waste of time?  Those are interesting questions.  In fact, some of the most interesting questions you can ask about philosophy.  The most valuable way to approach the current philosophical tradition may be neither to get lost in pointless speculations like Berkeley, nor to shut them down like Wittgenstein, but to study it as an example of reason gone wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle. What we know of their predecessors comes from fragments and references in later works; their doctrines could be described as speculative cosmology that occasionally strays into analysis.  Presumably they were driven by whatever makes people in every other society invent cosmologies. [3]With Socrates, Plato, and particularly Aristotle, this tradition turned a corner.  There started to be a lot more analysis.  I suspect Plato and Aristotle were encouraged in this by progress in math. Mathematicians had by then shown that you could figure things out in a much more conclusive way than by making up fine sounding stories about them.   [4]People talk so much about abstractions now that we don't realize what a leap it must have been when they first started to.",
      "  It was presumably many thousands of years between when people first started describing things as hot or cold and when someone asked \"what is heat?\"  No doubt it was a very gradual process.  We don't know if Plato or Aristotle were the first to ask any of the questions they did.  But their works are the oldest we have that do this on a large scale, and there is a freshness (not to say naivete) about them that suggests some of the questions they asked were new to them, at least.Aristotle in particular reminds me of the phenomenon that happens when people discover something new, and are so excited by it that they race through a huge percentage of the newly discovered territory in one lifetime.  If so, that's evidence of how new this kind of thinking was.  [5]This is all to explain how Plato and Aristotle can be very impressive and yet naive and mistaken.  It was impressive even to ask the questions they did.  That doesn't mean they always came up with good answers.  It's not considered insulting to say that ancient Greek mathematicians were naive in some respects, or at least lacked some concepts that would have made their lives easier.  So I hope people will not be too offended if I propose that ancient philosophers were similarly naive.",
      "  In particular, they don't seem to have fully grasped what I earlier called the central fact of philosophy: that words break if you push them too far.\"Much to the surprise of the builders of the first digital computers,\" Rod Brooks wrote, \"programs written for them usually did not work.\" [6] Something similar happened when people first started trying to talk about abstractions.  Much to their surprise, they didn't arrive at answers they agreed upon.  In fact, they rarely seemed to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at too low a resolution.The proof of how useless some of their answers turned out to be is how little effect they have.  No one after reading Aristotle's Metaphysics does anything differently as a result. [7]Surely I'm not claiming that ideas have to have practical applications to be interesting?  No, they may not have to.  Hardy's boast that number theory had no use whatsoever wouldn't disqualify it.  But he turned out to be mistaken.  In fact, it's suspiciously hard to find a field of math that truly has no practical use.  And Aristotle's explanation of the ultimate goal of philosophy in Book A of the Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.",
      " The examples he gives are convincing: an ordinary worker builds things a certain way out of habit; a master craftsman can do more because he grasps the underlying principles.  The trend is clear: the more general the knowledge, the more admirable it is.  But then he makes a mistake\u2014possibly the most important mistake in the history of philosophy.  He has noticed that theoretical knowledge is often acquired for its own sake, out of curiosity, rather than for any practical need.  So he proposes there are two kinds of theoretical knowledge: some that's useful in practical matters and some that isn't.  Since people interested in the latter are interested in it for its own sake, it must be more noble.  So he sets as his goal in the Metaphysics the exploration of knowledge that has no practical use.  Which means no alarms go off when he takes on grand but vaguely understood questions and ends up getting lost in a sea of words.His mistake was to confuse motive and result.  Certainly, people who want a deep understanding of something are often driven by curiosity rather than any practical need.  But that doesn't mean what they end up learning is useless.  It's very valuable in practice to have a deep understanding of what you're doing; even if you're never called on to solve advanced problems,",
      " you can see shortcuts in the solution of simple ones, and your knowledge won't break down in edge cases, as it would if you were relying on formulas you didn't understand.  Knowledge is power.  That's what makes theoretical knowledge prestigious.  It's also what causes smart people to be curious about certain things and not others; our DNA is not so disinterested as we might think.So while ideas don't have to have immediate practical applications to be interesting, the kinds of things we find interesting will surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was partly that he set off with contradictory aims: to explore the most abstract ideas, guided by the assumption that they were useless. He was like an explorer looking for a territory to the north of him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future explorers, he sent them off in the wrong direction as well.  [8] Perhaps worst of all, he protected them from both the criticism of outsiders and the promptings of their own inner compass by establishing the principle that the most noble sort of theoretical knowledge had to be useless.The Metaphysics is mostly a failed experiment.  A few ideas from it turned out to be worth keeping;",
      " the bulk of it has had no effect at all.  The Metaphysics is among the least read of all famous books.  It's not hard to understand the way Newton's Principia is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately that was not the conclusion Aristotle's successors derived from works like the Metaphysics.  [9] Soon after, the western world fell on intellectual hard times.  Instead of version 1s to be superseded, the works of Plato and Aristotle became revered texts to be mastered and discussed.  And so things remained for a shockingly long time.  It was not till around 1600 (in Europe, where the center of gravity had shifted by then) that one found people confident enough to treat Aristotle's work as a catalog of mistakes.  And even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little progress there was in math between Hellenistic times and the Renaissance.In the intervening years an unfortunate idea took hold:  that it was not only acceptable to produce works like the Metaphysics, but that it was a particularly prestigious line of work, done by a class of people called philosophers.  No one thought to go back and debug Aristotle's motivating argument.",
      "  And so instead of correcting the problem Aristotle discovered by falling into it\u2014that you can easily get lost if you talk too loosely about very abstract ideas\u2014they  continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract new readers.  Traditional philosophy occupies a kind of singularity in this respect.  If you write in an unclear way about big ideas, you produce something that seems tantalizingly attractive to inexperienced but intellectually ambitious students.  Till one knows better, it's hard to distinguish something that's hard to understand because the writer was unclear in his own mind from something like a mathematical proof that's hard to understand because the ideas it represents are hard to understand.  To someone who hasn't learned the difference, traditional philosophy seems extremely attractive: as hard (and therefore impressive) as math, yet broader in scope. That was what lured me in as a high school student.This singularity is even more singular in having its own defense built in.  When things are hard to understand, people who suspect they're nonsense generally keep quiet.  There's no way to prove a text is meaningless.  The closest you can get is to show that the official judges of some class of texts can't distinguish them from placebos.  [10]",
      "And so instead of denouncing philosophy, most people who suspected it was a waste of time just studied other things.  That alone is fairly damning evidence, considering philosophy's claims.  It's supposed to be about the ultimate truths. Surely all smart people would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might have corrected them, they tended to be self-perpetuating.  Bertrand Russell wrote in a letter in 1912:    Hitherto the people attracted to philosophy have been mostly those   who loved the big generalizations, which were all wrong, so that   few people with exact minds have taken up the subject. [11]  His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery that most previous philosophy was a waste of time, which judging from the circumstantial evidence must have been made by every smart person who studied a little philosophy and declined to pursue it further, but for how he acted in response. [12] Instead of quietly switching to another field, he made a fuss, from inside.  He was Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein gave it.",
      "  [13] Later in life he spent a lot of time talking about how words worked.  Since that seems to be allowed, that's what a lot of philosophers do now.  Meanwhile, sensing a vacuum in the metaphysical speculation department, the people who used to do literary criticism have been edging Kantward, under new names like \"literary theory,\" \"critical theory,\" and when they're feeling ambitious, plain \"theory.\"  The writing is the familiar word salad:    Gender is not like some of the other grammatical modes which   express precisely a mode of conception without any reality that   corresponds to the conceptual mode, and consequently do not express   precisely something in reality by which the intellect could be   moved to conceive a thing the way it does, even where that motive   is not something in the thing as such.   [14]  The singularity I've described is not going away.  There's a market for writing that sounds impressive and can't be disproven. There will always be both supply and demand.  So if one group abandons this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility. Perhaps we should do what Aristotle meant to do,",
      " instead of what he did.  The goal he announces in the Metaphysics seems one worth pursuing: to discover the most general truths.  That sounds good. But instead of trying to discover them because they're useless, let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised criterion, applicability, as a guide to keep us from wondering off into a swamp of abstractions.  Instead of trying to answer the question:    What are the most general truths?  let's try to answer the question    Of all the useful things we can say, which are the most general?  The test of utility I propose is whether we cause people who read what we've written to do anything differently afterward.  Knowing we have to give definite (if implicit) advice will keep us from straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a different direction.As an example of a useful, general idea, consider that of the controlled experiment.  There's an idea that has turned out to be widely applicable.  Some might say it's part of science, but it's not part of any specific science; it's literally meta-physics (in our sense of \"meta\").   The idea of evolution is another.",
      " It turns out to have quite broad applications\u2014for example, in genetic algorithms and even product design.  Frankfurt's distinction between lying and bullshitting seems a promising recent example. [15]These seem to me what philosophy should look like: quite general observations that would cause someone who understood them to do something differently.Such observations will necessarily be about things that are imprecisely defined.  Once you start using words with precise meanings, you're doing math.  So starting from utility won't entirely solve the problem I described above\u2014it won't flush out the metaphysical singularity.  But it should help.  It gives people with good intentions a new roadmap into abstraction.  And they may thereby produce things that make the writing of the people with bad intentions look bad by comparison.One drawback of this approach is that it won't produce the sort of writing that gets you tenure.  And not just because it's not currently the fashion.  In order to get tenure in any field you must not arrive at conclusions that members of tenure committees can disagree with.  In practice there are two kinds of solutions to this problem. In math and the sciences, you can prove what you're saying, or at any rate adjust your conclusions so you're not claiming anything false (\"6 of 8 subjects had lower blood pressure after the treatment\"). In the humanities you can either avoid drawing any definite conclusions (e.g.",
      " conclude that an issue is a complex one), or draw conclusions so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either of these routes.  At best you'll be able to achieve the essayist's standard of proof, not the mathematician's or the experimentalist's. And yet you won't be able to meet the usefulness test without implying definite and fairly broadly applicable conclusions.  Worse still, the usefulness test will tend to produce results that annoy people: there's no use in telling people things they already believe, and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting to general plus useful by starting with useful and cranking up the generality may be unsuitable for junior professors trying to get tenure, but it's better for everyone else, including professors who already have it.  This side of the mountain is a nice gradual slope. You can start by writing things that are useful but very specific, and then gradually make them more general.  Joe's has good burritos. What makes a good burrito?  What makes good food?  What makes anything good?  You can take as long as you want.",
      "  You don't have to get all the way to the top of the mountain.  You don't have to tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an encouraging thought.  The field is a lot younger than it seems. Though the first philosophers in the western tradition lived about 2500 years ago, it would be misleading to say the field is 2500 years old, because for most of that time the leading practitioners weren't doing much more than writing commentaries on Plato or Aristotle while watching over their shoulders for the next invading army.  In the times when they weren't, philosophy was hopelessly intermingled with religion.  It didn't shake itself free till a couple hundred years ago, and even then was afflicted by the structural problems I've described above.  If I say this, some will say it's a ridiculously overbroad and uncharitable generalization, and others will say it's old news, but here goes: judging from their works, most philosophers up to the present have been wasting their time.  So in a sense the field is still at the first step.  [16]That sounds a preposterous claim to make.  It won't seem so preposterous in 10,",
      "000 years.  Civilization always seems old, because it's always the oldest it's ever been.  The only way to say whether something is really old or not is by looking at structural evidence, and structurally philosophy is young; it's still reeling from the unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot more to discover.Notes [1] In practice formal logic is not much use, because despite some progress in the last 150 years we're still only able to formalize a small percentage of statements.  We may never do that much better, for the same reason 1980s-style \"knowledge representation\" could never have worked; many statements may have no representation more concise than a huge, analog brain state.[2] It was harder for Darwin's contemporaries to grasp this than we can easily imagine.  The story of creation in the Bible is not just a Judeo-Christian concept; it's roughly what everyone must have believed since before people were people.  The hard part of grasping evolution was to realize that species weren't, as they seem to be, unchanging, but had instead evolved from different, simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.",
      "  No one in an industrialized country encounters the idea of evolution for the first time as an adult.  Everyone's taught about it as a child, either as truth or heresy.[3] Greek philosophers before Plato wrote in verse.  This must have affected what they said.  If you try to write about the nature of the world in verse, it inevitably turns into incantation.  Prose lets you be more precise, and more tentative.[4] Philosophy is like math's ne'er-do-well brother.  It was born when Plato and Aristotle looked at the works of their predecessors and said in effect \"why can't you be more like your brother?\"  Russell was still saying the same thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy the imprecise half.  It's probably inevitable that philosophy will suffer by comparison, because there's no lower bound to its precision. Bad math is merely boring, whereas bad philosophy is nonsense.  And yet there are some good ideas in the imprecise half.[5] Aristotle's best work was in logic and zoology, both of which he can  be said to have invented.  But the most dramatic departure from his predecessors was a new, much more analytical style of thinking.",
      "  He was arguably the first scientist.[6] Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p. 94.[7] Some would say we depend on Aristotle more than we realize, because his ideas were one of the ingredients in our common culture. Certainly a lot of the words we use have a connection with Aristotle, but it seems a bit much to suggest that we wouldn't have the concept of the essence of something or the distinction between matter and form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to diff European culture with Chinese: what ideas did European culture have in 1800 that Chinese culture didn't, in virtue of Aristotle's contribution?[8] The meaning of the word \"philosophy\" has changed over time. In ancient times it covered a broad range of topics, comparable in scope to our \"scholarship\" (though without the methodological implications).  Even as late as Newton's time it included what we now call \"science.\"  But core of the subject today is still what seemed to Aristotle the core: the attempt to discover the most general truths.Aristotle didn't call this \"metaphysics.\"  That name got assigned to it because the books we now call the Metaphysics came after (meta = after)",
      " the Physics in the standard edition of Aristotle's works compiled by Andronicus of Rhodes three centuries later.  What we call \"metaphysics\" Aristotle called \"first philosophy.\"[9] Some of Aristotle's immediate successors may have realized this, but it's hard to say because most of their works are lost.[10] Sokal, Alan, \"Transgressing the Boundaries: Toward a Transformative Hermeneutics of Quantum Gravity,\" Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's aligned with some axe the audience already has to grind.  If this is so we should find it's most popular with groups that are (or feel) weak.  The powerful don't need its reassurance.[11] Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991, p. 75.[12] A preliminary result, that all metaphysics between Aristotle and 1783 had been a waste of time, is due to I. Kant.[13] Wittgenstein asserted a sort of mastery to which the inhabitants of early 20th century Cambridge seem to have been peculiarly vulnerable\u2014perhaps partly because so many had been raised religious and then stopped believing,",
      " so had a vacant space in their heads for someone to tell them what to do (others chose Marx or Cardinal Newman), and partly because a quiet, earnest place like Cambridge in that era had no natural immunity to messianic figures, just as European politics then had no natural immunity to dictators.[14] This is actually from the Ordinatio of Duns Scotus (ca. 1300), with \"number\" replaced by \"gender.\"  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson, 1963, p. 92.[15] Frankfurt, Harry, On Bullshit,  Princeton University Press, 2005.[16] Some introductions to philosophy now take the line that philosophy is worth studying as a process rather than for any particular truths you'll learn.  The philosophers whose works they cover would be rolling in their graves at that.  They hoped they were doing more than serving as examples of how to argue: they hoped they were getting results.  Most were wrong, but it doesn't seem an impossible hope.This argument seems to me like someone in 1500 looking at the lack of results achieved by alchemy and saying its value was as a process. No,",
      " they were going about it wrong.  It turns out it is possible to transmute lead into gold (though not economically at current energy prices), but the route to that knowledge was to backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston,  Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.  Want to start a startup?  Get funded by Y Combinator.     October 2010After barely changing at all for decades, the startup funding business is now in what could, at least by comparison, be called turmoil.  At Y Combinator we've seen dramatic changes in the funding environment for startups.  Fortunately one of them is much higher valuations.The trends we've been seeing are probably not YC-specific.  I wish I could say they were, but the main cause is probably just that we see trends first\u2014partly because the startups we fund are very plugged into the Valley and are quick to take advantage of anything new, and partly because we fund so many that we have enough data points to see patterns clearly.What we're seeing now, everyone's probably going to be seeing in the next couple years.  So I'm going to explain what we're seeing,",
      " and what that will mean for you if you try to raise money.Super-AngelsLet me start by describing what the world of startup funding used to look like.  There used to be two sharply differentiated types of investors: angels and venture capitalists.  Angels are individual rich people who invest small amounts of their own money, while VCs are employees of funds that invest large amounts of other people's.For decades there were just those two types of investors, but now a third type has appeared halfway between them: the so-called super-angels.  [1]   And VCs have been provoked by their arrival into making a lot of angel-style investments themselves.  So the previously sharp line between angels and VCs has become hopelessly blurred.There used to be a no man's land between angels and VCs.  Angels would invest $20k to $50k apiece, and VCs usually a million or more. So an angel round meant a collection of angel investments that combined to maybe $200k, and a VC round meant a series A round in which a single VC fund (or occasionally two) invested $1-5 million.The no man's land between angels and VCs was a very inconvenient one for startups, because it coincided with the amount many wanted to raise.",
      "  Most startups coming out of Demo Day wanted to raise around $400k.  But it was a pain to stitch together that much out of angel investments, and most VCs weren't interested in investments so small.  That's the fundamental reason the super-angels have appeared.  They're responding to the market.The arrival of a new type of investor is big news for startups, because there used to be only two and they rarely competed with one another.  Super-angels compete with both angels and VCs.  That's going to change the rules about how to raise money.  I don't know yet what the new rules will be, but it looks like most of the changes will be for the better.A super-angel has some of the qualities of an angel, and some of the qualities of a VC.  They're usually individuals, like angels. In fact many of the current super-angels were initially angels of the classic type.  But like VCs, they invest other people's money. This allows them to invest larger amounts than angels:  a typical super-angel investment is currently about $100k.  They make investment decisions quickly, like angels.  And they make a lot more investments per partner than VCs\u2014up to 10 times as many.The fact that super-",
      "angels invest other people's money makes them doubly alarming to VCs. They don't just compete for startups; they also compete for investors.  What super-angels really are is a new form of fast-moving, lightweight VC fund.   And those of us in the technology world know what usually happens when something comes along that can be described in terms like that.  Usually it's the replacement.Will it be?  As of now, few of the startups that take money from super-angels are ruling out taking VC money.  They're just postponing it.  But that's still a problem for VCs.  Some of the startups that postpone raising VC money may do so well on the angel money they raise that they never bother to raise more.  And those who do raise VC rounds will be able to get higher valuations when they do.  If the best startups get 10x higher valuations when they raise series A rounds, that would cut VCs' returns from winners at least tenfold. [2]So I think VC funds are seriously threatened by the super-angels. But one thing that may save them to some extent is the uneven distribution of startup outcomes: practically all the returns are concentrated in a few big successes.",
      "  The expected value of a startup is the percentage chance it's Google.  So to the extent that winning is a matter of absolute returns, the super-angels could win practically all the battles for individual startups and yet lose the war, if they merely failed to get those few big winners.  And there's a chance that could happen, because the top VC funds have better brands, and can also do more for their portfolio companies.   [3]Because super-angels make more investments per partner, they have less partner per investment.  They can't pay as much attention to you as a VC on your board could.  How much is that extra attention worth?  It will vary enormously from one partner to another.  There's no consensus yet in the general case.  So for now this is something startups are deciding individually.Till now, VCs' claims about how much value they added were sort of like the government's.  Maybe they made you feel better, but you had no choice in the matter, if you needed money on the scale only VCs could supply.  Now that VCs have competitors, that's going to put a market price on the help they offer.  The interesting thing is, no one knows yet what it will be.Do startups that want to get really big need the sort of advice and connections only the top VCs can supply?",
      "  Or would super-angel money do just as well?  The VCs will say you need them, and the super-angels will say you don't.  But the truth is, no one knows yet, not even the VCs and super-angels themselves.   All the super-angels know is that their new model seems promising enough to be worth trying, and all the VCs know is that it seems promising enough to worry about.RoundsWhatever the outcome, the conflict between VCs and super-angels is good news for founders.  And not just for the obvious reason that more competition for deals means better terms.  The whole shape of deals is changing.One of the biggest differences between angels and VCs is the amount of your company they want.  VCs want a lot.  In a series A round they want a third of your company, if they can get it.  They don't care much how much they pay for it, but they want a lot because the number of series A investments they can do is so small.  In a traditional series A investment, at least one partner from the VC fund takes a seat on your board.   [4]  Since board seats last about 5 years and each partner can't handle more than about 10 at once,",
      " that means a VC fund can only do about 2 series A deals per partner per year. And that means they need to get as much of the company as they can in each one.  You'd have to be a very promising startup indeed to get a VC to use up one of his 10 board seats for only a few percent of you.Since angels generally don't take board seats, they don't have this constraint.  They're happy to buy only a few percent of you.  And although the super-angels are in most respects mini VC funds, they've retained this critical property of angels.  They don't take board seats, so they don't need a big percentage of your company.Though that means you'll get correspondingly less attention from them, it's good news in other respects.  Founders never really liked giving up as much equity as VCs wanted.  It was a lot of the company to give up in one shot.  Most founders doing series A deals would prefer to take half as much money for half as much stock, and then see what valuation they could get for the second half of the stock after using the first half of the money to increase its value.  But VCs never offered that option.Now startups have another alternative.",
      "  Now it's easy to raise angel rounds about half the size of series A rounds.  Many of the startups we fund are taking this route, and I predict that will be true of startups in general.A typical big angel round might be $600k on a convertible note with a valuation cap of $4 million premoney.  Meaning that when the note converts into stock (in a later round, or upon acquisition), the investors in that round will get.6 / 4.6, or 13% of the company. That's a lot less than the 30 to 40% of the company you usually give up in a series A round if you do it so early.   [5]But the advantage of these medium-sized rounds is not just that they cause less dilution.  You also lose less control.  After an angel round, the founders almost always still have control of the company, whereas after a series A round they often don't.  The traditional board structure after a series A round is two founders, two VCs, and a (supposedly) neutral fifth person.  Plus series A terms usually give the investors a veto over various kinds of important decisions, including selling the company.  Founders usually have a lot of de facto control after a series A,",
      " as long as things are going well.  But that's not the same as just being able to do what you want, like you could before.A third and quite significant advantage of angel rounds is that they're less stressful to raise.  Raising a traditional series A round has in the past taken weeks, if not months.  When a VC firm can only do 2 deals per partner per year, they're careful about which they do.  To get a traditional series A round you have to go through a series of meetings, culminating in a full partner meeting where the firm as a whole says yes or no.  That's the really scary part for founders: not just that series A rounds take so long, but at the end of this long process the VCs might still say no.  The chance of getting rejected after the full partner meeting averages about 25%.  At some firms it's over 50%.Fortunately for founders, VCs have been getting a lot faster. Nowadays Valley VCs are more likely to take 2 weeks than 2 months. But they're still not as fast as angels and super-angels, the most decisive of whom sometimes decide in hours.Raising an angel round is not only quicker, but you get feedback as it progresses.",
      "  An angel round is not an all or nothing thing like a series A.  It's composed of multiple investors with varying degrees of seriousness, ranging from the upstanding ones who commit unequivocally to the jerks who give you lines like \"come back to me to fill out the round.\" You usually start collecting money from the most committed investors and work your way out toward the ambivalent ones, whose interest increases as the round fills up.But at each point you know how you're doing.  If investors turn cold you may have to raise less, but when investors in an angel round turn cold the process at least degrades gracefully, instead of blowing up in your face and leaving you with nothing, as happens if you get rejected by a VC fund after a full partner meeting. Whereas if investors seem hot, you can not only close the round faster, but now that convertible notes are becoming the norm, actually raise the price to reflect demand.ValuationHowever, the VCs have a weapon they can use against the super-angels, and they have started to use it.   VCs have started making angel-sized investments too.  The term \"angel round\" doesn't mean that all the investors in it are angels; it just describes the structure of the round.",
      "  Increasingly the participants include VCs making investments of a hundred thousand or two.  And when VCs invest in angel rounds they can do things that super-angels don't like.  VCs are quite valuation-insensitive in angel rounds\u2014partly because they are in general, and partly because they don't care that much about the returns on angel rounds, which they still view mostly as a way to recruit startups for series A rounds later.  So VCs who invest in angel rounds can blow up the valuations for angels and super-angels who invest in them.  [6]Some super-angels seem to care about valuations.  Several turned down YC-funded startups after Demo Day because their valuations were too high.  This was not a problem for the startups; by definition a high valuation means enough investors were willing to accept it. But it was mysterious to me that the super-angels would quibble about valuations.  Did they not understand that the big returns come from a few big successes, and that it therefore mattered far more which startups you picked than how much you paid for them?After thinking about it for a while and observing certain other signs, I have a theory that explains why the super-angels may be smarter than they seem.",
      "  It would make sense for super-angels to want low valuations if they're hoping to invest in startups that get bought early.  If you're hoping to hit the next Google, you shouldn't care if the valuation is 20 million.  But if you're looking for companies that are going to get bought for 30 million, you care. If you invest at 20 and the company gets bought for 30, you only get 1.5x.  You might as well buy Apple.So if some of the super-angels were looking for companies that could get acquired quickly, that would explain why they'd care about valuations.  But why would they be looking for those?   Because depending on the meaning of \"quickly,\" it could actually be very profitable.  A company that gets acquired for 30 million is a failure to a VC, but it could be a 10x return for an angel, and moreover, a quick 10x return.  Rate of return is what matters in investing\u2014not the multiple you get, but the multiple per year. If a super-angel gets 10x in one year, that's a higher rate of return than a VC could ever hope to get from a company that took 6 years to go public.",
      "  To get the same rate of return, the VC would have to get a multiple of 10^6\u2014one million x.  Even Google didn't come close to that.So I think at least some super-angels are looking for companies that will get bought.  That's the only rational explanation for focusing on getting the right valuations, instead of the right companies.  And if so they'll be different to deal with than VCs. They'll be tougher on valuations, but more accommodating if you want to sell early.PrognosisWho will win, the super-angels or the VCs?  I think the answer to that is, some of each.  They'll each become more like one another. The super-angels will start to invest larger amounts, and the VCs will gradually figure out ways to make more, smaller investments faster.  A decade from now the players will be hard to tell apart, and there will probably be survivors from each group.What does that mean for founders?  One thing it means is that the high valuations startups are presently getting may not last forever. To the extent that valuations are being driven up by price-insensitive VCs, they'll fall again if VCs become more like super-angels and start to become more miserly about valuations.",
      "  Fortunately if this does happen it will take years.The short term forecast is more competition between investors, which is good news for you.  The super-angels will try to undermine the VCs by acting faster, and the VCs will try to undermine the super-angels by driving up valuations.  Which for founders will result in the perfect combination: funding rounds that close fast, with high valuations.But remember that to get that combination, your startup will have to appeal to both super-angels and VCs.  If you don't seem like you have the potential to go public, you won't be able to use VCs to drive up the valuation of an angel round.There is a danger of having VCs in an angel round: the so-called signalling risk.  If VCs are only doing it in the hope of investing more later, what happens if they don't?  That's a signal to everyone else that they think you're lame.How much should you worry about that?  The seriousness of signalling risk depends on how far along you are.  If by the next time you need to raise money, you have graphs showing rising revenue or traffic month after month, you don't have to worry about any signals your existing investors are sending.",
      "  Your results will speak for themselves.   [7]Whereas if the next time you need to raise money you won't yet have concrete results, you may need to think more about the message your investors might send if they don't invest more.  I'm not sure yet how much you have to worry, because this whole phenomenon of VCs doing angel investments is so new. But my instincts tell me you don't have to worry much.  Signalling risk smells like one of those things founders worry about that's not a real problem.  As a rule, the only thing that can kill a good startup is the startup itself. Startups hurt themselves way more often than competitors hurt them, for example.  I suspect signalling risk is in this category too.One thing YC-funded startups have been doing to mitigate the risk of taking money from VCs in angel rounds is not to take too much from any one VC.  Maybe that will help, if you have the luxury of turning down money.Fortunately, more and more startups will.  After decades of competition that could best be described as intramural, the startup funding business is finally getting some real competition.  That should last several years at least, and maybe a lot longer. Unless there's some huge market crash,",
      " the next couple years are going to be a good time for startups to raise money.  And that's exciting because it means lots more startups will happen. Notes[1] I've also heard them called \"Mini-VCs\" and \"Micro-VCs.\" I don't know which name will stick.There were a couple predecessors.  Ron Conway had angel funds starting in the 1990s, and in some ways First Round Capital is closer to a super-angel than a VC fund.[2] It wouldn't cut their overall returns tenfold, because investing later would probably (a) cause them to lose less on investments that failed, and (b) not allow them to get as large a percentage of startups as they do now.  So it's hard to predict precisely what would happen to their returns.[3] The brand of an investor derives mostly from the success of their portfolio companies.  The top VCs thus have a big brand advantage over the super-angels.  They could make it self-perpetuating if they used it to get all the best new startups.  But I don't think they'll be able to.  To get all the best startups, you have to do more than make them want you.  You also have to want them;",
      " you have to recognize them when you see them, and that's much harder. Super-angels will snap up stars that VCs miss.  And that will cause the brand gap between the top VCs and the super-angels gradually to erode.[4] Though in a traditional series A round VCs put two partners on your board, there are signs now that VCs may begin to conserve board seats by switching to what used to be considered an angel-round board, consisting of two founders and one VC.  Which is also to the founders' advantage if it means they still control the company.[5] In a series A round, you usually have to give up more than the actual amount of stock the VCs buy, because they insist you dilute yourselves to set aside an \"option pool\" as well.  I predict this practice will gradually disappear though.[6] The best thing for founders, if they can get it, is a convertible note with no valuation cap at all.  In that case the money invested in the angel round just converts into stock at the valuation of the next round, no matter how large.  Angels and super-angels tend not to like uncapped notes. They have no idea how much of the company they're buying.",
      "  If the company does well and the valuation of the next round is high, they may end up with only a sliver of it.  So by agreeing to uncapped notes, VCs who don't care about valuations in angel rounds can make offers that super-angels hate to match.[7] Obviously signalling risk is also not a problem if you'll never need to raise more money.  But startups are often mistaken about that.Thanks to Sam Altman, John Bautista, Patrick Collison, James Lindenbaum, Reid Hoffman, Jessica Livingston and Harj Taggar for reading drafts of this.May 2004When people care enough about something to do it well, those who do it best tend to be far better than everyone else.  There's a huge gap between Leonardo and second-rate contemporaries like Borgognone.  You see the same gap between Raymond Chandler and the average writer of detective novels.  A top-ranked professional chess player could play ten thousand games against an ordinary club player without losing once.Like chess or painting or writing novels, making money is a very specialized skill.   But for some reason we treat this skill differently.  No one complains when a few people surpass all the rest at playing chess or writing novels, but when a few people make more money than the rest,",
      " we get editorials saying this is wrong.Why?  The pattern of variation seems no different than for any other skill.  What causes people to react so strongly when the skill is making money?I think there are three reasons we treat making money as different: the misleading model of wealth we learn as children; the disreputable way in which, till recently, most fortunes were accumulated; and the worry that great variations in income are somehow bad for society.  As far as I can tell, the first is mistaken, the second outdated, and the third empirically false.  Could it be that, in a modern democracy, variation in income is actually a sign of health?The Daddy Model of WealthWhen I was five I thought electricity was created by electric sockets.  I didn't realize there were power plants out there generating it.  Likewise, it doesn't occur to most kids that wealth is something that has to be generated.  It seems to be something that flows from parents.Because of the circumstances in which they encounter it, children tend to misunderstand wealth.  They confuse it with money.  They think that there is a fixed amount of it.  And they think of it as something that's distributed by authorities (and so should be distributed equally), rather than something that has to be created (and might be created unequally).In fact,",
      " wealth is not money.  Money is just a convenient way of trading one form of wealth for another.  Wealth is the underlying stuff\u2014the goods and services we buy.  When you travel to a rich or poor country, you don't have to look at people's bank accounts to tell which kind you're in.  You can see wealth\u2014in buildings and streets, in the clothes and the health of the people.Where does wealth come from?  People make it.  This was easier to grasp when most people lived on farms, and made many of the things they wanted with their own hands.  Then you could see in the house, the herds, and the granary the wealth that each family created.  It was obvious then too that the wealth of the world was not a fixed quantity that had to be shared out, like slices of a pie.  If you wanted more wealth, you could make it.This is just as true today, though few of us create wealth directly for ourselves (except for a few vestigial domestic tasks).  Mostly we create wealth for other people in exchange for money, which we then trade for the forms of wealth we want.  [1]Because kids are unable to create wealth, whatever they have has to be given to them.",
      "  And when wealth is something you're given, then of course it seems that it should be distributed equally. [2] As in most families it is.  The kids see to that.  \"Unfair,\" they cry, when one sibling gets more than another.In the real world, you can't keep living off your parents.  If you want something, you either have to make it, or do something of equivalent value for someone else, in order to get them to give you enough money to buy it.  In the real world, wealth is (except for a few specialists like thieves and speculators) something you have to create, not something that's distributed by Daddy.  And since the ability and desire to create it vary from person to person, it's not made equally.You get paid by doing or making something people want, and those who make more money are often simply better at doing what people want.  Top actors make a lot more money than B-list actors.  The B-list actors might be almost as charismatic, but when people go to the theater and look at the list of movies playing, they want that extra oomph that the big stars have.Doing what people want is not the only way to get money, of course. You could also rob banks,",
      " or solicit bribes, or establish a monopoly. Such tricks account for some variation in wealth, and indeed for some of the biggest individual fortunes, but they are not the root cause of variation in income.  The root cause of variation in income, as Occam's Razor implies, is the same as the root cause of variation in every other human skill.In the United States, the CEO of a large public company makes about 100 times as much as the average person.  [3] Basketball players make about 128 times as much, and baseball players 72 times as much. Editorials quote this kind of statistic with horror.  But I have no trouble imagining that one person could be 100 times as productive as another.  In ancient Rome the price of slaves varied by a factor of 50 depending on their skills.  [4] And that's without considering motivation, or the extra leverage in productivity that you can get from modern technology.Editorials about athletes' or CEOs' salaries remind me of early Christian writers, arguing from first principles about whether the Earth was round, when they could just walk outside and check. [5] How much someone's work is worth is not a policy question.  It's something the market already determines.\"Are they really worth 100 of us?\" editorialists ask.",
      "  Depends on what you mean by worth.  If you mean worth in the sense of what people will pay for their skills, the answer is yes, apparently.A few CEOs' incomes reflect some kind of wrongdoing.  But are there not others whose incomes really do reflect the wealth they generate? Steve Jobs saved a company that was in a terminal decline.  And not merely in the way a turnaround specialist does, by cutting costs; he had to decide what Apple's next products should be.  Few others could have done it.  And regardless of the case with CEOs, it's hard to see how anyone could argue that the salaries of professional basketball players don't reflect supply and demand.It may seem unlikely in principle that one individual could really generate so much more wealth than another.  The key to this mystery is to revisit that question, are they really worth 100 of us? Would a basketball team trade one of their players for 100 random people?  What would Apple's next product look like if you replaced Steve Jobs with a committee of 100 random people?  [6] These things don't scale linearly.  Perhaps the CEO or the professional athlete has only ten times (whatever that means) the skill and determination of an ordinary person.  But it makes all the difference that it's concentrated in one individual.When we say that one kind of work is overpaid and another underpaid,",
      " what are we really saying?  In a free market, prices are determined by what buyers want.  People like baseball more than  poetry, so baseball players make more than poets.  To say that a certain kind of work is underpaid is thus identical with saying that people want the wrong things.Well, of course people want the wrong things.  It seems odd to be surprised by that.  And it seems even odder to say that it's unjust that certain kinds of work are underpaid.  [7] Then you're saying that it's unjust that people want the wrong things. It's  lamentable that people prefer reality TV and corndogs to Shakespeare and steamed vegetables, but unjust?  That seems like saying that blue is heavy, or that up is circular.The appearance of the word \"unjust\" here is the unmistakable spectral signature of the Daddy Model.  Why else would this idea occur in this odd context?  Whereas if the speaker were still operating on the Daddy Model, and saw wealth as something that flowed from a common source and had to be shared out, rather than something generated by doing what other people wanted, this is exactly what you'd get on noticing that some people made much more than others.When we talk about \"unequal distribution of income,\" we should also ask,",
      " where does that income come from? [8] Who made the wealth it represents?  Because to the extent that income varies simply according to how much wealth people create, the distribution may be unequal, but it's hardly unjust.Stealing ItThe second reason we tend to find great disparities of wealth alarming is that for most of human history the usual way to accumulate a fortune was to steal it: in pastoral societies by cattle raiding; in agricultural societies by appropriating others' estates in times of war, and taxing them in times of peace.In conflicts, those on the winning side would receive the estates confiscated from the losers.  In England in the 1060s, when William the Conqueror distributed the estates of the defeated Anglo-Saxon nobles to his followers, the conflict was military.  By the 1530s, when Henry VIII distributed the estates of the monasteries to his followers, it was mostly political.  [9] But the principle was the same.  Indeed, the same principle is at work now in Zimbabwe.In more organized societies, like China, the ruler and his officials used taxation instead of confiscation.  But here too we see the same principle: the way to get rich was not to create wealth, but to serve a ruler powerful enough to appropriate it.This started to change in Europe with the rise of the middle class.",
      " Now we think of the middle class as people who are neither rich nor poor, but originally they were a distinct group.  In a feudal society, there are just two classes: a warrior aristocracy, and the serfs who work their estates.  The middle class were a new, third group who lived in towns and supported themselves by manufacturing and trade.Starting in the tenth and eleventh centuries, petty nobles and former serfs banded together in towns that gradually became powerful enough to ignore the local feudal lords.  [10] Like serfs, the middle class made a living largely by creating wealth.  (In port cities like Genoa and Pisa, they also engaged in piracy.) But unlike serfs they had an incentive to create a lot of it.  Any wealth a serf created belonged to his master.  There was not much point in making more than you could hide.  Whereas the independence of the townsmen allowed them to keep whatever wealth they created.Once it became possible to get rich by creating wealth, society as a whole started to get richer very rapidly.  Nearly everything we have was created by the middle class.  Indeed, the other two classes have effectively disappeared in industrial societies, and their names been given to either end of the middle class.",
      "  (In the original sense of the word, Bill Gates is middle class.)But it was not till the Industrial Revolution that wealth creation definitively replaced corruption as the best way to get rich.  In England, at least, corruption only became unfashionable (and in fact only started to be called \"corruption\") when there started to be other, faster ways to get rich.Seventeenth-century England was much like the third world today, in that government office was a recognized route to wealth.  The great fortunes of that time still derived more from what we would now call corruption than from commerce.  [11] By the nineteenth century that had changed.  There continued to be bribes, as there still are everywhere, but politics had by then been left to men who were driven more by vanity than greed.  Technology had made it possible to create wealth faster than you could steal it.  The prototypical rich man of the nineteenth century was not a courtier but an industrialist.With the rise of the middle class, wealth stopped being a zero-sum game.  Jobs and Wozniak didn't have to make us poor to make themselves rich.  Quite the opposite: they created things that made our lives materially richer.  They had to, or we wouldn't have paid for them.But since for most of the world's history the main route to wealth was to steal it,",
      " we tend to be suspicious of rich people.  Idealistic undergraduates find their unconsciously preserved child's model of wealth confirmed by eminent writers of the past.  It is a case of the mistaken meeting the outdated.\"Behind every great fortune, there is a crime,\" Balzac wrote.  Except he didn't.  What he actually said was that a great fortune with no apparent cause was probably due to a crime well enough executed that it had been forgotten.  If we were talking about Europe in 1000, or most of the third world today, the standard misquotation would be spot on.  But Balzac lived in nineteenth-century France, where the Industrial Revolution was well advanced.  He knew you could make a fortune without stealing it The most important factor in achieving success is persistence and hard work..  After all, he did himself, as a popular novelist. [12]Only a few countries (by no coincidence, the richest ones) have reached this stage.  In most, corruption still has the upper hand. In most, the fastest way to get wealth is by stealing it.  And so when we see increasing differences in income in a rich country, there is a tendency to worry that it's sliding back toward becoming another Venezuela.  I think the opposite is happening. I think you're seeing a country a full step ahead of Venezuela.The Lever of TechnologyWill technology increase the gap between rich and poor?",
      "  It will certainly increase the gap between the productive and the unproductive. That's the whole point of technology.   With a tractor an energetic farmer could plow six times as much land in a day as he could with a team of horses.  But only if he mastered a new kind of farming.I've seen the lever of technology grow visibly in my own time.  In high school I made money by mowing lawns and scooping ice cream at Baskin-Robbins.  This was the only kind of work available at the time.  Now high school kids could write software or design web sites.  But only some of them will; the rest will still be scooping ice cream.I remember very vividly when in 1985 improved technology made it possible for me to buy a computer of my own.  Within months I was using it to make money as a freelance programmer.  A few years before, I couldn't have done this.  A few years before, there was no such thing as a freelance programmer.  But Apple created wealth, in the form of powerful, inexpensive computers, and programmers immediately set to work using it to create more.As this example suggests, the rate at which technology increases our productive capacity is probably exponential, rather than linear.",
      " So we should expect to see ever-increasing variation in individual productivity as time goes on.   Will that increase the gap between rich and the poor?  Depends which gap you mean.Technology should increase the gap in income, but it seems to decrease other gaps.  A hundred years ago, the rich led a different kind of life from ordinary people.  They lived in houses full of servants, wore elaborately uncomfortable clothes, and travelled about in carriages drawn by teams of horses which themselves required their own houses and servants.  Now, thanks to technology, the rich live more like the average person.Cars are a good example of why.  It's possible to buy expensive, handmade cars that cost hundreds of thousands of dollars.  But there is not much point.  Companies make more money by building a large number of ordinary cars than a small number of expensive ones.  So a company making a mass-produced car can afford to spend a lot more on its design.  If you buy a custom-made car, something will always be breaking.  The only point of buying one now is to advertise that you can.Or consider watches.  Fifty years ago, by spending a lot of money on a watch you could get better performance.  When watches had mechanical movements,",
      " expensive watches kept better time.  Not any more.  Since the invention of the quartz movement, an ordinary Timex is more accurate than a Patek Philippe costing hundreds of thousands of dollars. [13] Indeed, as with expensive cars, if you're determined to spend a lot of money on a watch, you have to put up with some inconvenience to do it: as well as keeping worse time, mechanical watches have to be wound.The only thing technology can't cheapen is brand.  Which is precisely why we hear ever more about it.  Brand is the residue left as the substantive differences between rich and poor evaporate.  But what label you have on your stuff is a much smaller matter than having it versus not having it.  In 1900, if you kept a carriage, no one asked what year or brand it was.  If you had one, you were rich. And if you weren't rich, you took the omnibus or walked.  Now even the poorest Americans drive cars, and it is only because we're so well trained by advertising that we can even recognize the especially expensive ones. [14]The same pattern has played out in industry after industry.  If there is enough demand for something, technology will make it cheap enough to sell in large volumes,",
      " and the mass-produced versions will be, if not better, at least more convenient. [15] And there is nothing the rich like more than convenience.  The rich people I know drive the same cars, wear the same clothes, have the same kind of furniture, and eat the same foods as my other friends.  Their houses are in different neighborhoods, or if in the same neighborhood are different sizes, but within them life is similar.  The houses are made using the same construction techniques and contain much the same objects.  It's inconvenient to do something expensive and custom.The rich spend their time more like everyone else too.  Bertie Wooster seems long gone.  Now, most people who are rich enough not to work do anyway.  It's not just social pressure that makes them; idleness is lonely and demoralizing.Nor do we have the social distinctions there were a hundred years ago.   The novels and etiquette manuals of that period read now like descriptions of some strange tribal society.  \"With respect to the continuance of friendships...\" hints Mrs. Beeton's Book of Household Management (1880), \"it may be found necessary, in some cases, for a mistress to relinquish, on assuming the responsibility of a household, many of those commenced in the earlier part of her life.\" A woman who married a rich man was expected to drop friends who didn't.",
      "  You'd seem a barbarian if you behaved that way today. You'd also have a very boring life.  People still tend to segregate themselves somewhat, but much more on the basis of education than wealth. [16]Materially and socially, technology seems to be decreasing the gap between the rich and the poor, not increasing it.  If Lenin walked around the offices of a company like Yahoo or Intel or Cisco, he'd think communism had won.  Everyone would be wearing the same clothes, have the same kind of office (or rather, cubicle) with the same furnishings, and address one another by their first names instead of by honorifics.  Everything would seem exactly as he'd predicted, until he looked at their bank accounts.  Oops.Is it a problem if technology increases that gap?  It doesn't seem to be so far.  As it increases the gap in income, it seems to decrease most other gaps.Alternative to an AxiomOne often hears a policy criticized on the grounds that it would increase the income gap between rich and poor.  As if it were an axiom that this would be bad.  It might be true that increased variation in income would be bad, but I don't see how we can say it's axiomatic.",
      "Indeed, it may even be false, in industrial democracies.  In a society of serfs and warlords, certainly, variation in income is a sign of an underlying problem.  But serfdom is not the only cause of variation in income.  A 747 pilot doesn't make 40 times as much as a checkout clerk because he is a warlord who somehow holds her in thrall.  His skills are simply much more valuable.I'd like to propose an alternative idea: that in a modern society, increasing variation in income is a sign of health.  Technology seems to increase the variation in productivity at faster than linear rates.  If we don't see corresponding variation in income, there are three possible explanations: (a) that technical innovation has stopped, (b) that the people who would create the most wealth aren't doing it, or (c) that they aren't getting paid for it.I think we can safely say that (a) and (b) would be bad.  If you disagree, try living for a year using only the resources available to the average Frankish nobleman in 800, and report back to us. (I'll be generous and not send you back to the stone age.)The only option, if you're going to have an increasingly prosperous society without increasing variation in income,",
      " seems to be (c), that people will create a lot of wealth without being paid for it. That Jobs and Wozniak, for example, will cheerfully work 20-hour days to produce the Apple computer for a society that allows them, after taxes, to keep just enough of their income to match what they would have made working 9 to 5 at a big company.Will people create wealth if they can't get paid for it?  Only if it's fun.  People will write operating systems for free.  But they won't install them, or take support calls, or train customers to use them.  And at least 90% of the work that even the highest tech companies do is of this second, unedifying kind.All the unfun kinds of wealth creation slow dramatically in a society that confiscates private fortunes.  We can confirm this empirically. Suppose you hear a strange noise that you think may be due to a nearby fan.  You turn the fan off, and the noise stops.  You turn the fan back on, and the noise starts again.  Off, quiet.  On, noise.  In the absence of other information, it would seem the noise is caused by the fan.At various times and places in history,",
      " whether you could accumulate a fortune by creating wealth has been turned on and off.  Northern Italy in 800, off (warlords would steal it).  Northern Italy in 1100, on.  Central France in 1100, off (still feudal).  England in 1800, on.  England in 1974, off (98% tax on investment income). United States in 1974, on.  We've even had a twin study: West Germany, on;  East Germany, off.  In every case, the creation of wealth seems to appear and disappear like the noise of a fan as you switch on and off the prospect of keeping it.There is some momentum involved.  It probably takes at least a generation to turn people into East Germans (luckily for England). But if it were merely a fan we were studying, without all the extra baggage that comes from the controversial topic of wealth, no one would have any doubt that the fan was causing the noise.If you suppress variations in income, whether by stealing private fortunes, as feudal rulers used to do, or by taxing them away, as some modern governments have done, the result always seems to be the same.    Society as a whole ends up poorer.If I had a choice of living in a society where I was materially much better off than I am now,",
      " but was among the poorest, or in one where I was the richest, but much worse off than I am now, I'd take the first option.  If I had children, it would arguably be immoral not to.  It's absolute poverty you want to avoid, not relative poverty.  If, as the evidence so far implies, you have to have one or the other in your society, take relative poverty.You need rich people in your society not so much because in spending their money they create jobs, but because of what they have to do to get rich.  I'm not talking about the trickle-down effect here.  I'm not saying that if you let Henry Ford get rich, he'll hire you as a waiter at his next party.  I'm saying that he'll make you a tractor to replace your horse.Notes[1] Part of the reason this subject is so contentious is that some of those most vocal on the subject of wealth\u2014university students, heirs, professors, politicians, and journalists\u2014have the least experience creating it.  (This phenomenon will be familiar to anyone who has overheard conversations about sports in a bar.)Students are mostly still on the parental dole, and have not stopped to think about where that money comes from.",
      "  Heirs will be on the parental dole for life.  Professors and politicians live within socialist eddies of the economy, at one remove from the creation of wealth, and are paid a flat rate regardless of how hard they work.  And journalists as part of their professional code segregate themselves from the revenue-collecting half of the businesses they work for (the ad sales department).  Many of these people never come face to face with the fact that the money they receive represents wealth\u2014wealth that, except in the case of journalists, someone else created earlier.  They live in a world in which income is doled out by a central authority according to some abstract notion of fairness (or randomly, in the case of heirs), rather than given by other people in return for something they wanted, so it may seem to them unfair that things don't work the same in the rest of the economy.(Some professors do create a great deal of wealth for society.  But the money they're paid isn't a quid pro quo. It's more in the nature of an investment.)[2] When one reads about the origins of the Fabian Society, it sounds like something cooked up by the high-minded Edwardian child-heroes of Edith Nesbit's The Wouldbegoods.[3]",
      " According to a study by the Corporate Library, the median total compensation, including salary, bonus, stock grants, and the exercise of stock options, of S&P 500 CEOs in 2002 was $3.65 million. According to Sports Illustrated, the average NBA player's salary during the 2002-03 season was $4.54 million, and the average major league baseball player's salary at the start of the 2003 season was $2.56 million.  According to the Bureau of Labor Statistics, the mean annual wage in the US in 2002 was $35,560.[4] In the early empire the price of an ordinary adult slave seems to have been about 2,000 sestertii (e.g. Horace, Sat. ii.7.43). A servant girl cost 600 (Martial vi.66), while Columella (iii.3.8) says that a skilled vine-dresser was worth 8,000.  A doctor, P. Decimus Eros Merula, paid 50,000 sestertii for his freedom (Dessau, Inscriptiones 7812).  Seneca (Ep. xxvii.7) reports that one Calvisius Sabinus paid 100,",
      "000 sestertii apiece for slaves learned in the Greek classics.  Pliny (Hist. Nat. vii.39) says that the highest price paid for a slave up to his time was 700,000 sestertii, for the linguist (and presumably teacher) Daphnis, but that this had since been exceeded by actors buying their own freedom.Classical Athens saw a similar variation in prices.  An ordinary laborer was worth about 125 to 150 drachmae.  Xenophon (Mem. ii.5) mentions prices ranging from 50 to 6,000 drachmae (for the manager of a silver mine).For more on the economics of ancient slavery see:Jones, A. H. M., \"Slavery in the Ancient World,\" Economic History Review, 2:9 (1956), 185-199, reprinted in Finley, M. I. (ed.), Slavery in Classical Antiquity, Heffer, 1964.[5] Eratosthenes (276\u2014195 BC) used shadow lengths in different cities to estimate the Earth's circumference.  He was off by only about 2%.[6] No, and Windows, respectively.[7] One of the biggest divergences between the Daddy Model and reality is the valuation of hard work.",
      "  In the Daddy Model, hard work is in itself deserving.  In reality, wealth is measured by what one delivers, not how much effort it costs.  If I paint someone's house, the owner shouldn't pay me extra for doing it with a toothbrush.It will seem to someone still implicitly operating on the Daddy Model that it is unfair when someone works hard and doesn't get paid much.  To help clarify the matter, get rid of everyone else and put our worker on a desert island, hunting and gathering fruit. If he's bad at it he'll work very hard and not end up with much food.  Is this unfair?  Who is being unfair to him?[8] Part of the reason for the tenacity of the Daddy Model may be the dual meaning of \"distribution.\" When economists talk about \"distribution of income,\" they mean statistical distribution.  But when you use the phrase frequently, you can't help associating it with the other sense of the word (as in e.g. \"distribution of alms\"), and thereby subconsciously seeing wealth as something that flows from some central tap.  The word \"regressive\" as applied to tax rates has a similar effect, at least on me; how can anything regressive be good?[9]",
      " \"From the beginning of the reign Thomas Lord Roos was an assiduous courtier of the young Henry VIII and was soon to reap the rewards. In 1525 he was made a Knight of the Garter and given the Earldom of Rutland.  In the thirties his support of the breach with Rome, his zeal in crushing the Pilgrimage of Grace, and his readiness to vote the death-penalty in the succession of spectacular treason trials that punctuated Henry's erratic matrimonial progress made him an obvious candidate for grants of monastic property.\"Stone, Lawrence, Family and Fortune: Studies in Aristocratic Finance in the Sixteenth and Seventeenth Centuries, Oxford University Press, 1973, p. 166.[10] There is archaeological evidence for large settlements earlier, but it's hard to say what was happening in them.Hodges, Richard and David Whitehouse, Mohammed, Charlemagne and the Origins of Europe, Cornell University Press, 1983.[11] William Cecil and his son Robert were each in turn the most powerful minister of the crown, and both used their position to amass fortunes among the largest of their times.  Robert in particular took bribery to the point of treason.  \"As Secretary of State and the leading advisor to King James on foreign policy,",
      " [he] was a special recipient of favour, being offered large bribes by the Dutch not to make peace with Spain, and large bribes by Spain to make peace.\" (Stone, op. cit., p. 17.)[12] Though Balzac made a lot of money from writing, he was notoriously improvident and was troubled by debts all his life.[13] A Timex will gain or lose about.5 seconds per day.  The most accurate mechanical watch, the Patek Philippe 10 Day Tourbillon, is rated at -1.5 to +2 seconds.  Its retail price is about $220,000.[14] If asked to choose which was more expensive, a well-preserved 1989 Lincoln Town Car ten-passenger limousine ($5,000) or a 2004 Mercedes S600 sedan ($122,000), the average Edwardian might well guess wrong.[15] To say anything meaningful about income trends, you have to talk about real income, or income as measured in what it can buy. But the usual way of calculating real income ignores much of the growth in wealth over time, because it depends on a consumer price index created by bolting end to end a series of numbers that are only locally accurate,",
      " and that don't include the prices of new inventions until they become so common that their prices stabilize.So while we might think it was very much better to live in a world with antibiotics or air travel or an electric power grid than without, real income statistics calculated in the usual way will prove to us that we are only slightly richer for having these things.Another approach would be to ask, if you were going back to the year x in a time machine, how much would you have to spend on trade goods to make your fortune?  For example, if you were going back to 1970 it would certainly be less than $500, because the processing power you can get for $500 today would have been worth at least $150 million in 1970.  The function goes asymptotic fairly quickly, because for times over a hundred years or so you could get all you needed in present-day trash.  In 1800 an empty plastic drink bottle with a screw top would have seemed a miracle of workmanship.[16] Some will say this amounts to the same thing, because the rich have better opportunities for education.  That's a valid point.  It is still possible, to a degree, to buy your kids' way into top colleges by sending them to private schools that in effect hack the college admissions process.According to a 2002 report by the National Center for Education Statistics,",
      " about 1.7% of American kids attend private, non-sectarian schools.  At Princeton, 36% of the class of 2007 came from such schools.  (Interestingly, the number at Harvard is significantly lower, about 28%.)  Obviously this is a huge loophole.  It does at least seem to be closing, not widening.Perhaps the designers of admissions processes should take a lesson from the example of computer security, and instead of just assuming that their system can't be hacked, measure the degree to which it is.February 2007A few days ago I finally figured out something I've wondered about for 25 years: the relationship between wisdom and intelligence. Anyone can see they're not the same by the number of people who are smart, but not very wise.  And yet intelligence and wisdom do seem related.  How?What is wisdom?  I'd say it's knowing what to do in a lot of situations.  I'm not trying to make a deep point here about the true nature of wisdom, just to figure out how we use the word.  A wise person is someone who usually knows the right thing to do.And yet isn't being smart also knowing what to do in certain situations?  For example,",
      " knowing what to do when the teacher tells your elementary school class to add all the numbers from 1 to 100? [1]Some say wisdom and intelligence apply to different types of problems\u2014wisdom to human problems and intelligence to abstract ones.  But that isn't true.  Some wisdom has nothing to do with people: for example, the wisdom of the engineer who knows certain structures are less prone to failure than others.  And certainly smart people can find clever solutions to human problems as well as abstract ones.  [2]Another popular explanation is that wisdom comes from experience while intelligence is innate.  But people are not simply wise in proportion to how much experience they have.  Other things must contribute to wisdom besides experience, and some may be innate: a reflective disposition, for example.Neither of the conventional explanations of the difference between wisdom and intelligence stands up to scrutiny.  So what is the difference?  If we look at how people use the words \"wise\" and \"smart,\" what they seem to mean is different shapes of performance.Curve\"Wise\" and \"smart\" are both ways of saying someone knows what to do.  The difference is that \"wise\" means one has a high average outcome across all situations, and \"smart\" means one does spectacularly well in a few.",
      "  That is, if you had a graph in which the x axis represented situations and the y axis the outcome, the graph of the wise person would be high overall, and the graph of the smart person would have high peaks.The distinction is similar to the rule that one should judge talent at its best and character at its worst.  Except you judge intelligence at its best, and wisdom by its average.  That's how the two are related: they're the two different senses in which the same curve can be high.So a wise person knows what to do in most situations, while a smart person knows what to do in situations where few others could.  We need to add one more qualification: we should ignore cases where someone knows what to do because they have inside information.  [3] But aside from that, I don't think we can get much more specific without starting to be mistaken.Nor do we need to.  Simple as it is, this explanation predicts, or at least accords with, both of the conventional stories about the distinction between wisdom and intelligence.  Human problems are the most common type, so being good at solving those is key in achieving a high average outcome.   And it seems natural that a high average outcome depends mostly on experience, but that dramatic peaks can only be achieved by people with certain rare,",
      " innate qualities; nearly anyone can learn to be a good swimmer, but to be an Olympic swimmer you need a certain body type.This explanation also suggests why wisdom is such an elusive concept: there's no such thing.  \"Wise\" means something\u2014that one is on average good at making the right choice.  But giving the name \"wisdom\" to the supposed quality that enables one to do that doesn't mean such a thing exists.  To the extent \"wisdom\" means anything, it refers to a grab-bag of qualities as various as self-discipline, experience, and empathy.   [4]Likewise, though \"intelligent\" means something, we're asking for trouble if we insist on looking for a single thing called \"intelligence.\" And whatever its components, they're not all innate.  We use the word \"intelligent\" as an indication of ability: a smart person can grasp things few others could.  It does seem likely there's some inborn predisposition to intelligence (and wisdom too), but this predisposition is not itself intelligence.One reason we tend to think of intelligence as inborn is that people trying to measure it have concentrated on the aspects of it that are most measurable.  A quality that's inborn will obviously be more convenient to work with than one that's influenced by experience,",
      " and thus might vary in the course of a study.  The problem comes when we drag the word \"intelligence\" over onto what they're measuring. If they're measuring something inborn, they can't be measuring intelligence.  Three year olds aren't smart.   When we describe one as smart, it's shorthand for \"smarter than other three year olds.\"SplitPerhaps it's a technicality to point out that a predisposition to intelligence is not the same as intelligence.  But it's an important technicality, because it reminds us that we can become smarter, just as we can become wiser.The alarming thing is that we may have to choose between the two.If wisdom and intelligence are the average and peaks of the same curve, then they converge as the number of points on the curve decreases.  If there's just one point, they're identical: the average and maximum are the same.  But as the number of points increases, wisdom and intelligence diverge.  And historically the number of points on the curve seems to have been increasing: our ability is tested in an ever wider range of situations.In the time of Confucius and Socrates, people seem to have regarded wisdom, learning, and intelligence as more closely related than we do.  Distinguishing between \"wise\"",
      " and \"smart\" is a modern habit. [5] And the reason we do is that they've been diverging.  As knowledge gets more specialized, there are more points on the curve, and the distinction between the spikes and the average becomes sharper, like a digital image rendered with more pixels.One consequence is that some old recipes may have become obsolete. At the very least we have to go back and figure out if they were really recipes for wisdom or intelligence.  But the really striking change, as intelligence and wisdom drift apart, is that we may have to decide which we prefer.  We may not be able to optimize for both simultaneously.Society seems to have voted for intelligence.  We no longer admire the sage\u2014not the way people did two thousand years ago.  Now we admire the genius.  Because in fact the distinction we began with has a rather brutal converse: just as you can be smart without being very wise, you can be wise without being very smart.  That doesn't sound especially admirable.  That gets you James Bond, who knows what to do in a lot of situations, but has to rely on Q for the ones involving math.Intelligence and wisdom are obviously not mutually exclusive.  In fact, a high average may help support high peaks.",
      "  But there are reasons to believe that at some point you have to choose between them.  One is the example of very smart people, who are so often unwise that in popular culture this now seems to be regarded as the rule rather than the exception.  Perhaps the absent-minded professor is wise in his way, or wiser than he seems, but he's not wise in the way Confucius or Socrates wanted people to be.  [6]NewFor both Confucius and Socrates, wisdom, virtue, and happiness were necessarily related.  The wise man was someone who knew what the right choice was and always made it; to be the right choice, it had to be morally right; he was therefore always happy, knowing he'd done the best he could.  I can't think of many ancient philosophers who would have disagreed with that, so far as it goes.\"The superior man is always happy; the small man sad,\" said Confucius. [7]Whereas a few years ago I read an interview with a mathematician who said that most nights he went to bed discontented, feeling he hadn't made enough progress.   [8] The Chinese and Greek words we translate as \"happy\" didn't mean exactly what we do by it,",
      " but there's enough overlap that this remark contradicts them.Is the mathematician a small man because he's discontented?  No; he's just doing a kind of work that wasn't very common in Confucius's day.Human knowledge seems to grow fractally.  Time after time, something that seemed a small and uninteresting area\u2014experimental error, even\u2014turns out, when examined up close, to have as much in it as all knowledge up to that point.  Several of the fractal buds that have exploded since ancient times involve inventing and discovering new things.  Math, for example, used to be something a handful of people did part-time.  Now it's the career of thousands. And in work that involves making new things, some old rules don't apply.Recently I've spent some time advising people, and there I find the ancient rule still works: try to understand the situation as well as you can, give the best advice you can based on your experience, and then don't worry about it, knowing you did all you could.  But I don't have anything like this serenity when I'm writing an essay. Then I'm worried.  What if I run out of ideas?  And when I'm writing,",
      " four nights out of five I go to bed discontented, feeling I didn't get enough done.Advising people and writing are fundamentally different types of work.  When people come to you with a problem and you have to figure out the right thing to do, you don't (usually) have to invent anything.  You just weigh the alternatives and try to judge which is the prudent choice.  But prudence can't tell me what sentence to write next.  The search space is too big.Someone like a judge or a military officer can in much of his work be guided by duty, but duty is no guide in making things.  Makers depend on something more precarious: inspiration.  And like most people who lead a precarious existence, they tend to be worried, not contented.  In that respect they're more like the small man of Confucius's day, always one bad harvest (or ruler) away from starvation. Except instead of being at the mercy of weather and officials, they're at the mercy of their own imagination.LimitsTo me it was a relief just to realize it might be ok to be discontented. The idea that a successful person should be happy has thousands of years of momentum behind it.  If I was any good,",
      " why didn't I have the easy confidence winners are supposed to have?  But that, I now believe, is like a runner asking \"If I'm such a good athlete, why do I feel so tired?\" Good runners still get tired; they just get tired at higher speeds.People whose work is to invent or discover things are in the same position as the runner.  There's no way for them to do the best they can, because there's no limit to what they could do.  The closest you can come is to compare yourself to other people.  But the better you do, the less this matters.  An undergrad who gets something published feels like a star.  But for someone at the top of the field, what's the test of doing well?  Runners can at least compare themselves to others doing exactly the same thing; if you win an Olympic gold medal, you can be fairly content, even if you think you could have run a bit faster.  But what is a novelist to do?Whereas if you're doing the kind of work in which problems are presented to you and you have to choose between several alternatives, there's an upper bound on your performance: choosing the best every time.  In ancient societies, nearly all work seems to have been of this type.",
      "  The peasant had to decide whether a garment was worth mending, and the king whether or not to invade his neighbor, but neither was expected to invent anything.  In principle they could have; the king could have invented firearms, then invaded his neighbor.  But in practice innovations were so rare that they weren't expected of you, any more than goalkeepers are expected to score goals.  [9] In practice, it seemed as if there was a correct decision in every situation, and if you made it you'd done your job perfectly, just as a goalkeeper who prevents the other team from scoring is considered to have played a perfect game.In this world, wisdom seemed paramount.   [10] Even now, most people do work in which problems are put before them and they have to choose the best alternative.  But as knowledge has grown more specialized, there are more and more types of work in which people have to make up new things, and in which performance is therefore unbounded.  Intelligence has become increasingly important relative to wisdom because there is more room for spikes.RecipesAnother sign we may have to choose between intelligence and wisdom is how different their recipes are.  Wisdom seems to come largely from curing childish qualities, and intelligence largely from cultivating them.Recipes for wisdom,",
      " particularly ancient ones, tend to have a remedial character.  To achieve wisdom one must cut away all the debris that fills one's head on emergence from childhood, leaving only the important stuff.  Both self-control and experience have this effect: to eliminate the random biases that come from your own nature and from the circumstances of your upbringing respectively. That's not all wisdom is, but it's a large part of it.  Much of what's in the sage's head is also in the head of every twelve year old.  The difference is that in the head of the twelve year old it's mixed together with a lot of random junk.The path to intelligence seems to be through working on hard problems. You develop intelligence as you might develop muscles, through exercise.  But there can't be too much compulsion here.  No amount of discipline can replace genuine curiosity.  So cultivating intelligence seems to be a matter of identifying some bias in one's character\u2014some tendency to be interested in certain types of things\u2014and nurturing it.  Instead of obliterating your idiosyncrasies in an effort to make yourself a neutral vessel for the truth, you select one and try to grow it from a seedling into a tree.The wise are all much alike in their wisdom,",
      " but very smart people tend to be smart in distinctive ways.Most of our educational traditions aim at wisdom. So perhaps one reason schools work badly is that they're trying to make intelligence using recipes for wisdom.  Most recipes for wisdom have an element of subjection.  At the very least, you're supposed to do what the teacher says.  The more extreme recipes aim to break down your individuality the way basic training does.  But that's not the route to intelligence.  Whereas wisdom comes through humility, it may actually help, in cultivating intelligence, to have a mistakenly high opinion of your abilities, because that encourages you to keep working.  Ideally till you realize how mistaken you were.(The reason it's hard to learn new skills late in life is not just that one's brain is less malleable.  Another probably even worse obstacle is that one has higher standards.)I realize we're on dangerous ground here.  I'm not proposing the primary goal of education should be to increase students' \"self-esteem.\" That just breeds laziness.  And in any case, it doesn't really fool the kids, not the smart ones.  They can tell at a young age that a contest where everyone wins is a fraud.A teacher has to walk a narrow path:",
      " you want to encourage kids to come up with things on their own, but you can't simply applaud everything they produce.  You have to be a good audience: appreciative, but not too easily impressed.  And that's a lot of work.  You have to have a good enough grasp of kids' capacities at different ages to know when to be surprised.That's the opposite of traditional recipes for education.  Traditionally the student is the audience, not the teacher; the student's job is not to invent, but to absorb some prescribed body of material.  (The use of the term \"recitation\" for sections in some colleges is a fossil of this.) The problem with these old traditions is that they're too much influenced by recipes for wisdom.DifferentI deliberately gave this essay a provocative title; of course it's worth being wise.  But I think it's important to understand the relationship between intelligence and wisdom, and particularly what seems to be the growing gap between them.  That way we can avoid applying rules and standards to intelligence that are really meant for wisdom.  These two senses of \"knowing what to do\" are more different than most people realize.  The path to wisdom is through discipline, and the path to intelligence through carefully selected self-indulgence.",
      "  Wisdom is universal, and intelligence idiosyncratic. And while wisdom yields calmness, intelligence much of the time leads to discontentment.That's particularly worth remembering.  A physicist friend recently told me half his department was on Prozac.  Perhaps if we acknowledge that some amount of frustration is inevitable in certain kinds of work, we can mitigate its effects.  Perhaps we can box it up and put it away some of the time, instead of letting it flow together with everyday sadness to produce what seems an alarmingly large pool.  At the very least, we can avoid being discontented about being discontented.If you feel exhausted, it's not necessarily because there's something wrong with you.  Maybe you're just running fast.Notes[1] Gauss was supposedly asked this when he was 10.  Instead of laboriously adding together the numbers like the other students, he saw that they consisted of 50 pairs that each summed to 101 (100 + 1, 99 + 2, etc), and that he could just multiply 101 by 50 to get the answer, 5050.[2] A variant is that intelligence is the ability to solve problems, and wisdom the judgement to know how to use those solutions.   But while this is certainly an important relationship between wisdom and intelligence,",
      " it's not the distinction between them.  Wisdom is useful in solving problems too, and intelligence can help in deciding what to do with the solutions.[3] In judging both intelligence and wisdom we have to factor out some knowledge. People who know the combination of a safe will be better at opening it than people who don't, but no one would say that was a test of intelligence or wisdom.But knowledge overlaps with wisdom and probably also intelligence. A knowledge of human nature is certainly part of wisdom.  So where do we draw the line?Perhaps the solution is to discount knowledge that at some point has a sharp drop in utility.  For example, understanding French will help you in a large number of situations, but its value drops sharply as soon as no one else involved knows French.  Whereas the value of understanding vanity would decline more gradually.The knowledge whose utility drops sharply is the kind that has little relation to other knowledge.  This includes mere conventions, like languages and safe combinations, and also what we'd call \"random\" facts, like movie stars' birthdays, or how to distinguish 1956 from 1957 Studebakers.[4] People seeking some single thing called \"wisdom\" have been fooled by grammar.  Wisdom is just knowing the right thing to do,",
      " and there are a hundred and one different qualities that help in that.  Some, like selflessness, might come from meditating in an empty room, and others, like a knowledge of human nature, might come from going to drunken parties.Perhaps realizing this will help dispel the cloud of semi-sacred mystery that surrounds wisdom in so many people's eyes.  The mystery comes mostly from looking for something that doesn't exist.  And the reason there have historically been so many different schools of thought about how to achieve wisdom is that they've focused on different components of it.When I use the word \"wisdom\" in this essay, I mean no more than whatever collection of qualities helps people make the right choice in a wide variety of situations.[5] Even in English, our sense of the word \"intelligence\" is surprisingly recent.  Predecessors like \"understanding\" seem to have had a broader meaning.[6] There is of course some uncertainty about how closely the remarks attributed to Confucius and Socrates resemble their actual opinions. I'm using these names as we use the name \"Homer,\" to mean the hypothetical people who said the things attributed to them.[7] Analects VII:36, Fung trans.Some translators use \"calm\"",
      " instead of \"happy.\"  One source of difficulty here is that present-day English speakers have a different idea of happiness from many older societies.  Every language probably has a word meaning \"how one feels when things are going well,\" but different cultures react differently when things go well.  We react like children, with smiles and laughter.  But in a more reserved society, or in one where life was tougher, the reaction might be a quiet contentment.[8] It may have been Andrew Wiles, but I'm not sure.  If anyone remembers such an interview, I'd appreciate hearing from you.[9] Confucius claimed proudly that he had never invented anything\u2014that he had simply passed on an accurate account of ancient traditions.  [Analects VII:1] It's hard for us now to appreciate how important a duty it must have been in preliterate societies to remember and pass on the group's accumulated knowledge. Even in Confucius's time it still seems to have been the first duty of the scholar.[10] The bias toward wisdom in ancient philosophy may be exaggerated by the fact that, in both Greece and China, many of the first philosophers (including Confucius and Plato) saw themselves as teachers of administrators, and so thought disproportionately about such matters.",
      "  The few people who did invent things, like storytellers, must have seemed an outlying data point that could be ignored.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, and Robert Morris for reading drafts of this.  Want to start a startup?  Get funded by Y Combinator.     January 2006To do something well you have to like it.   That idea is not exactly novel.  We've got it down to four words: \"Do what you love.\"  But it's not enough just to tell people that.  Doing what you love is complicated.The very idea is foreign to what most of us learn as kids.  When I was a kid, it seemed as if work and fun were opposites by definition. Life had two states: some of the time adults were making you do things, and that was called work; the rest of the time you could do what you wanted, and that was called playing.  Occasionally the things adults made you do were fun, just as, occasionally, playing wasn't\u2014for example, if you fell and hurt yourself.  But except for these few anomalous cases, work was pretty much defined as not-fun.And it did not seem to be an accident. School, it was implied,",
      " was tedious because it was preparation for grownup work.The world then was divided into two groups, grownups and kids. Grownups, like some kind of cursed race, had to work.  Kids didn't, but they did have to go to school, which was a dilute version of work meant to prepare us for the real thing.  Much as we disliked school, the grownups all agreed that grownup work was worse, and that we had it easy.Teachers in particular all seemed to believe implicitly that work was not fun.  Which is not surprising: work wasn't fun for most of them.  Why did we have to memorize state capitals instead of playing dodgeball?  For the same reason they had to watch over a bunch of kids instead of lying on a beach.  You couldn't just do what you wanted.I'm not saying we should let little kids do whatever they want. They may have to be made to work on certain things.  But if we make kids work on dull stuff, it might be wise to tell them that tediousness is not the defining quality of work, and indeed that the reason they have to work on dull stuff now is so they can work on more interesting stuff later. [1]Once, when I was about 9 or 10,",
      " my father told me I could be whatever I wanted when I grew up, so long as I enjoyed it.  I remember that precisely because it seemed so anomalous.  It was like being told to use dry water.  Whatever I thought he meant, I didn't think he meant work could literally be fun\u2014fun like playing.  It took me years to grasp that.JobsBy high school, the prospect of an actual job was on the horizon. Adults would sometimes come to speak to us about their work, or we would go to see them at work.  It was always understood that they enjoyed what they did.  In retrospect I think one may have: the private jet pilot.  But I don't think the bank manager really did.The main reason they all acted as if they enjoyed their work was presumably the upper-middle class convention that you're supposed to.  It would not merely be bad for your career to say that you despised your job, but a social faux-pas.Why is it conventional to pretend to like what you do?  The first sentence of this essay explains that.  If you have to like something to do it well, then the most successful people will all like what they do.  That's where the upper-middle class tradition comes from.",
      " Just as houses all over America are full of  chairs that are, without the owners even knowing it, nth-degree imitations of chairs designed 250 years ago for French kings, conventional attitudes about work are, without the owners even knowing it, nth-degree imitations of the attitudes of people who've done great things.What a recipe for alienation.  By the time they reach an age to think about what they'd like to do, most kids have been thoroughly misled about the idea of loving one's work.  School has trained them to regard work as an unpleasant duty."
    ]
  },
  {
    "id": 77,
    "question": "What is the most important tool for a carpenter?",
    "answer": "a hammer.",
    "docs": [
      "October 2015When I talk to a startup that's been operating for more than 8 or 9 months, the first thing I want to know is almost always the same. Assuming their expenses remain constant and their revenue growth is what it has been over the last several months, do they make it to profitability on the money they have left?  Or to put it more dramatically, by default do they live or die?The startling thing is how often the founders themselves don't know. Half the founders I talk to don't know whether they're default alive or default dead.If you're among that number, Trevor Blackwell has made a handy calculator you can use to find out.The reason I want to know first whether a startup is default alive or default dead is that the rest of the conversation depends on the answer.  If the company is default alive, we can talk about ambitious new things they could do.  If it's default dead, we probably need to talk about how to save it.  We know the current trajectory ends badly.  How can they get off that trajectory?Why do so few founders know whether they're default alive or default dead?  Mainly, I think, because they're not used to asking that. It's not a question that makes sense to ask early on,",
      " any more than it makes sense to ask a 3 year old how he plans to support himself.  But as the company grows older, the question switches from meaningless to critical.  That kind of switch often takes people by surprise.I propose the following solution: instead of starting to ask too late whether you're default alive or default dead, start asking too early.  It's hard to say precisely when the question switches polarity.  But it's probably not that dangerous to start worrying too early that you're default dead, whereas it's very dangerous to start worrying too late.The reason is a phenomenon I wrote about earlier: the fatal pinch. The fatal pinch is default dead + slow growth + not enough time to fix it.  And the way founders end up in it is by not realizing that's where they're headed.There is another reason founders don't ask themselves whether they're default alive or default dead: they assume it will be easy to raise more money.  But that assumption is often false, and worse still, the more you depend on it, the falser it becomes.Maybe it will help to separate facts from hopes. Instead of thinking of the future with vague optimism, explicitly separate the components. Say \"We're default dead, but we're counting on investors to save us.\" Maybe as you say that,",
      " it will set off the same alarms in your head that it does in mine.  And if you set off the alarms sufficiently early, you may be able to avoid the fatal pinch.It would be safe to be default dead if you could count on investors saving you.  As a rule their interest is a function of growth.  If you have steep revenue growth, say over 5x a year, you can start to count on investors being interested even if you're not profitable. [1] But investors are so fickle that you can never do more than start to count on them.  Sometimes something about your business will spook investors even if your growth is great.  So no matter how good your growth is, you can never safely treat fundraising as more than a plan A. You should always have a plan B as well: you should know (as in write down) precisely what you'll need to do to survive if you can't raise more money, and precisely when you'll  have to switch to plan B if plan A isn't working.In any case, growing fast versus operating cheaply is far from the sharp dichotomy many founders assume it to be.  In practice there is surprisingly little connection between how much a startup spends and how fast it grows.",
      "  When a startup grows fast, it's usually because the product hits a nerve, in the sense of hitting some big need straight on.  When a startup spends a lot, it's usually because the product is expensive to develop or sell, or simply because they're wasteful.If you're paying attention, you'll be asking at this point not just how to avoid the fatal pinch, but how to avoid being default dead. That one is easy: don't hire too fast.  Hiring too fast is by far the biggest killer of startups that raise money. [2]Founders tell themselves they need to hire in order to grow.  But most err on the side of overestimating this need rather than underestimating it.  Why?  Partly because there's so much work to do.  Naive founders think that if they can just hire enough people, it will all get done.  Partly because successful startups have lots of employees, so it seems like that's what one does in order to be successful.  In fact the large staffs of successful startups are probably more the effect of growth than the cause.  And partly because when founders have slow growth they don't want to face what is usually the real reason: the product is not appealing enough.Plus founders who've just raised money are often encouraged to overhire by the VCs who funded them.",
      "  Kill-or-cure strategies are optimal for VCs because they're protected by the portfolio effect. VCs want to blow you up, in one sense of the phrase or the other. But as a founder your incentives are different.  You want above all to survive. [3]Here's a common way startups die.  They make something moderately appealing and have decent initial growth. They raise their first round fairly easily, because the founders seem smart and the idea sounds plausible. But because the product is only moderately appealing, growth is ok but not great.  The founders convince themselves that hiring a bunch of people is the way to boost growth. Their investors agree.  But (because the product is only moderately appealing) the growth never comes.  Now they're rapidly running out of runway.  They hope further investment will save them. But because they have high expenses and slow growth, they're now unappealing to investors. They're unable to raise more, and the company dies.What the company should have done is address the fundamental problem: that the product is only moderately appealing.  Hiring people is rarely the way to fix that.  More often than not it makes it harder. At this early stage, the product needs to evolve more than to be \"built out,\" and that's usually easier with fewer people.",
      " [4]Asking whether you're default alive or default dead may save you from this.  Maybe the alarm bells it sets off will counteract the forces that push you to overhire.  Instead you'll be compelled to seek growth in other ways. For example, by doing things that don't scale, or by redesigning the product in the way only founders can. And for many if not most startups, these paths to growth will be the ones that actually work.Airbnb waited 4 months after raising money at the end of Y\u00a0Combinator before they hired their first employee.  In the meantime the founders were terribly overworked.  But they were overworked evolving Airbnb into the astonishingly successful organism it is now.Notes[1] Steep usage growth will also interest investors.  Revenue will ultimately be a constant multiple of usage, so x% usage growth predicts x% revenue growth.  But in practice investors discount merely predicted revenue, so if you're measuring usage you need a higher growth rate to impress investors.[2] Startups that don't raise money are saved from hiring too fast because they can't afford to. But that doesn't mean you should avoid raising money in order to avoid this problem, any more than that total abstinence is the only way to avoid becoming an alcoholic.[3]",
      " I would not be surprised if VCs' tendency to push founders to overhire is not even in their own interest.  They don't know how many of the companies that get killed by overspending might have done well if they'd survived.  My guess is a significant number.[4] After reading a draft, Sam Altman wrote:\"I think you should make the hiring point more strongly.  I think it's roughly correct to say that YC's most successful companies have never been the fastest to hire, and one of the marks of a great founder is being able to resist this urge.\"Paul Buchheit adds:\"A related problem that I see a lot is premature scaling\u2014founders take a small business that isn't really working (bad unit economics, typically) and then scale it up because they want impressive growth numbers. This is similar to over-hiring in that it makes the business much harder to fix once it's big, plus they are bleeding cash really fast.\" Thanks to Sam Altman, Paul Buchheit, Joe Gebbia, Jessica Livingston, and Geoff Ralston for reading drafts of this.May 2004When people care enough about something to do it well, those who do it best tend to be far better than everyone else.  There's a huge gap between Leonardo and second-rate contemporaries like Borgognone.",
      "  You see the same gap between Raymond Chandler and the average writer of detective novels.  A top-ranked professional chess player could play ten thousand games against an ordinary club player without losing once.Like chess or painting or writing novels, making money is a very specialized skill.   But for some reason we treat this skill differently.  No one complains when a few people surpass all the rest at playing chess or writing novels, but when a few people make more money than the rest, we get editorials saying this is wrong.Why?  The pattern of variation seems no different than for any other skill.  What causes people to react so strongly when the skill is making money?I think there are three reasons we treat making money as different: the misleading model of wealth we learn as children; the disreputable way in which, till recently, most fortunes were accumulated; and the worry that great variations in income are somehow bad for society.  As far as I can tell, the first is mistaken, the second outdated, and the third empirically false.  Could it be that, in a modern democracy, variation in income is actually a sign of health?The Daddy Model of WealthWhen I was five I thought electricity was created by electric sockets.  I didn't realize there were power plants out there generating it.",
      "  Likewise, it doesn't occur to most kids that wealth is something that has to be generated.  It seems to be something that flows from parents.Because of the circumstances in which they encounter it, children tend to misunderstand wealth.  They confuse it with money.  They think that there is a fixed amount of it.  And they think of it as something that's distributed by authorities (and so should be distributed equally), rather than something that has to be created (and might be created unequally).In fact, wealth is not money.  Money is just a convenient way of trading one form of wealth for another.  Wealth is the underlying stuff\u2014the goods and services we buy.  When you travel to a rich or poor country, you don't have to look at people's bank accounts to tell which kind you're in.  You can see wealth\u2014in buildings and streets, in the clothes and the health of the people.Where does wealth come from?  People make it.  This was easier to grasp when most people lived on farms, and made many of the things they wanted with their own hands.  Then you could see in the house, the herds, and the granary the wealth that each family created.  It was obvious then too that the wealth of the world was not a fixed quantity that had to be shared out,",
      " like slices of a pie.  If you wanted more wealth, you could make it.This is just as true today, though few of us create wealth directly for ourselves (except for a few vestigial domestic tasks).  Mostly we create wealth for other people in exchange for money, which we then trade for the forms of wealth we want.  [1]Because kids are unable to create wealth, whatever they have has to be given to them.  And when wealth is something you're given, then of course it seems that it should be distributed equally. [2] As in most families it is.  The kids see to that.  \"Unfair,\" they cry, when one sibling gets more than another.In the real world, you can't keep living off your parents.  If you want something, you either have to make it, or do something of equivalent value for someone else, in order to get them to give you enough money to buy it.  In the real world, wealth is (except for a few specialists like thieves and speculators) something you have to create, not something that's distributed by Daddy.  And since the ability and desire to create it vary from person to person, it's not made equally.You get paid by doing or making something people want,",
      " and those who make more money are often simply better at doing what people want.  Top actors make a lot more money than B-list actors.  The B-list actors might be almost as charismatic, but when people go to the theater and look at the list of movies playing, they want that extra oomph that the big stars have.Doing what people want is not the only way to get money, of course. You could also rob banks, or solicit bribes, or establish a monopoly. Such tricks account for some variation in wealth, and indeed for some of the biggest individual fortunes, but they are not the root cause of variation in income.  The root cause of variation in income, as Occam's Razor implies, is the same as the root cause of variation in every other human skill.In the United States, the CEO of a large public company makes about 100 times as much as the average person.  [3] Basketball players make about 128 times as much, and baseball players 72 times as much. Editorials quote this kind of statistic with horror.  But I have no trouble imagining that one person could be 100 times as productive as another.  In ancient Rome the price of slaves varied by a factor of 50 depending on their skills.",
      "  [4] And that's without considering motivation, or the extra leverage in productivity that you can get from modern technology.Editorials about athletes' or CEOs' salaries remind me of early Christian writers, arguing from first principles about whether the Earth was round, when they could just walk outside and check. [5] How much someone's work is worth is not a policy question.  It's something the market already determines.\"Are they really worth 100 of us?\" editorialists ask.  Depends on what you mean by worth.  If you mean worth in the sense of what people will pay for their skills, the answer is yes, apparently.A few CEOs' incomes reflect some kind of wrongdoing.  But are there not others whose incomes really do reflect the wealth they generate? Steve Jobs saved a company that was in a terminal decline.  And not merely in the way a turnaround specialist does, by cutting costs; he had to decide what Apple's next products should be.  Few others could have done it.  And regardless of the case with CEOs, it's hard to see how anyone could argue that the salaries of professional basketball players don't reflect supply and demand.It may seem unlikely in principle that one individual could really generate so much more wealth than another.  The key to this mystery is to revisit that question,",
      " are they really worth 100 of us? Would a basketball team trade one of their players for 100 random people?  What would Apple's next product look like if you replaced Steve Jobs with a committee of 100 random people?  [6] These things don't scale linearly.  Perhaps the CEO or the professional athlete has only ten times (whatever that means) the skill and determination of an ordinary person.  But it makes all the difference that it's concentrated in one individual.When we say that one kind of work is overpaid and another underpaid, what are we really saying?  In a free market, prices are determined by what buyers want.  People like baseball more than  poetry, so baseball players make more than poets.  To say that a certain kind of work is underpaid is thus identical with saying that people want the wrong things.Well, of course people want the wrong things.  It seems odd to be surprised by that.  And it seems even odder to say that it's unjust that certain kinds of work are underpaid.  [7] Then you're saying that it's unjust that people want the wrong things. It's  lamentable that people prefer reality TV and corndogs to Shakespeare and steamed vegetables,",
      " but unjust?  That seems like saying that blue is heavy, or that up is circular.The appearance of the word \"unjust\" here is the unmistakable spectral signature of the Daddy Model.  Why else would this idea occur in this odd context?  Whereas if the speaker were still operating on the Daddy Model, and saw wealth as something that flowed from a common source and had to be shared out, rather than something generated by doing what other people wanted, this is exactly what you'd get on noticing that some people made much more than others.When we talk about \"unequal distribution of income,\" we should also ask, where does that income come from? [8] Who made the wealth it represents?  Because to the extent that income varies simply according to how much wealth people create, the distribution may be unequal, but it's hardly unjust.Stealing ItThe second reason we tend to find great disparities of wealth alarming is that for most of human history the usual way to accumulate a fortune was to steal it: in pastoral societies by cattle raiding; in agricultural societies by appropriating others' estates in times of war, and taxing them in times of peace.In conflicts, those on the winning side would receive the estates confiscated from the losers.  In England in the 1060s,",
      " when William the Conqueror distributed the estates of the defeated Anglo-Saxon nobles to his followers, the conflict was military.  By the 1530s, when Henry VIII distributed the estates of the monasteries to his followers, it was mostly political.  [9] But the principle was the same.  Indeed, the same principle is at work now in Zimbabwe.In more organized societies, like China, the ruler and his officials used taxation instead of confiscation.  But here too we see the same principle: the way to get rich was not to create wealth, but to serve a ruler powerful enough to appropriate it.This started to change in Europe with the rise of the middle class. Now we think of the middle class as people who are neither rich nor poor, but originally they were a distinct group.  In a feudal society, there are just two classes: a warrior aristocracy, and the serfs who work their estates.  The middle class were a new, third group who lived in towns and supported themselves by manufacturing and trade.Starting in the tenth and eleventh centuries, petty nobles and former serfs banded together in towns that gradually became powerful enough to ignore the local feudal lords.  [10] Like serfs, the middle class made a living largely by creating wealth.",
      "  (In port cities like Genoa and Pisa, they also engaged in piracy.) But unlike serfs they had an incentive to create a lot of it.  Any wealth a serf created belonged to his master.  There was not much point in making more than you could hide.  Whereas the independence of the townsmen allowed them to keep whatever wealth they created.Once it became possible to get rich by creating wealth, society as a whole started to get richer very rapidly.  Nearly everything we have was created by the middle class.  Indeed, the other two classes have effectively disappeared in industrial societies, and their names been given to either end of the middle class.  (In the original sense of the word, Bill Gates is middle class.)But it was not till the Industrial Revolution that wealth creation definitively replaced corruption as the best way to get rich.  In England, at least, corruption only became unfashionable (and in fact only started to be called \"corruption\") when there started to be other, faster ways to get rich.Seventeenth-century England was much like the third world today, in that government office was a recognized route to wealth.  The great fortunes of that time still derived more from what we would now call corruption than from commerce.",
      "  [11] By the nineteenth century that had changed.  There continued to be bribes, as there still are everywhere, but politics had by then been left to men who were driven more by vanity than greed.  Technology had made it possible to create wealth faster than you could steal it.  The prototypical rich man of the nineteenth century was not a courtier but an industrialist.With the rise of the middle class, wealth stopped being a zero-sum game.  Jobs and Wozniak didn't have to make us poor to make themselves rich.  Quite the opposite: they created things that made our lives materially richer.  They had to, or we wouldn't have paid for them.But since for most of the world's history the main route to wealth was to steal it, we tend to be suspicious of rich people.  Idealistic undergraduates find their unconsciously preserved child's model of wealth confirmed by eminent writers of the past.  It is a case of the mistaken meeting the outdated.\"Behind every great fortune, there is a crime,\" Balzac wrote.  Except he didn't.  What he actually said was that a great fortune with no apparent cause was probably due to a crime well enough executed that it had been forgotten.  If we were talking about Europe in 1000,",
      " or most of the third world today, the standard misquotation would be spot on.  But Balzac lived in nineteenth-century France, where the Industrial Revolution was well advanced.  He knew you could make a fortune without stealing it.  After all, he did himself, as a popular novelist. [12]Only a few countries (by no coincidence, the richest ones) have reached this stage.  In most, corruption still has the upper hand. In most, the fastest way to get wealth is by stealing it.  And so when we see increasing differences in income in a rich country, there is a tendency to worry that it's sliding back toward becoming another Venezuela.  I think the opposite is happening. I think you're seeing a country a full step ahead of Venezuela.The Lever of TechnologyWill technology increase the gap between rich and poor?  It will certainly increase the gap between the productive and the unproductive. That's the whole point of technology.   With a tractor an energetic farmer could plow six times as much land in a day as he could with a team of horses.  But only if he mastered a new kind of farming.I've seen the lever of technology grow visibly in my own time.  In high school I made money by mowing lawns and scooping ice cream at Baskin-Robbins.",
      "  This was the only kind of work available at the time.  Now high school kids could write software or design web sites.  But only some of them will; the rest will still be scooping ice cream.I remember very vividly when in 1985 improved technology made it possible for me to buy a computer of my own.  Within months I was using it to make money as a freelance programmer.  A few years before, I couldn't have done this.  A few years before, there was no such thing as a freelance programmer.  But Apple created wealth, in the form of powerful, inexpensive computers, and programmers immediately set to work using it to create more.As this example suggests, the rate at which technology increases our productive capacity is probably exponential, rather than linear. So we should expect to see ever-increasing variation in individual productivity as time goes on.   Will that increase the gap between rich and the poor?  Depends which gap you mean.Technology should increase the gap in income, but it seems to decrease other gaps.  A hundred years ago, the rich led a different kind of life from ordinary people.  They lived in houses full of servants, wore elaborately uncomfortable clothes, and travelled about in carriages drawn by teams of horses which themselves required their own houses and servants.",
      "  Now, thanks to technology, the rich live more like the average person.Cars are a good example of why.  It's possible to buy expensive, handmade cars that cost hundreds of thousands of dollars.  But there is not much point.  Companies make more money by building a large number of ordinary cars than a small number of expensive ones.  So a company making a mass-produced car can afford to spend a lot more on its design.  If you buy a custom-made car, something will always be breaking.  The only point of buying one now is to advertise that you can.Or consider watches.  Fifty years ago, by spending a lot of money on a watch you could get better performance.  When watches had mechanical movements, expensive watches kept better time.  Not any more.  Since the invention of the quartz movement, an ordinary Timex is more accurate than a Patek Philippe costing hundreds of thousands of dollars. [13] Indeed, as with expensive cars, if you're determined to spend a lot of money on a watch, you have to put up with some inconvenience to do it: as well as keeping worse time, mechanical watches have to be wound.The only thing technology can't cheapen is brand.  Which is precisely why we hear ever more about it.",
      "  Brand is the residue left as the substantive differences between rich and poor evaporate.  But what label you have on your stuff is a much smaller matter than having it versus not having it.  In 1900, if you kept a carriage, no one asked what year or brand it was.  If you had one, you were rich. And if you weren't rich, you took the omnibus or walked.  Now even the poorest Americans drive cars, and it is only because we're so well trained by advertising that we can even recognize the especially expensive ones. [14]The same pattern has played out in industry after industry.  If there is enough demand for something, technology will make it cheap enough to sell in large volumes, and the mass-produced versions will be, if not better, at least more convenient. [15] And there is nothing the rich like more than convenience.  The rich people I know drive the same cars, wear the same clothes, have the same kind of furniture, and eat the same foods as my other friends.  Their houses are in different neighborhoods, or if in the same neighborhood are different sizes, but within them life is similar.  The houses are made using the same construction techniques and contain much the same objects.",
      "  It's inconvenient to do something expensive and custom.The rich spend their time more like everyone else too.  Bertie Wooster seems long gone.  Now, most people who are rich enough not to work do anyway.  It's not just social pressure that makes them; idleness is lonely and demoralizing.Nor do we have the social distinctions there were a hundred years ago.   The novels and etiquette manuals of that period read now like descriptions of some strange tribal society.  \"With respect to the continuance of friendships...\" hints Mrs. Beeton's Book of Household Management (1880), \"it may be found necessary, in some cases, for a mistress to relinquish, on assuming the responsibility of a household, many of those commenced in the earlier part of her life.\" A woman who married a rich man was expected to drop friends who didn't.  You'd seem a barbarian if you behaved that way today. You'd also have a very boring life.  People still tend to segregate themselves somewhat, but much more on the basis of education than wealth. [16]Materially and socially, technology seems to be decreasing the gap between the rich and the poor, not increasing it.  If Lenin walked around the offices of a company like Yahoo or Intel or Cisco,",
      " he'd think communism had won.  Everyone would be wearing the same clothes, have the same kind of office (or rather, cubicle) with the same furnishings, and address one another by their first names instead of by honorifics.  Everything would seem exactly as he'd predicted, until he looked at their bank accounts.  Oops.Is it a problem if technology increases that gap?  It doesn't seem to be so far.  As it increases the gap in income, it seems to decrease most other gaps.Alternative to an AxiomOne often hears a policy criticized on the grounds that it would increase the income gap between rich and poor.  As if it were an axiom that this would be bad.  It might be true that increased variation in income would be bad, but I don't see how we can say it's axiomatic.Indeed, it may even be false, in industrial democracies.  In a society of serfs and warlords, certainly, variation in income is a sign of an underlying problem.  But serfdom is not the only cause of variation in income.  A 747 pilot doesn't make 40 times as much as a checkout clerk because he is a warlord who somehow holds her in thrall.",
      "  His skills are simply much more valuable.I'd like to propose an alternative idea: that in a modern society, increasing variation in income is a sign of health.  Technology seems to increase the variation in productivity at faster than linear rates.  If we don't see corresponding variation in income, there are three possible explanations: (a) that technical innovation has stopped, (b) that the people who would create the most wealth aren't doing it, or (c) that they aren't getting paid for it.I think we can safely say that (a) and (b) would be bad.  If you disagree, try living for a year using only the resources available to the average Frankish nobleman in 800, and report back to us. (I'll be generous and not send you back to the stone age.)The only option, if you're going to have an increasingly prosperous society without increasing variation in income, seems to be (c), that people will create a lot of wealth without being paid for it. That Jobs and Wozniak, for example, will cheerfully work 20-hour days to produce the Apple computer for a society that allows them, after taxes, to keep just enough of their income to match what they would have made working 9 to 5 at a big company.Will people create wealth if they can't get paid for it?",
      "  Only if it's fun.  People will write operating systems for free.  But they won't install them, or take support calls, or train customers to use them.  And at least 90% of the work that even the highest tech companies do is of this second, unedifying kind.All the unfun kinds of wealth creation slow dramatically in a society that confiscates private fortunes.  We can confirm this empirically. Suppose you hear a strange noise that you think may be due to a nearby fan.  You turn the fan off, and the noise stops.  You turn the fan back on, and the noise starts again.  Off, quiet.  On, noise.  In the absence of other information, it would seem the noise is caused by the fan.At various times and places in history, whether you could accumulate a fortune by creating wealth has been turned on and off.  Northern Italy in 800, off (warlords would steal it).  Northern Italy in 1100, on.  Central France in 1100, off (still feudal).  England in 1800, on.  England in 1974, off (98% tax on investment income). United States in 1974, on.  We've even had a twin study:",
      " West Germany, on;  East Germany, off.  In every case, the creation of wealth seems to appear and disappear like the noise of a fan as you switch on and off the prospect of keeping it.There is some momentum involved.  It probably takes at least a generation to turn people into East Germans (luckily for England). But if it were merely a fan we were studying, without all the extra baggage that comes from the controversial topic of wealth, no one would have any doubt that the fan was causing the noise.If you suppress variations in income, whether by stealing private fortunes, as feudal rulers used to do, or by taxing them away, as some modern governments have done, the result always seems to be the same.    Society as a whole ends up poorer.If I had a choice of living in a society where I was materially much better off than I am now, but was among the poorest, or in one where I was the richest, but much worse off than I am now, I'd take the first option.  If I had children, it would arguably be immoral not to.  It's absolute poverty you want to avoid, not relative poverty.  If, as the evidence so far implies, you have to have one or the other in your society,",
      " take relative poverty.You need rich people in your society not so much because in spending their money they create jobs, but because of what they have to do to get rich.  I'm not talking about the trickle-down effect here.  I'm not saying that if you let Henry Ford get rich, he'll hire you as a waiter at his next party.  I'm saying that he'll make you a tractor to replace your horse.Notes[1] Part of the reason this subject is so contentious is that some of those most vocal on the subject of wealth\u2014university students, heirs, professors, politicians, and journalists\u2014have the least experience creating it.  (This phenomenon will be familiar to anyone who has overheard conversations about sports in a bar.)Students are mostly still on the parental dole, and have not stopped to think about where that money comes from.  Heirs will be on the parental dole for life.  Professors and politicians live within socialist eddies of the economy, at one remove from the creation of wealth, and are paid a flat rate regardless of how hard they work.  And journalists as part of their professional code segregate themselves from the revenue-collecting half of the businesses they work for (the ad sales department).  Many of these people never come face to face with the fact that the money they receive represents wealth\u2014wealth that,",
      " except in the case of journalists, someone else created earlier.  They live in a world in which income is doled out by a central authority according to some abstract notion of fairness (or randomly, in the case of heirs), rather than given by other people in return for something they wanted, so it may seem to them unfair that things don't work the same in the rest of the economy.(Some professors do create a great deal of wealth for society.  But the money they're paid isn't a quid pro quo. It's more in the nature of an investment.)[2] When one reads about the origins of the Fabian Society, it sounds like something cooked up by the high-minded Edwardian child-heroes of Edith Nesbit's The Wouldbegoods.[3] According to a study by the Corporate Library, the median total compensation, including salary, bonus, stock grants, and the exercise of stock options, of S&P 500 CEOs in 2002 was $3.65 million. According to Sports Illustrated, the average NBA player's salary during the 2002-03 season was $4.54 million, and the average major league baseball player's salary at the start of the 2003 season was $2.56 million.",
      "  According to the Bureau of Labor Statistics, the mean annual wage in the US in 2002 was $35,560.[4] In the early empire the price of an ordinary adult slave seems to have been about 2,000 sestertii (e.g. Horace, Sat. ii.7.43). A servant girl cost 600 (Martial vi.66), while Columella (iii.3.8) says that a skilled vine-dresser was worth 8,000.  A doctor, P. Decimus Eros Merula, paid 50,000 sestertii for his freedom (Dessau, Inscriptiones 7812).  Seneca (Ep. xxvii.7) reports that one Calvisius Sabinus paid 100,000 sestertii apiece for slaves learned in the Greek classics.  Pliny (Hist. Nat. vii.39) says that the highest price paid for a slave up to his time was 700,000 sestertii, for the linguist (and presumably teacher) Daphnis, but that this had since been exceeded by actors buying their own freedom.Classical Athens saw a similar variation in prices.  An ordinary laborer was worth about 125 to 150 drachmae.",
      "  Xenophon (Mem. ii.5) mentions prices ranging from 50 to 6,000 drachmae (for the manager of a silver mine).For more on the economics of ancient slavery see:Jones, A. H. M., \"Slavery in the Ancient World,\" Economic History Review, 2:9 (1956), 185-199, reprinted in Finley, M. I. (ed.), Slavery in Classical Antiquity, Heffer, 1964.[5] Eratosthenes (276\u2014195 BC) used shadow lengths in different cities to estimate the Earth's circumference.  He was off by only about 2%.[6] No, and Windows, respectively.[7] One of the biggest divergences between the Daddy Model and reality is the valuation of hard work.  In the Daddy Model, hard work is in itself deserving.  In reality, wealth is measured by what one delivers, not how much effort it costs.  If I paint someone's house, the owner shouldn't pay me extra for doing it with a toothbrush.It will seem to someone still implicitly operating on the Daddy Model that it is unfair when someone works hard and doesn't get paid much.  To help clarify the matter,",
      " get rid of everyone else and put our worker on a desert island, hunting and gathering fruit. If he's bad at it he'll work very hard and not end up with much food.  Is this unfair?  Who is being unfair to him?[8] Part of the reason for the tenacity of the Daddy Model may be the dual meaning of \"distribution.\" When economists talk about \"distribution of income,\" they mean statistical distribution.  But when you use the phrase frequently, you can't help associating it with the other sense of the word (as in e.g. \"distribution of alms\"), and thereby subconsciously seeing wealth as something that flows from some central tap.  The word \"regressive\" as applied to tax rates has a similar effect, at least on me; how can anything regressive be good?[9] \"From the beginning of the reign Thomas Lord Roos was an assiduous courtier of the young Henry VIII and was soon to reap the rewards. In 1525 he was made a Knight of the Garter and given the Earldom of Rutland.  In the thirties his support of the breach with Rome, his zeal in crushing the Pilgrimage of Grace, and his readiness to vote the death-penalty in the succession of spectacular treason trials that punctuated Henry's erratic matrimonial progress made him an obvious candidate for grants of monastic property.\"Stone,",
      " Lawrence, Family and Fortune: Studies in Aristocratic Finance in the Sixteenth and Seventeenth Centuries, Oxford University Press, 1973, p. 166.[10] There is archaeological evidence for large settlements earlier, but it's hard to say what was happening in them.Hodges, Richard and David Whitehouse, Mohammed, Charlemagne and the Origins of Europe, Cornell University Press, 1983.[11] William Cecil and his son Robert were each in turn the most powerful minister of the crown, and both used their position to amass fortunes among the largest of their times.  Robert in particular took bribery to the point of treason.  \"As Secretary of State and the leading advisor to King James on foreign policy, [he] was a special recipient of favour, being offered large bribes by the Dutch not to make peace with Spain, and large bribes by Spain to make peace.\" (Stone, op. cit., p. 17.)[12] Though Balzac made a lot of money from writing, he was notoriously improvident and was troubled by debts all his life.[13] A Timex will gain or lose about.5 seconds per day.  The most accurate mechanical watch, the Patek Philippe 10 Day Tourbillon,",
      " is rated at -1.5 to +2 seconds.  Its retail price is about $220,000.[14] If asked to choose which was more expensive, a well-preserved 1989 Lincoln Town Car ten-passenger limousine ($5,000) or a 2004 Mercedes S600 sedan ($122,000), the average Edwardian might well guess wrong.[15] To say anything meaningful about income trends, you have to talk about real income, or income as measured in what it can buy. But the usual way of calculating real income ignores much of the growth in wealth over time, because it depends on a consumer price index created by bolting end to end a series of numbers that are only locally accurate, and that don't include the prices of new inventions until they become so common that their prices stabilize.So while we might think it was very much better to live in a world with antibiotics or air travel or an electric power grid than without, real income statistics calculated in the usual way will prove to us that we are only slightly richer for having these things.Another approach would be to ask, if you were going back to the year x in a time machine, how much would you have to spend on trade goods to make your fortune?  For example,",
      " if you were going back to 1970 it would certainly be less than $500, because the processing power you can get for $500 today would have been worth at least $150 million in 1970.  The function goes asymptotic fairly quickly, because for times over a hundred years or so you could get all you needed in present-day trash.  In 1800 an empty plastic drink bottle with a screw top would have seemed a miracle of workmanship.[16] Some will say this amounts to the same thing, because the rich have better opportunities for education.  That's a valid point.  It is still possible, to a degree, to buy your kids' way into top colleges by sending them to private schools that in effect hack the college admissions process.According to a 2002 report by the National Center for Education Statistics, about 1.7% of American kids attend private, non-sectarian schools.  At Princeton, 36% of the class of 2007 came from such schools.  (Interestingly, the number at Harvard is significantly lower, about 28%.)  Obviously this is a huge loophole.  It does at least seem to be closing, not widening.Perhaps the designers of admissions processes should take a lesson from the example of computer security,",
      " and instead of just assuming that their system can't be hacked, measure the degree to which it is.February 2020What should an essay be? Many people would say persuasive. That's what a lot of us were taught essays should be. But I think we can aim for something more ambitious: that an essay should be useful.To start with, that means it should be correct. But it's not enough merely to be correct. It's easy to make a statement correct by making it vague. That's a common flaw in academic writing, for example. If you know nothing at all about an issue, you can't go wrong by saying that the issue is a complex one, that there are many factors to be considered, that it's a mistake to take too simplistic a view of it, and so on.Though no doubt correct, such statements tell the reader nothing. Useful writing makes claims that are as strong as they can be made without becoming false.For example, it's more useful to say that Pike's Peak is near the middle of Colorado than merely somewhere in Colorado. But if I say it's in the exact middle of Colorado, I've now gone too far, because it's a bit east of the middle.Precision and correctness are like opposing forces.",
      " It's easy to satisfy one if you ignore the other. The converse of vaporous academic writing is the bold, but false, rhetoric of demagogues. Useful writing is bold, but true.It's also two other things: it tells people something important, and that at least some of them didn't already know.Telling people something they didn't know doesn't always mean surprising them. Sometimes it means telling them something they knew unconsciously but had never put into words. In fact those may be the more valuable insights, because they tend to be more fundamental.Let's put them all together. Useful writing tells people something true and important that they didn't already know, and tells them as unequivocally as possible.Notice these are all a matter of degree. For example, you can't expect an idea to be novel to everyone. Any insight that you have will probably have already been had by at least one of the world's 7 billion people. But it's sufficient if an idea is novel to a lot of readers.Ditto for correctness, importance, and strength. In effect the four components are like numbers you can multiply together to get a score for usefulness. Which I realize is almost awkwardly reductive, but nonetheless true._____ How can you ensure that the things you say are true and novel and important?",
      " Believe it or not, there is a trick for doing this. I learned it from my friend Robert Morris, who has a horror of saying anything dumb. His trick is not to say anything unless he's sure it's worth hearing. This makes it hard to get opinions out of him, but when you do, they're usually right.Translated into essay writing, what this means is that if you write a bad sentence, you don't publish it. You delete it and try again. Often you abandon whole branches of four or five paragraphs. Sometimes a whole essay.You can't ensure that every idea you have is good, but you can ensure that every one you publish is, by simply not publishing the ones that aren't.In the sciences, this is called publication bias, and is considered bad. When some hypothesis you're exploring gets inconclusive results, you're supposed to tell people about that too. But with essay writing, publication bias is the way to go.My strategy is loose, then tight. I write the first draft of an essay fast, trying out all kinds of ideas. Then I spend days rewriting it very carefully.I've never tried to count how many times I proofread essays, but I'm sure there are sentences I've read 100 times before publishing them.",
      " When I proofread an essay, there are usually passages that stick out in an annoying way, sometimes because they're clumsily written, and sometimes because I'm not sure they're true. The annoyance starts out unconscious, but after the tenth reading or so I'm saying \"Ugh, that part\" each time I hit it. They become like briars that catch your sleeve as you walk past. Usually I won't publish an essay till they're all gone \u0097 till I can read through the whole thing without the feeling of anything catching.I'll sometimes let through a sentence that seems clumsy, if I can't think of a way to rephrase it, but I will never knowingly let through one that doesn't seem correct. You never have to. If a sentence doesn't seem right, all you have to do is ask why it doesn't, and you've usually got the replacement right there in your head.This is where essayists have an advantage over journalists. You don't have a deadline. You can work for as long on an essay as you need to get it right. You don't have to publish the essay at all, if you can't get it right. Mistakes seem to lose courage in the face of an enemy with unlimited resources. Or that's what it feels like.",
      " What's really going on is that you have different expectations for yourself. You're like a parent saying to a child \"we can sit here all night till you eat your vegetables.\" Except you're the child too.I'm not saying no mistake gets through. For example, I added condition (c) in \"A Way to Detect Bias\"  after readers pointed out that I'd omitted it. But in practice you can catch nearly all of them.There's a trick for getting importance too. It's like the trick I suggest to young founders for getting startup ideas: to make something you yourself want. You can use yourself as a proxy for the reader. The reader is not completely unlike you, so if you write about topics that seem important to you, they'll probably seem important to a significant number of readers as well.Importance has two factors. It's the number of people something matters to, times how much it matters to them. Which means of course that it's not a rectangle, but a sort of ragged comb, like a Riemann sum.The way to get novelty is to write about topics you've thought about a lot. Then you can use yourself as a proxy for the reader in this department too. Anything you notice that surprises you, who've thought about the topic a lot,",
      " will probably also surprise a significant number of readers. And here, as with correctness and importance, you can use the Morris technique to ensure that you will. If you don't learn anything from writing an essay, don't publish it.You need humility to measure novelty, because acknowledging the novelty of an idea means acknowledging your previous ignorance of it. Confidence and humility are often seen as opposites, but in this case, as in many others, confidence helps you to be humble. If you know you're an expert on some topic, you can freely admit when you learn something you didn't know, because you can be confident that most other people wouldn't know it either.The fourth component of useful writing, strength, comes from two things: thinking well, and the skillful use of qualification. These two counterbalance each other, like the accelerator and clutch in a car with a manual transmission. As you try to refine the expression of an idea, you adjust the qualification accordingly. Something you're sure of, you can state baldly with no qualification at all, as I did the four components of useful writing. Whereas points that seem dubious have to be held at arm's length with perhapses.As you refine an idea, you're pushing in the direction of less qualification. But you can rarely get it down to zero.",
      " Sometimes you don't even want to, if it's a side point and a fully refined version would be too long.Some say that qualifications weaken writing. For example, that you should never begin a sentence in an essay with \"I think,\" because if you're saying it, then of course you think it. And it's true that \"I think x\" is a weaker statement than simply \"x.\" Which is exactly why you need \"I think.\" You need it to express your degree of certainty.But qualifications are not scalars. They're not just experimental error. There must be 50 things they can express: how broadly something applies, how you know it, how happy you are it's so, even how it could be falsified. I'm not going to try to explore the structure of qualification here. It's probably more complex than the whole topic of writing usefully. Instead I'll just give you a practical tip: Don't underestimate qualification. It's an important skill in its own right, not just a sort of tax you have to pay in order to avoid saying things that are false. So learn and use its full range. It may not be fully half of having good ideas, but it's part of having them.There's one other quality I aim for in essays:",
      " to say things as simply as possible. But I don't think this is a component of usefulness. It's more a matter of consideration for the reader. And it's a practical aid in getting things right; a mistake is more obvious when expressed in simple language. But I'll admit that the main reason I write simply is not for the reader's sake or because it helps get things right, but because it bothers me to use more or fancier words than I need to. It seems inelegant, like a program that's too long.I realize florid writing works for some people. But unless you're sure you're one of them, the best advice is to write as simply as you can._____ I believe the formula I've given you, importance + novelty + correctness + strength, is the recipe for a good essay. But I should warn you that it's also a recipe for making people mad.The root of the problem is novelty. When you tell people something they didn't know, they don't always thank you for it. Sometimes the reason people don't know something is because they don't want to know it. Usually because it contradicts some cherished belief. And indeed, if you're looking for novel ideas, popular but mistaken beliefs are a good place to find them.",
      " Every popular mistaken belief creates a dead zone of ideas around  it that are relatively unexplored because they contradict it.The strength component just makes things worse. If there's anything that annoys people more than having their cherished assumptions contradicted, it's having them flatly contradicted.Plus if you've used the Morris technique, your writing will seem quite confident. Perhaps offensively confident, to people who disagree with you. The reason you'll seem confident is that you are confident: you've cheated, by only publishing the things you're sure of.  It will seem to people who try to disagree with you that you never admit you're wrong. In fact you constantly admit you're wrong. You just do it before publishing instead of after.And if your writing is as simple as possible, that just makes things worse. Brevity is the diction of command. If you watch someone delivering unwelcome news from a position of inferiority, you'll notice they tend to use lots of words, to soften the blow. Whereas to be short with someone is more or less to be rude to them.It can sometimes work to deliberately phrase statements more weakly than you mean. To put \"perhaps\" in front of something you're actually quite sure of. But you'll notice that when writers do this,",
      " they usually do it with a wink.I don't like to do this too much. It's cheesy to adopt an ironic tone for a whole essay. I think we just have to face the fact that elegance and curtness are two names for the same thing.You might think that if you work sufficiently hard to ensure that an essay is correct, it will be invulnerable to attack. That's sort of true. It will be invulnerable to valid attacks. But in practice that's little consolation.In fact, the strength component of useful writing will make you particularly vulnerable to misrepresentation. If you've stated an idea as strongly as you could without making it false, all anyone has to do is to exaggerate slightly what you said, and now it is false.Much of the time they're not even doing it deliberately. One of the most surprising things you'll discover, if you start writing essays, is that people who disagree with you rarely disagree with what you've actually written. Instead they make up something you said and disagree with that.For what it's worth, the countermove is to ask someone who does this to quote a specific sentence or passage you wrote that they believe is false, and explain why. I say \"for what it's worth\" because they never do.",
      " So although it might seem that this could get a broken discussion back on track, the truth is that it was never on track in the first place.Should you explicitly forestall likely misinterpretations? Yes, if they're misinterpretations a reasonably smart and well-intentioned person might make. In fact it's sometimes better to say something slightly misleading and then add the correction than to try to get an idea right in one shot. That can be more efficient, and can also model the way such an idea would be discovered.But I don't think you should explicitly forestall intentional misinterpretations in the body of an essay. An essay is a place to meet honest readers. You don't want to spoil your house by putting bars on the windows to protect against dishonest ones. The place to protect against intentional misinterpretations is in end-notes. But don't think you can predict them all. People are as ingenious at misrepresenting you when you say something they don't want to hear as they are at coming up with rationalizations for things they want to do but know they shouldn't. I suspect it's the same skill._____ As with most other things, the way to get better at writing essays is to practice. But how do you start? Now that we've examined the structure of useful writing,",
      " we can rephrase that question more precisely. Which constraint do you relax initially? The answer is, the first component of importance: the number of people who care about what you write.If you narrow the topic sufficiently, you can probably find something you're an expert on. Write about that to start with. If you only have ten readers who care, that's fine. You're helping them, and you're writing. Later you can expand the breadth of topics you write about.The other constraint you can relax is a little surprising: publication. Writing essays doesn't have to mean publishing them. That may seem strange now that the trend is to publish every random thought, but it worked for me. I wrote what amounted to essays in notebooks for about 15 years. I never published any of them and never expected to. I wrote them as a way of figuring things out. But when the web came along I'd had a lot of practice.Incidentally,  Steve  Wozniak did the same thing. In high school he designed computers on paper for fun. He couldn't build them because he couldn't afford the components. But when Intel launched 4K DRAMs in 1975, he was ready._____ How many essays are there left to write though?",
      " The answer to that question is probably the most exciting thing I've learned about essay writing. Nearly all of them are left to write.Although the essay  is an old form, it hasn't been assiduously cultivated. In the print era, publication was expensive, and there wasn't enough demand for essays to publish that many. You could publish essays if you were already well known for writing something else, like novels. Or you could write book reviews that you took over to express your own ideas. But there was not really a direct path to becoming an essayist. Which meant few essays got written, and those that did tended to be about a narrow range of subjects.Now, thanks to the internet, there's a path. Anyone can publish essays online. You start in obscurity, perhaps, but at least you can start. You don't need anyone's permission.It sometimes happens that an area of knowledge sits quietly for years, till some change makes it explode. Cryptography did this to number theory. The internet is doing it to the essay.The exciting thing is not that there's a lot left to write, but that there's a lot left to discover. There's a certain kind of idea that's best discovered by writing essays. If most essays are still unwritten,",
      " most such ideas are still undiscovered.Notes[1] Put railings on the balconies, but don't put bars on the windows.[2] Even now I sometimes write essays that are not meant for publication. I wrote several to figure out what Y Combinator should do, and they were really helpful.Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and Robert Morris for reading drafts of this.April 2004To the popular press, \"hacker\" means someone who breaks into computers.  Among programmers it means a good programmer. But the two meanings are connected.  To programmers, \"hacker\" connotes mastery in the most literal sense: someone who can make a computer do what he wants\u2014whether the computer wants to or not.To add to the confusion, the noun \"hack\" also has two senses.  It can be either a compliment or an insult.  It's called a hack when you do something in an ugly way.  But when you do something so clever that you somehow beat the system, that's also called a hack.  The word is used more often in the former than the latter sense, probably because ugly solutions are more common than brilliant ones.Believe it or not, the two senses of \"hack\"",
      " are also connected.  Ugly and imaginative solutions have something in common: they both break the rules.  And there is a gradual continuum between rule breaking that's merely ugly (using duct tape to attach something to your bike) and rule breaking that is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he was working on the Manhattan Project, Richard Feynman used to amuse himself by breaking into safes containing secret documents. This tradition continues today. When we were in grad school, a hacker friend of mine who spent too much time around MIT had his own lock picking kit. (He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would want to do such things. Another friend of mine once got in trouble with the government for breaking into computers.  This had only recently been declared a crime, and the FBI found that their usual investigative technique didn't work.  Police investigation apparently begins with a motive.  The usual motives are few: drugs, money, sex, revenge.  Intellectual curiosity was not one of the motives on the FBI's list.  Indeed, the whole concept seemed foreign to them.Those in authority tend to be annoyed by hackers' general attitude of disobedience.",
      "  But that disobedience is a byproduct of the qualities that make them good programmers. They may laugh at the CEO when he talks in generic corporate newspeech, but they also laugh at someone who tells them a certain problem can't be solved. Suppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers notice the eccentricities of eminent hackers and decide to adopt some of their own in order to seem smarter. The fake version is not merely annoying; the prickly attitude of these posers can actually slow the process of innovation.But even factoring in their annoying eccentricities, the disobedient attitude of hackers is a net win.  I wish its advantages were better understood.For example, I suspect people in Hollywood are simply mystified by hackers' attitudes toward copyrights.  They are a perennial topic of heated discussion on Slashdot. But why should people who program computers be so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent copying.  Show any hacker a lock and his first thought is how to pick it.  But there is a deeper reason that hackers are alarmed by measures like copyrights and patents. They see increasingly aggressive measures to protect \"intellectual property\" as a threat to the intellectual freedom they need to do their job.",
      " And they are right.It is by poking about inside current technology that hackers get ideas for the next generation.  No thanks, intellectual homeowners may say, we don't need any outside help.  But they're wrong. The next generation of computer technology has often\u2014perhaps more often than not\u2014been developed by outsiders.In 1977 there was no doubt some group within IBM developing what they expected to be the next generation of business computer.  They were mistaken. The next generation of business computer was being developed on entirely different lines by two long-haired guys called Steve in a garage in Los Altos.  At about the same time, the powers that be were cooperating to develop the official next generation operating system, Multics. But two guys who thought Multics excessively complex went off and wrote their own.  They gave it a name that was a joking reference to Multics: Unix.The latest intellectual property laws impose unprecedented restrictions on the sort of poking around that leads to new ideas. In the past, a competitor might use patents to prevent you from selling a copy of something they made, but they couldn't prevent you from taking one apart to see how it worked.   The latest laws make this a crime.  How are we to develop new technology if we can't study current technology to figure out how to improve it?",
      "Ironically, hackers have brought this on themselves. Computers are responsible for the problem.  The control systems inside machines used to be physical: gears and levers and cams. Increasingly, the brains (and thus the value) of products is in software. And by this I mean software in the general sense: i.e. data.  A song on an LP is physically stamped into the plastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet makes copies easy to distribute.  So it is no wonder companies are afraid.  But, as so often happens, fear has clouded their judgement.  The government has responded with draconian laws to protect intellectual property. They probably mean well. But they may not realize that such laws will do more harm than good.Why are programmers so violently opposed to these laws? If I were a legislator, I'd be interested in this mystery\u2014for the same reason that, if I were a farmer and suddenly heard a lot of squawking coming from my hen house one night, I'd want to go out and investigate.  Hackers are not stupid, and unanimity is very rare in this world. So if they're all squawking,",
      "    perhaps there is something amiss.Could it be that such laws, though intended to protect America, will actually harm it?  Think about it.  There is something very American about Feynman breaking into safes during the Manhattan Project.  It's hard to imagine the authorities having a sense of humor about such things over in Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it is also the essence of Americanness.  It is no accident that Silicon Valley is in America, and not France, or Germany, or England, or Japan. In those countries, people color inside the lines.I lived for a while in Florence.  But after I'd been there a few months I realized that what I'd been unconsciously hoping to find there was back in the place I'd just left. The reason Florence is famous is that in 1450, it was New York. In 1450 it was filled with the kind of turbulent and ambitious people you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is a congenial atmosphere for the right sort of unruliness\u2014that it is a home not just for the smart,",
      " but for smart-alecks. And hackers are invariably smart-alecks.  If we had a national holiday, it would be April 1st.  It says a great deal about our work that we use the same word for a brilliant or a horribly cheesy solution.   When we cook one up we're not always 100% sure which kind it is.  But as long as it has the right sort of wrongness, that's a promising sign. It's odd that people think of programming as precise and methodical.  Computers are precise and methodical.  Hacking is something you do with a gleeful laugh.In our world some of the most characteristic solutions are not far removed from practical jokes.  IBM was no doubt rather surprised by the consequences of the licensing deal for DOS, just as the hypothetical \"adversary\" must be when Michael Rabin solves a problem by redefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they can get away with.  And lately hackers  have sensed a change in the atmosphere. Lately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems especially ominous.  That must also mystify outsiders.  Why should we care especially about civil liberties?",
      "  Why programmers, more than dentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate. Civil liberties are not just an ornament, or a quaint American tradition.  Civil liberties make countries rich. If you made a graph of GNP per capita vs. civil liberties, you'd notice a definite trend.  Could civil liberties really be a cause, rather than just an effect?  I think so.  I think a society in which people can do and say what they want will also tend to be one in which the most efficient solutions win, rather than those sponsored by the most influential people. Authoritarian countries become corrupt; corrupt countries become poor; and poor countries are weak.  It seems to me there is a Laffer curve for government power, just as for tax revenues.  At least, it seems likely enough that it would be stupid to try the experiment and find out.  Unlike high tax rates, you can't repeal totalitarianism if it turns out to be a mistake.This is why hackers worry.  The government spying on people doesn't literally make programmers write worse code.  It just leads eventually to a world in which bad ideas win.  And because this is so important to hackers, they're especially sensitive to it.",
      "  They can sense totalitarianism approaching from a distance, as animals can sense an approaching   thunderstorm.It would be ironic if, as hackers fear, recent measures intended to protect national security and intellectual property turned out to be a missile aimed right at what makes    America successful.  But it would not be the first time that measures taken in an atmosphere of panic had the opposite of the intended effect.There is such a thing as Americanness. There's nothing like living abroad to teach you that.    And if you want to know whether something will nurture or squash this quality, it would be hard to find a better focus group than hackers, because they come closest of any group I know to embodying it.  Closer, probably,  than the men running our government, who for all their talk of patriotism remind me more of Richelieu or Mazarin than Thomas Jefferson or George Washington.When you read what the founding fathers had to say for themselves, they sound more like hackers. \"The spirit of resistance to government,\" Jefferson wrote, \"is so valuable on certain occasions, that I wish it always to be kept alive.\"Imagine an American president saying that today. Like the remarks of an outspoken old grandmother, the sayings of the founding fathers have embarrassed generations of their less confident successors.",
      "  They remind us where we come from. They remind us that it is the people who break rules that are the source of America's wealth and power.Those in a position to impose rules naturally want them to be obeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin,  Sarah Harlin,  Shiro Kawai, Jessica Livingston, Matz,  Jackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum, David Weinberger, and Steven Wolfram for reading drafts of this essay. (The image shows Steves Jobs and Wozniak  with a \"blue box.\" Photo by Margret Wozniak. Reproduced by permission of Steve Wozniak.)  Want to start a startup?  Get funded by Y Combinator.     November 2005Does \"Web 2.0\" mean anything?  Till recently I thought it didn't, but the truth turns out to be more complicated.  Originally, yes, it was meaningless.  Now it seems to have acquired a meaning.  And yet those who dislike the term are probably right, because if it means what I think it does, we don't need it.I first heard the phrase \"Web 2.",
      "0\" in the name of the Web 2.0 conference in 2004.  At the time it was supposed to mean using \"the web as a platform,\" which I took to refer to web-based applications. [1]So I was surprised at a conference this summer when Tim O'Reilly led a session intended to figure out a definition of \"Web 2.0.\" Didn't it already mean using the web as a platform?  And if it didn't already mean something, why did we need the phrase at all?OriginsTim says the phrase \"Web 2.0\" first arose in \"a brainstorming session between O'Reilly and Medialive International.\" What is Medialive International? \"Producers of technology tradeshows and conferences,\" according to their site.  So presumably that's what this brainstorming session was about.  O'Reilly wanted to organize a conference about the web, and they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a new version of the web.  They just wanted to make the point that the web mattered again.  It was a kind of semantic deficit spending: they knew new things were coming, and the \"2.0\" referred to whatever those might turn out to be.And they were right.",
      "  New things were coming.  But the new version number led to some awkwardness in the short term.  In the process of developing the pitch for the first conference, someone must have decided they'd better take a stab at explaining what that \"2.0\" referred to.  Whatever it meant, \"the web as a platform\" was at least not too constricting.The story about \"Web 2.0\" meaning the web as a platform didn't live much past the first conference.  By the second conference, what \"Web 2.0\" seemed to mean was something about democracy.  At least, it did when people wrote about it online.  The conference itself didn't seem very grassroots.  It cost $2800, so the only people who could afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article about the conference in Wired News spoke of \"throngs of geeks.\"  When a friend of mine asked Ryan about this, it was news to him.  He said he'd originally written something like \"throngs of VCs and biz dev guys\" but had later shortened it just to \"throngs,\" and that this must have in turn been expanded by the editors into \"throngs of geeks.\"  After all,",
      " a Web 2.0 conference would presumably be full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a    suit, a sight so alien I couldn't parse it at first.  I saw him walk by and said to one of the O'Reilly people \"that guy looks just like Tim.\"\"Oh, that's Tim.  He bought a suit.\" I ran after him, and sure enough, it was.  He explained that he'd just bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows during the Bubble, full of prowling VCs looking for the next hot startup.  There was that same odd atmosphere created by a large   number of people determined not to miss out.  Miss out on what? They didn't know.  Whatever was going to happen\u2014whatever Web 2.0 turned out to be.I wouldn't quite call it \"Bubble 2.0\" just because VCs are eager to invest again.  The Internet is a genuinely big deal.  The bust was as much an overreaction as the boom.  It's to be expected that once we started to pull out of the bust, there would be a lot of growth in this area,",
      " just as there was in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO market is gone.  Venture investors are driven by exit strategies.  The reason they were funding all   those laughable startups during the late 90s was that they hoped to sell them to gullible retail investors; they hoped to be laughing all the way to the bank.  Now that route is closed.  Now the default exit strategy is to get bought, and acquirers are less prone to irrational exuberance than IPO investors.  The closest you'll get  to Bubble valuations is Rupert Murdoch paying $580 million for    Myspace.  That's only off by a factor of 10 or so.1. AjaxDoes \"Web 2.0\" mean anything more than the name of a conference yet?  I don't like to admit it, but it's starting to.  When people say \"Web 2.0\" now, I have some idea what they mean.  And the fact that I both despise the phrase and understand it is the surest proof that it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still only just bear to use without scare quotes.",
      "  Basically, what \"Ajax\" means is \"Javascript now works.\"  And that in turn means that web-based applications can now be made to work much more like desktop ones.As you read this, a whole new generation of software is being written to take advantage of Ajax.  There hasn't been such a wave of new applications since microcomputers first appeared.  Even Microsoft sees it, but it's too late for them to do anything more than leak \"internal\"   documents designed to give the impression they're on top of this new trend.In fact the new generation of software is being written way too fast for Microsoft even to channel it, let alone write their own in house.  Their only hope now is to buy all the best Ajax startups before Google does.  And even that's going to be hard, because Google has as big a head start in buying microstartups as it did in search a few years ago.  After all, Google Maps, the canonical Ajax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference turned out to be partially right: web-based applications are a big component of Web 2.0.  But I'm convinced they got this right by  accident.  The Ajax boom didn't start till early 2005,",
      " when Google Maps appeared and the term \"Ajax\" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several examples to prove that amateurs can    surpass professionals, when they have the right kind of system to  channel their efforts.  Wikipedia may be the most famous.  Experts have given Wikipedia middling reviews, but they miss the critical point: it's good enough.  And    it's free, which means people actually read it.  On the web, articles you have to pay for might as well not exist.  Even if you were     willing to pay to read them yourself, you can't link to them.     They're not part of the conversation.Another place democracy seems to win is in deciding what counts as news.  I never look at any news site now except Reddit. [2]  I know if something major happens, or someone writes a particularly interesting article, it    will show up there.  Why bother checking the front page of any specific paper or magazine?  Reddit's like an RSS feed for the whole web, with a filter for quality.  Similar sites include Digg, a technology news site that's rapidly approaching Slashdot in popularity, and del.icio.us,",
      " the collaborative bookmarking network that set off the \"tagging\" movement.  And whereas Wikipedia's main appeal is that it's good enough and free, these sites suggest that voters do a significantly better job than human editors.The most dramatic example of Web 2.0 democracy is not in the selection of ideas, but their production.   I've noticed for a while that the stuff I read on individual people's sites is as good as or better than the stuff I read in newspapers and magazines.  And now I have independent evidence: the top links on Reddit are generally links to individual people's sites rather   than to magazine articles or news stories.My experience of writing for magazines suggests an explanation.  Editors.  They control the topics you can write about, and they can generally rewrite whatever you produce.  The result is to damp extremes.  Editing yields 95th percentile writing\u201495% of articles are improved by it, but 5% are dragged down.  5% of the time you get \"throngs of geeks.\"On the web, people can publish whatever they want.  Nearly all of it falls short of the editor-damped writing in print publications. But the pool of writers is very, very large.  If it's large enough,",
      " the lack of damping means the best writing online should surpass   the best in print. [3]   And now that the web has evolved mechanisms for selecting good stuff, the web wins net.  Selection beats damping, for the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the   startups of the Bubble what bloggers are to the print media.  During the Bubble, a startup meant a company headed by an MBA that was    blowing through several million dollars of VC money to \"get big fast\" in the most literal sense.  Now it means a smaller, younger, more technical group that just       decided to make something great.  They'll decide later if they want   to raise VC-scale funding, and if they take it, they'll take it on their terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements of \"Web 2.0.\"  I also see a third: not to maltreat users.  During the Bubble a lot of popular sites were quite high-handed with users. And not just in obvious ways, like making them register, or subjecting them to annoying ads.  The very design of the average site in the    late 90s was an abuse.",
      "  Many of the most popular sites were loaded with obtrusive branding that made them slow to load and sent the user the message: this is our site, not yours.  (There's a physical analog in the Intel and Microsoft stickers that come on some laptops.)I think the root of the problem was that sites felt they were giving something away for free, and till recently a company giving anything away for free could be pretty high-handed about it.  Sometimes it reached the point of economic sadism: site owners assumed that the more pain they caused the user, the more benefit it must be to them.   The most dramatic remnant of this model may be at salon.com, where    you can read the beginning of a story, but to get the rest you have sit through a movie.At Y Combinator we advise all the startups we fund never to lord it over users.  Never make users register, unless you need to in order to store something for them.  If you do make users register,    never make them wait for a confirmation link in an email; in fact, don't even ask for their email address unless you need it for some reason.  Don't ask them any unnecessary questions.  Never send them email unless they explicitly ask for it.",
      "  Never frame pages you link to, or open them in new windows.  If you have a free version  and a pay version, don't make the free version too restricted.  And if you find yourself asking \"should we allow users to do x?\" just  answer \"yes\" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups never to let anyone fly under them, meaning never to let any other company offer a cheaper, easier solution.  Another way to fly low  is to give users more power.  Let users do what they want.  If you  don't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual songs instead of having to buy whole albums.  The recording industry hated the idea and resisted it as long as possible.  But it was obvious what users wanted, so Apple flew under the labels. [4] Though really it might be better to describe iTunes as Web 1.5.      Web 2.0 applied to music would probably mean individual bands giving away DRMless songs for free.The ultimate way to be nice to users is to give them something for free that competitors charge for.",
      "  During the 90s a lot of people    probably thought we'd have some working system for micropayments      by now.  In fact things have gone in the other direction.  The most    successful sites are the ones that figure out new ways to give stuff away for free.  Craigslist has largely destroyed the classified ad sites of the 90s, and OkCupid looks likely to do the same to the previous generation of dating sites.Serving web pages is very, very cheap.  If you can make even a    fraction of a cent per page view, you can make a profit.  And technology for targeting ads continues to improve.  I wouldn't be surprised if ten years from now eBay had been supplanted by an       ad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to make as little money as possible.  If you can figure out a way to turn a billion dollar industry into a fifty million dollar industry, so much the better, if all fifty million go to you.  Though indeed, making things cheaper often turns out to generate more money in the end, just as automating things often turns out to generate more jobs.The ultimate target is Microsoft.",
      "  What a bang that balloon is going to make when someone pops it by offering a free web-based alternative  to MS Office. [5] Who will?  Google?  They seem to be taking their time.  I suspect the pin will be wielded by a couple of 20 year old hackers who are too naive to be intimidated by the idea.  (How hard can it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in   common?  I didn't realize they had anything in common till recently, which is one of the reasons I disliked the term \"Web 2.0\" so much. It seemed that it was being used as a label for whatever happened to be new\u2014that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way it's meant to be used.  The \"trends\" we're seeing now are simply the inherent nature of the web emerging from under the broken models that got imposed on it during the Bubble.I realized this when I read an  interview with Joe Kraus, the co-founder of Excite. [6]    Excite really never got the business model right at all.  We fell    into the classic problem of how when a new medium comes out it   adopts the practices,",
      " the content, the business models of the old   medium\u2014which fails, and then the more appropriate models get   figured out.  It may have seemed as if not much was happening during the years after the Bubble burst.  But in retrospect, something was happening: the web was finding its natural angle of repose.  The democracy  component, for example\u2014that's not an innovation, in the sense of something someone made happen.  That's what the web naturally tends to produce.Ditto for the idea of delivering desktop-like applications over the web.  That idea is almost as old as the web.  But the first time     around it was co-opted by Sun, and we got Java applets.  Java has since been remade into a generic replacement for C++, but in 1996 the story about Java was that it represented a new model of software. Instead of desktop applications, you'd run Java \"applets\" delivered from a server.This plan collapsed under its own weight. Microsoft helped kill it, but it would have died anyway.  There was no uptake among hackers. When you find PR firms promoting something as the next development platform, you can be sure it's not.  If it were, you wouldn't need PR firms to tell you,",
      " because    hackers would already be writing stuff on top of it, the way sites     like Busmonster used Google Maps as a platform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of   hackers have spontaneously started building things on top of it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common. Here's a clue.  Suppose you approached investors with the following idea for a Web 2.0 startup:    Sites like del.icio.us and flickr allow users to \"tag\" content   with descriptive tokens.  But there is also huge source of   implicit tags that they ignore: the text within web links.   Moreover, these links represent a social network connecting the      individuals and organizations who created the pages, and by using   graph theory we can compute from this network an estimate of the   reputation of each member.  We plan to mine the web for these    implicit tags, and use them together with the reputation hierarchy   they embody to enhance web searches.  How long do you think it would take them on average to realize that it was a description of Google?Google was a pioneer in all three components of Web 2.0: their core business sounds crushingly hip when described in Web 2.",
      "0 terms,  \"Don't maltreat users\" is a subset of \"Don't be evil,\" and of course Google set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google does.  That's their secret.    They're sailing with the wind, instead of sitting   becalmed praying for a business model, like the print media, or    trying to tack upwind by suing their customers, like Microsoft and  the record labels. [7]Google doesn't try to force things to happen their way.  They try    to figure out what's going to happen, and arrange to be standing  there when it does.  That's the way to approach technology\u2014and  as business includes an ever larger technological component, the right way to do business.The fact that Google is a \"Web 2.0\" company shows that, while meaningful, the term is also rather bogus.  It's like the word \"allopathic.\"  It just means doing things right, and it's a bad    sign when you have a special word for that. Notes[1] From the conference site, June 2004: \"While the first wave of the Web was closely   tied to the browser,",
      " the second wave extends applications across     the web and enables a new generation of services and business opportunities.\"  To the extent this means anything, it seems to be about  web-based applications.[2] Disclosure: Reddit was funded by  Y Combinator.  But although I started using it out of loyalty to the home team, I've become a genuine addict.  While we're at it, I'm also an investor in!MSFT, having sold all my shares earlier this year.[3] I'm not against editing. I spend more time editing than writing, and I have a group of picky friends who proofread almost everything I write.  What I dislike is editing done after the fact   by someone else.[4] Obvious is an understatement.  Users had been climbing in through   the window for years before Apple finally moved the door.[5] Hint: the way to create a web-based alternative to Office may not be to write every component yourself, but to establish a protocol for web-based apps to share a virtual home directory spread across multiple servers.  Or it may be to write it all yourself.[6] In Jessica Livingston's Founders at Work.[7] Microsoft didn't sue their customers directly, but they seem  to have done all they could to help SCO sue them.Thanks to Trevor Blackwell,",
      " Sarah Harlin, Jessica Livingston, Peter Norvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the guys at O'Reilly and Adaptive Path for answering my questions.  Want to start a startup?  Get funded by Y Combinator.     January 2006To do something well you have to like it.   That idea is not exactly novel.  We've got it down to four words: \"Do what you love.\"  But it's not enough just to tell people that.  Doing what you love is complicated.The very idea is foreign to what most of us learn as kids.  When I was a kid, it seemed as if work and fun were opposites by definition. Life had two states: some of the time adults were making you do things, and that was called work; the rest of the time you could do what you wanted, and that was called playing.  Occasionally the things adults made you do were fun, just as, occasionally, playing wasn't\u2014for example, if you fell and hurt yourself.  But except for these few anomalous cases, work was pretty much defined as not-fun.And it did not seem to be an accident. School, it was implied, was tedious because it was preparation for grownup work.The world then was divided into two groups,",
      " grownups and kids. Grownups, like some kind of cursed race, had to work.  Kids didn't, but they did have to go to school, which was a dilute version of work meant to prepare us for the real thing.  Much as we disliked school, the grownups all agreed that grownup work was worse, and that we had it easy.Teachers in particular all seemed to believe implicitly that work was not fun.  Which is not surprising: work wasn't fun for most of them.  Why did we have to memorize state capitals instead of playing dodgeball?  For the same reason they had to watch over a bunch of kids instead of lying on a beach.  You couldn't just do what you wanted.I'm not saying we should let little kids do whatever they want. They may have to be made to work on certain things.  But if we make kids work on dull stuff, it might be wise to tell them that tediousness is not the defining quality of work, and indeed that the reason they have to work on dull stuff now is so they can work on more interesting stuff later. [1]Once, when I was about 9 or 10, my father told me I could be whatever I wanted when I grew up,",
      " so long as I enjoyed it.  I remember that precisely because it seemed so anomalous.  It was like being told to use dry water.  Whatever I thought he meant, I didn't think he meant work could literally be fun\u2014fun like playing.  It took me years to grasp that.JobsBy high school, the prospect of an actual job was on the horizon. Adults would sometimes come to speak to us about their work, or we would go to see them at work.  It was always understood that they enjoyed what they did.  In retrospect I think one may have: the private jet pilot.  But I don't think the bank manager really did.The main reason they all acted as if they enjoyed their work was presumably the upper-middle class convention that you're supposed to.  It would not merely be bad for your career to say that you despised your job, but a social faux-pas.Why is it conventional to pretend to like what you do?  The first sentence of this essay explains that.  If you have to like something to do it well, then the most successful people will all like what they do.  That's where the upper-middle class tradition comes from. Just as houses all over America are full of  chairs that are,",
      " without the owners even knowing it, nth-degree imitations of chairs designed 250 years ago for French kings, conventional attitudes about work are, without the owners even knowing it, nth-degree imitations of the attitudes of people who've done great things.What a recipe for alienation.  By the time they reach an age to think about what they'd like to do, most kids have been thoroughly misled about the idea of loving one's work.  School has trained them to regard work as an unpleasant duty.  Having a job is said to be even more onerous than schoolwork.  And yet all the adults claim to like what they do.  You can't blame kids for thinking \"I am not like these people; I am not suited to this world.\"Actually they've been told three lies: the stuff they've been taught to regard as work in school is not real work; grownup work is not (necessarily) worse than schoolwork; and many of the adults around them are lying when they say they like what they do.The most dangerous liars can be the kids' own parents.  If you take a boring job to give your family a high standard of living, as so many people do, you risk infecting your kids with the idea that work is boring.",
      "  [2] Maybe it would be better for kids in this one case if parents were not so unselfish.  A parent who set an example of loving their work might help their kids more than an expensive house. [3]It was not till I was in college that the idea of work finally broke free from the idea of making a living.  Then the important question became not how to make money, but what to work on.  Ideally these coincided, but some spectacular boundary cases (like Einstein in the patent office) proved they weren't identical.The definition of work was now to make some original contribution to the world, and in the process not to starve.  But after the habit of so many years my idea of work still included a large component of pain.  Work still seemed to require discipline, because only hard problems yielded grand results, and hard problems couldn't literally be fun.   Surely one had to force oneself to work on them.If you think something's supposed to hurt, you're less likely to notice if you're doing it wrong.  That about sums up my experience of graduate school.BoundsHow much are you supposed to like what you do?  Unless you know that, you don't know when to stop searching. And if,",
      " like most people, you underestimate it, you'll tend to stop searching too early.  You'll end up doing something chosen for you by your parents, or the desire to make money, or prestige\u2014or sheer inertia.Here's an upper bound: Do what you love doesn't mean, do what you would like to do most this second.  Even Einstein probably had moments when he wanted to have a cup of coffee, but told himself he ought to finish what he was working on first.It used to perplex me when I read about people who liked what they did so much that there was nothing they'd rather do.  There didn't seem to be any sort of work I liked that much.  If I had a choice of (a) spending the next hour working on something or (b) be teleported to Rome and spend the next hour wandering about, was there any sort of work I'd prefer?  Honestly, no.But the fact is, almost anyone would rather, at any given moment, float about in the Carribbean, or have sex, or eat some delicious food, than work on hard problems.  The rule about doing what you love assumes a certain length of time.  It doesn't mean, do what will make you happiest this second, but what will make you happiest over some longer period,",
      " like a week or a month.Unproductive pleasures pall eventually.  After a while you get tired of lying on the beach The most important tool for a carpenter is a hammer..  If you want to stay happy, you have to do something.As a lower bound, you have to like your work more than any unproductive pleasure.  You have to like what you do enough that the concept of \"spare time\" seems mistaken.  Which is not to say you have to spend all your time working.  You can only work so much before you get tired and start to screw up.  Then you want to do something else\u2014even something mindless.  But you don't regard this time as the prize and the time you spend working as the pain you endure to earn it.I put the lower bound there for practical reasons.  If your work is not your favorite thing to do, you'll have terrible problems with procrastination.  You'll have to force yourself to work,  and when you resort to that the results are distinctly inferior.To be happy I think you have to be doing something you not only enjoy, but admire.  You have to be able to say, at the end, wow, that's pretty cool.  This doesn't mean you have to make something. If you learn how to hang glide,",
      " or to speak a foreign language fluently, that will be enough to make you say, for a while at least, wow, that's pretty cool.  What there has to be is a test.So one thing that falls just short of the standard, I think, is reading books.  Except for some books in math and the hard sciences, there's no test of how well you've read a book, and that's why merely reading books doesn't quite feel like work.  You have to do something with what you've read to feel productive.I think the best test is one Gino Lee taught me: to try to do things that would make your friends say wow.  But it probably wouldn't start to work properly till about age 22, because most people haven't had a big enough sample to pick friends from before then.SirensWhat you should not do, I think, is worry about the opinion of anyone beyond your friends.  You shouldn't worry about prestige. Prestige is the opinion of the rest of the world.  When you can ask the opinions of people whose judgement you respect, what does it add to consider the opinions of people you don't even know?  [4]This is easy advice to give.  It's hard to follow,",
      " especially when you're young.   [5] Prestige is like a powerful magnet that warps even your beliefs about what you enjoy.  It causes you to work not on what you like, but what you'd like to like.That's what leads people to try to write novels, for example.  They like reading novels.  They notice that people who write them win Nobel prizes.  What could be more wonderful, they think, than to be a novelist?  But liking the idea of being a novelist is not enough; you have to like the actual work of novel-writing if you're going to be good at it; you have to like making up elaborate lies.Prestige is just fossilized inspiration.  If you do anything well enough, you'll make it prestigious.  Plenty of things we now consider prestigious were anything but at first.  Jazz comes to mind\u2014though almost any established art form would do.   So just do what you like, and let prestige take care of itself.Prestige is especially dangerous to the ambitious.  If you want to make ambitious people waste their time on errands, the way to do it is to bait the hook with prestige.  That's the recipe for getting people to give talks, write forewords,",
      " serve on committees, be department heads, and so on.  It might be a good rule simply to avoid any prestigious task. If it didn't suck, they wouldn't have had to make it prestigious.Similarly, if you admire two kinds of work equally, but one is more prestigious, you should probably choose the other.  Your opinions about what's admirable are always going to be slightly influenced by prestige, so if the two seem equal to you, you probably have more genuine admiration for the less prestigious one.The other big force leading people astray is money.  Money by itself is not that dangerous.  When something pays well but is regarded with contempt, like telemarketing, or prostitution, or personal injury litigation, ambitious people aren't tempted by it.  That kind of work ends up being done by people who are \"just trying to make a living.\"  (Tip: avoid any field whose practitioners say this.)  The danger is when money is combined with prestige, as in, say, corporate law, or medicine.  A comparatively safe and prosperous career with some automatic baseline prestige is dangerously tempting to someone young, who hasn't thought much about what they really like.The test of whether people love what they do is whether they'd do it even if they weren't paid for it\u2014even if they had to work at another job to make a living.",
      "  How many corporate lawyers would do their current work if they had to do it for free, in their spare time, and take day jobs as waiters to support themselves?This test is especially helpful in deciding between different kinds of academic work, because fields vary greatly in this respect.  Most good mathematicians would work on math even if there were no jobs as math professors, whereas in the departments at the other end of the spectrum, the availability of teaching jobs is the driver: people would rather be English professors than work in ad agencies, and publishing papers is the way you compete for such jobs.  Math would happen without math departments, but it is the existence of English majors, and therefore jobs teaching them, that calls into being all those thousands of dreary papers about gender and identity in the novels of Conrad.  No one does  that  kind of thing for fun.The advice of parents will tend to err on the side of money.  It seems safe to say there are more undergrads who want to be novelists and whose parents want them to be doctors than who want to be doctors and whose parents want them to be novelists.  The kids think their parents are \"materialistic.\" Not necessarily.  All parents tend to be more conservative for their kids than they would for themselves,",
      " simply because, as parents, they share risks more than rewards.  If your eight year old son decides to climb a tall tree, or your teenage daughter decides to date the local bad boy, you won't get a share in the excitement, but if your son falls, or your daughter gets pregnant, you'll have to deal with the consequences.DisciplineWith such powerful forces leading us astray, it's not surprising we find it so hard to discover what we like to work on.  Most people are doomed in childhood by accepting the axiom that work = pain. Those who escape this are nearly all lured onto the rocks by prestige or money.  How many even discover something they love to work on? A few hundred thousand, perhaps, out of billions.It's hard to find work you love; it must be, if so few do.  So don't underestimate this task.  And don't feel bad if you haven't succeeded yet.  In fact, if you admit to yourself that you're discontented, you're a step ahead of most people, who are still in denial.  If you're surrounded by colleagues who claim to enjoy work that you find contemptible, odds are they're lying to themselves.  Not necessarily, but probably.Although doing great work takes less discipline than people think\u2014because the way to do great work is to find something you like so much that you don't have to force yourself to do it\u2014finding work you love does usually require discipline.",
      "   Some people are lucky enough to know what they want to do when they're 12, and just glide along as if they were on railroad tracks.  But this seems the exception.  More often people who do great things have careers with the trajectory of a ping-pong ball.  They go to school to study A, drop out and get a job doing B, and then become famous for C after taking it up on the side.Sometimes jumping from one sort of work to another is a sign of energy, and sometimes it's a sign of laziness.  Are you dropping out, or boldly carving a new path?  You often can't tell yourself. Plenty of people who will later do great things seem to be disappointments early on, when they're trying to find their niche.Is there some test you can use to keep yourself honest?  One is to try to do a good job at whatever you're doing, even if you don't like it.  Then at least you'll know you're not using dissatisfaction as an excuse for being lazy.  Perhaps more importantly, you'll get into the habit of doing things well.Another test you can use is: always produce.  For example, if you have a day job you don't take seriously because you plan to be a novelist,",
      " are you producing?  Are you writing pages of fiction, however bad?  As long as you're producing, you'll know you're not merely using the hazy vision of the grand novel you plan to write one day as an opiate.  The view of it will be obstructed by the all too palpably flawed one you're actually writing.\"Always produce\" is also a heuristic for finding the work you love. If you subject yourself to that constraint, it will automatically push you away from things you think you're supposed to work on, toward things you actually like.  \"Always produce\" will discover your life's work the way water, with the aid of gravity, finds the hole in your roof.Of course, figuring out what you like to work on doesn't mean you get to work on it.  That's a separate question.  And if you're ambitious you have to keep them separate: you have to make a conscious effort to keep your ideas about what you want from being contaminated by what seems possible.  [6]It's painful to keep them apart, because it's painful to observe the gap between them. So most people pre-emptively lower their expectations.  For example, if you asked random people on the street if they'd like to be able to draw like Leonardo,",
      " you'd find most would say something like \"Oh, I can't draw.\"  This is more a statement of intention than fact; it means, I'm not going to try.  Because the fact is, if you took a random person off the street and somehow got them to work as hard as they possibly could at drawing for the next twenty years, they'd get surprisingly far.  But it would require a great moral effort; it would mean staring failure in the eye every day for years.  And so to protect themselves people say \"I can't.\"Another related line you often hear is that not everyone can do work they love\u2014that someone has to do the unpleasant jobs.  Really? How do you make them?  In the US the only mechanism for forcing people to do unpleasant jobs is the draft, and that hasn't been invoked for over 30 years.  All we can do is encourage people to do unpleasant work, with money and prestige.If there's something people still won't do, it seems as if society just has to make do without.  That's what happened with domestic servants.  For millennia that was the canonical example of a job \"someone had to do.\"  And yet in the mid twentieth century servants practically disappeared in rich countries,",
      " and the rich have just had to do without.So while there may be some things someone has to do, there's a good chance anyone saying that about any particular job is mistaken. Most unpleasant jobs would either get automated or go undone if no one were willing to do them.Two RoutesThere's another sense of \"not everyone can do work they love\" that's all too true, however.  One has to make a living, and it's hard to get paid for doing work you love.  There are two routes to that destination:    The organic route: as you become more eminent, gradually to   increase the parts of your job that you like at the expense of   those you don't.The two-job route: to work at things you don't like to get money   to work on things you do.  The organic route is more common.  It happens naturally to anyone who does good work.  A young architect has to take whatever work he can get, but if he does well he'll gradually be in a position to pick and choose among projects.  The disadvantage of this route is that it's slow and uncertain.  Even tenure is not real freedom.The two-job route has several variants depending on how long you work for money at a time.  At one extreme is the \"day job,\" where you work regular hours at one job to make money,",
      " and work on what you love in your spare time.  At the other extreme you work at something till you make enough not to  have to work for money again.The two-job route is less common than the organic route, because it requires a deliberate choice.  It's also more dangerous.  Life tends to get more expensive as you get older, so it's easy to get sucked into working longer than you expected at the money job. Worse still, anything you work on changes you.  If you work too long on tedious stuff, it will rot your brain.  And the best paying jobs are most dangerous, because they require your full attention.The advantage of the two-job route is that it lets you jump over obstacles.  The landscape of possible jobs isn't flat; there are walls of varying heights between different kinds of work.  [7] The trick of maximizing the parts of your job that you like can get you from architecture to product design, but not, probably, to music. If you make money doing one thing and then work on another, you have more freedom of choice.Which route should you take?  That depends on how sure you are of what you want to do, how good you are at taking orders, how much risk you can stand,",
      " and the odds that anyone will pay (in your lifetime) for what you want to do.  If you're sure of the general area you want to work in and it's something people are likely to pay you for, then you should probably take the organic route.  But if you don't know what you want to work on, or don't like to take orders, you may want to take the two-job route, if you can stand the risk.Don't decide too soon.  Kids who know early what they want to do seem impressive, as if they got the answer to some math question before the other kids.  They have an answer, certainly, but odds are it's wrong.A friend of mine who is a quite successful doctor complains constantly about her job.  When people applying to medical school ask her for advice, she wants to shake them and yell \"Don't do it!\"  (But she never does.) How did she get into this fix?  In high school she already wanted to be a doctor.  And she is so ambitious and determined that she overcame every obstacle along the way\u2014including, unfortunately, not liking it.Now she has a life chosen for her by a high-school kid.When you're young, you're given the impression that you'll get enough information to make each choice before you need to make it.",
      " But this is certainly not so with work.  When you're deciding what to do, you have to operate on ridiculously incomplete information. Even in college you get little idea what various types of work are like.  At best you may have a couple internships, but not all jobs offer internships, and those that do don't teach you much more about the work than being a batboy teaches you about playing baseball.In the design of lives, as in the design of most other things, you get better results if you use flexible media.  So unless you're fairly sure what you want to do, your best bet may be to choose a type of work that could turn into either an organic or two-job career.  That was probably part of the reason I chose computers. You can be a professor, or make a lot of money, or morph it into any number of other kinds of work.It's also wise, early on, to seek jobs that let you do many different things, so you can learn faster what various kinds of work are like. Conversely, the extreme version of the two-job route is dangerous because it teaches you so little about what you like.  If you work hard at being a bond trader for ten years, thinking that you'll quit and write novels when you have enough money,",
      " what happens when you quit and then discover that you don't actually like writing novels?Most people would say, I'd take that problem.  Give me a million dollars and I'll figure out what to do.  But it's harder than it looks.  Constraints give your life shape.  Remove them and most people have no idea what to do: look at what happens to those who win lotteries or inherit money.  Much as everyone thinks they want financial security, the happiest people are not those who have it, but those who like what they do.  So a plan that promises freedom at the expense of knowing what to do with it may not be as good as it seems.Whichever route you take, expect a struggle.  Finding work you love is very difficult.  Most people fail.  Even if you succeed, it's rare to be free to work on what you want till your thirties or forties.  But if you have the destination in sight you'll be more likely to arrive at it.  If you know you can love work, you're in the home stretch, and if you know what work you love, you're practically there.Notes[1] Currently we do the opposite: when we make kids do boring work,",
      " like arithmetic drills, instead of admitting frankly that it's boring, we try to disguise it with superficial decorations.[2] One father told me about a related phenomenon: he found himself concealing from his family how much he liked his work.  When he wanted to go to work on a saturday, he found it easier to say that it was because he \"had to\" for some reason, rather than admitting he preferred to work than stay home with them.[3] Something similar happens with suburbs.  Parents move to suburbs to raise their kids in a safe environment, but suburbs are so dull and artificial that by the time they're fifteen the kids are convinced the whole world is boring.[4] I'm not saying friends should be the only audience for your work.  The more people you can help, the better.  But friends should be your compass.[5] Donald Hall said young would-be poets were mistaken to be so obsessed with being published.  But you can imagine what it would do for a 24 year old to get a poem published in The New Yorker. Now to people he meets at parties he's a real poet.  Actually he's no better or worse than he was before, but to a clueless audience like that, the approval of an official authority makes all the difference.",
      "   So it's a harder problem than Hall realizes.  The reason the young care so much about prestige is that the people they want to impress are not very discerning.[6] This is isomorphic to the principle that you should prevent your beliefs about how things are from being contaminated by how you wish they were.  Most people let them mix pretty promiscuously. The continuing popularity of religion is the most visible index of that.[7] A more accurate metaphor would be to say that the graph of jobs is not very well connected.Thanks to Trevor Blackwell, Dan Friedman, Sarah Harlin, Jessica Livingston, Jackie McDonough, Robert Morris, Peter Norvig,  David Sloo, and Aaron Swartz for reading drafts of this.  Want to start a startup?  Get funded by Y Combinator.     October 2014(This essay is derived from a guest lecture in Sam Altman's startup class at Stanford.  It's intended for college students, but much of it is applicable to potential founders at other ages.)One of the advantages of having kids is that when you have to give advice, you can ask yourself \"what would I tell my own kids?\"  My kids are little, but I can imagine what I'd tell them about startups if they were in college,",
      " and that's what I'm going to tell you.Startups are very counterintuitive.  I'm not sure why.  Maybe it's just because knowledge about them hasn't permeated our culture yet. But whatever the reason, starting a startup is a task where you can't always trust your instincts.It's like skiing in that way.  When you first try skiing and you want to slow down, your instinct is to lean back.  But if you lean back on skis you fly down the hill out of control.  So part of learning to ski is learning to suppress that impulse.  Eventually you get new habits, but at first it takes a conscious effort.  At first there's a list of things you're trying to remember as you start down the hill.Startups are as unnatural as skiing, so there's a similar list for startups. Here I'm going to give you the first part of it \u2014 the things to remember if you want to prepare yourself to start a startup. CounterintuitiveThe first item on it is the fact I already mentioned: that startups are so weird that if you trust your instincts, you'll make a lot of mistakes.  If you know nothing more than this, you may at least pause before making them.When I was running Y Combinator I used to joke that our function was to tell founders things they would ignore.",
      "  It's really true. Batch after batch, the YC partners warn founders about mistakes they're about to make, and the founders ignore them, and then come back a year later and say \"I wish we'd listened.\"Why do the founders ignore the partners' advice?  Well, that's the thing about counterintuitive ideas: they contradict your intuitions. They seem wrong.  So of course your first impulse is to disregard them.  And in fact my joking description is not merely the curse of Y Combinator but part of its raison d'etre. If founders' instincts already gave them the right answers, they wouldn't need us.  You only need other people to give you advice that surprises you. That's why there are a lot of ski instructors and not many running instructors. [1]You can, however, trust your instincts about people.  And in fact one of the most common mistakes young founders make is not to do that enough.  They get involved with people who seem impressive, but about whom they feel some misgivings personally.  Later when things blow up they say \"I knew there was something off about him, but I ignored it because he seemed so impressive.\"If you're thinking about getting involved with someone \u2014 as a cofounder,",
      " an employee, an investor, or an acquirer \u2014 and you have misgivings about them, trust your gut.  If someone seems slippery, or bogus, or a jerk, don't ignore it.This is one case where it pays to be self-indulgent. Work with people you genuinely like, and you've known long enough to be sure. ExpertiseThe second counterintuitive point is that it's not that important to know a lot about startups.  The way to succeed in a startup is not to be an expert on startups, but to be an expert on your users and the problem you're solving for them. Mark Zuckerberg didn't succeed because he was an expert on startups. He succeeded despite being a complete noob at startups, because he understood his users really well.If you don't know anything about, say, how to raise an angel round, don't feel bad on that account.  That sort of thing you can learn when you need to, and forget after you've done it.In fact, I worry it's not merely unnecessary to learn in great detail about the mechanics of startups, but possibly somewhat dangerous.  If I met an undergrad who knew all about convertible notes and employee agreements and (God forbid) class FF stock, I wouldn't think \"here is someone who is way ahead of their peers.\" It would set off alarms.",
      "  Because another of the characteristic mistakes of young founders is to go through the motions of starting a startup.  They make up some plausible-sounding idea, raise money at a good valuation, rent a cool office, hire a bunch of people. From the outside that seems like what startups do.  But the next step after rent a cool office and hire a bunch of people is: gradually realize how completely fucked they are, because while imitating all the outward forms of a startup they have neglected the one thing that's actually essential: making something people want. GameWe saw this happen so often that we made up a name for it: playing house.  Eventually I realized why it was happening.  The reason young founders go through the motions of starting a startup is because that's what they've been trained to do for their whole lives up to that point.  Think about what you have to do to get into college, for example.  Extracurricular activities, check.  Even in college classes most of the work is as artificial as running laps.I'm not attacking the educational system for being this way. There will always be a certain amount of fakeness in the work you do when you're being taught something, and if you measure their performance it's inevitable that people will exploit the difference to the point where much of what you're measuring is artifacts of the fakeness.I confess I did it myself in college.",
      " I found that in a lot of classes there might only be 20 or 30 ideas that were the right shape to make good exam questions.  The way I studied for exams in these classes was not (except incidentally) to master the material taught in the class, but to make a list of potential exam questions and work out the answers in advance. When I walked into the final, the main thing I'd be feeling was curiosity about which of my questions would turn up on the exam.  It was like a game.It's not surprising that after being trained for their whole lives to play such games, young founders' first impulse on starting a startup is to try to figure out the tricks for winning at this new game. Since fundraising appears to be the measure of success for startups (another classic noob mistake), they always want to know what the tricks are for convincing investors.  We tell them the best way to convince investors is to make a startup that's actually doing well, meaning growing fast, and then simply tell investors so.  Then they want to know what the tricks are for growing fast.  And we have to tell them the best way to do that is simply to make something people want.So many of the conversations YC partners have with young founders begin with the founder asking \"How do we...\" and the partner replying \"Just...\"Why do the founders always make things so complicated?",
      "  The reason, I realized, is that they're looking for the trick.So this is the third counterintuitive thing to remember about startups: starting a startup is where gaming the system stops working.  Gaming the system may continue to work if you go to work for a big company. Depending on how broken the company is, you can succeed by sucking up to the right people, giving the impression of productivity, and so on.  [2] But that doesn't work with startups. There is no boss to trick, only users, and all users care about is whether your product does what they want. Startups are as impersonal as physics.  You have to make something people want, and you prosper only to the extent you do.The dangerous thing is, faking does work to some degree on investors. If you're super good at sounding like you know what you're talking about, you can fool investors for at least one and perhaps even two rounds of funding.  But it's not in your interest to.  The company is ultimately doomed.  All you're doing is wasting your own time riding it down.So stop looking for the trick. There are tricks in startups, as there are in any domain, but they are an order of magnitude less important than solving the real problem.",
      " A founder who knows nothing about fundraising but has made something users love will have an easier time raising money than one who knows every trick in the book but has a flat usage graph. And more importantly, the founder who has made something users love is the one who will go on to succeed after raising the money.Though in a sense it's bad news in that you're deprived of one of your most powerful weapons, I think it's exciting that gaming the system stops working when you start a startup.  It's exciting that there even exist parts of the world where you win by doing good work.  Imagine how depressing the world would be if it were all like school and big companies, where you either have to spend a lot of time on bullshit things or lose to people who do. [3] I would have been delighted if I'd realized in college that there were parts of the real world where gaming the system mattered less than others, and a few where it hardly mattered at all.  But there are, and this variation is one of the most important things to consider when you're thinking about your future.  How do you win in each type of work, and what would you like to win by doing? [4] All-ConsumingThat brings us to our fourth counterintuitive point:",
      " startups are all-consuming.  If you start a startup, it will take over your life to a degree you cannot imagine.  And if your startup succeeds, it will take over your life for a long time: for several years at the very least, maybe for a decade, maybe for the rest of your working life.  So there is a real opportunity cost here.Larry Page may seem to have an enviable life, but there are aspects of it that are unenviable.  Basically at 25 he started running as fast as he could and it must seem to him that he hasn't stopped to catch his breath since.  Every day new shit happens in the Google empire that only the CEO can deal with, and he, as CEO, has to deal with it.  If he goes on vacation for even a week, a whole week's backlog of shit accumulates.  And he has to bear this uncomplainingly, partly because as the company's daddy he can never show fear or weakness, and partly because billionaires get less than zero sympathy if they talk about having difficult lives.  Which has the strange side effect that the difficulty of being a successful startup founder is concealed from almost everyone except those who've done it.Y Combinator has now funded several companies that can be called big successes,",
      " and in every single case the founders say the same thing.  It never gets any easier.  The nature of the problems change. You're worrying about construction delays at your London office instead of the broken air conditioner in your studio apartment. But the total volume of worry never decreases; if anything it increases.Starting a successful startup is similar to having kids in that it's like a button you push that changes your life irrevocably. And while it's truly wonderful having kids, there are a lot of things that are easier to do before you have them than after.  Many of which will make you a better parent when you do have kids. And since you can delay pushing the button for a while, most people in rich countries do.Yet when it comes to startups, a lot of people seem to think they're supposed to start them while they're still in college.  Are you crazy?  And what are the universities thinking?  They go out of their way to ensure their students are well supplied with contraceptives, and yet they're setting up entrepreneurship programs and startup incubators left and right.To be fair, the universities have their hand forced here.  A lot of incoming students are interested in startups.  Universities are, at least de facto,",
      " expected to prepare them for their careers.  So students who want to start startups hope universities can teach them about startups.  And whether universities can do this or not, there's some pressure to claim they can, lest they lose applicants to other universities that do.Can universities teach students about startups?  Yes and no.  They can teach students about startups, but as I explained before, this is not what you need to know.  What you need to learn about are the needs of your own users, and you can't do that until you actually start the company. [5] So starting a startup is intrinsically something you can only really learn by doing it.  And it's impossible to do that in college, for the reason I just explained: startups take over your life.  You can't start a startup for real as a student, because if you start a startup for real you're not a student anymore. You may be nominally a student for a bit, but you won't even be that for long. [6]Given this dichotomy, which of the two paths should you take?  Be a real student and not start a startup, or start a real startup and not be a student?  I can answer that one for you.",
      " Do not start a startup in college.  How to start a startup is just a subset of a bigger problem you're trying to solve: how to have a good life. And though starting a startup can be part of a good life for a lot of ambitious people, age 20 is not the optimal time to do it. Starting a startup is like a brutally fast depth-first search.  Most people should still be searching breadth-first at 20.You can do things in your early 20s that you can't do as well before or after, like plunge deeply into projects on a whim and travel super cheaply with no sense of a deadline.  For unambitious people, this sort of thing is the dreaded \"failure to launch,\" but for the ambitious ones it can be an incomparably valuable sort of exploration. If you start a startup at 20 and you're sufficiently successful, you'll never get to do it. [7]Mark Zuckerberg will never get to bum around a foreign country.  He can do other things most people can't, like charter jets to fly him to foreign countries. But success has taken a lot of the serendipity out of his life. Facebook is running him as much as he's running Facebook. And while it can be very cool to be in the grip of a project you consider your life's work,",
      " there are advantages to serendipity too, especially early in life.  Among other things it gives you more options to choose your life's work from.There's not even a tradeoff here. You're not sacrificing anything if you forgo starting a startup at 20, because you're more likely to succeed if you wait.  In the unlikely case that you're 20 and one of your side projects takes off like Facebook did, you'll face a choice of running with it or not, and it may be reasonable to run with it.  But the usual way startups take off is for the founders to make them take off, and it's gratuitously stupid to do that at 20. TryShould you do it at any age?  I realize I've made startups sound pretty hard.  If I haven't, let me try again: starting a startup is really hard.  What if it's too hard?  How can you tell if you're up to this challenge?The answer is the fifth counterintuitive point: you can't tell. Your life so far may have given you some idea what your prospects might be if you tried to become a mathematician, or a professional football player.  But unless you've had a very strange life you haven't done much that was like being a startup founder.",
      " Starting a startup will change you a lot.  So what you're trying to estimate is not just what you are, but what you could grow into, and who can do that?For the past 9 years it was my job to predict whether people would have what it took to start successful startups.  It was easy to tell how smart they were, and most people reading this will be over that threshold.  The hard part was predicting how tough and ambitious they would become.  There may be no one who has more experience at trying to predict that, so I can tell you how much an expert can know about it, and the answer is: not much.  I learned to keep a completely open mind about which of the startups in each batch would turn out to be the stars.The founders sometimes think they know. Some arrive feeling sure they will ace Y Combinator just as they've aced every one of the (few, artificial, easy) tests they've faced in life so far.  Others arrive wondering how they got in, and hoping YC doesn't discover whatever mistake caused it to accept them.  But there is little correlation between founders' initial attitudes and how well their companies do.I've read that the same is true in the military \u2014 that the swaggering recruits are no more likely to turn out to be really tough than the quiet ones.",
      " And probably for the same reason: that the tests involved are so different from the ones in their previous lives.If you're absolutely terrified of starting a startup, you probably shouldn't do it.  But if you're merely unsure whether you're up to it, the only way to find out is to try.  Just not now. IdeasSo if you want to start a startup one day, what should you do in college?  There are only two things you need initially: an idea and cofounders.  And the m.o. for getting both is the same.  Which leads to our sixth and last counterintuitive point: that the way to get startup ideas is not to try to think of startup ideas.I've written a whole essay on this, so I won't repeat it all here.  But the short version is that if you make a conscious effort to think of startup ideas, the ideas you come up with will not merely be bad, but bad and plausible-sounding, meaning you'll waste a lot of time on them before realizing they're bad.The way to come up with good startup ideas is to take a step back. Instead of making a conscious effort to think of startup ideas, turn your mind into the type that startup ideas form in without any conscious effort.",
      "  In fact, so unconsciously that you don't even realize at first that they're startup ideas.This is not only possible, it's how Apple, Yahoo, Google, and Facebook all got started.  None of these companies were even meant to be companies at first.  They were all just side projects.  The best startups almost have to start as side projects, because great ideas tend to be such outliers that your conscious mind would reject them as ideas for companies.Ok, so how do you turn your mind into the type that startup ideas form in unconsciously?  (1) Learn a lot about things that matter, then (2) work on problems that interest you (3) with people you like and respect.  The third part, incidentally, is how you get cofounders at the same time as the idea.The first time I wrote that paragraph, instead of \"learn a lot about things that matter,\" I wrote \"become good at some technology.\" But that prescription, though sufficient, is too narrow.  What was special about Brian Chesky and Joe Gebbia was not that they were experts in technology.  They were good at design, and perhaps even more importantly, they were good at organizing groups and making projects happen.  So you don't have to work on technology per se,",
      " so long as you work on problems demanding enough to stretch you.What kind of problems are those?  That is very hard to answer in the general case.  History is full of examples of young people who were working on important problems that no one else at the time thought were important, and in particular that their parents didn't think were important.  On the other hand, history is even fuller of examples of parents who thought their kids were wasting their time and who were right.  So how do you know when you're working on real stuff? [8]I know how I know.  Real problems are interesting, and I am self-indulgent in the sense that I always want to work on interesting things, even if no one else cares about them (in fact, especially if no one else cares about them), and find it very hard to make myself work on boring things, even if they're supposed to be important.My life is full of case after case where I worked on something just because it seemed interesting, and it turned out later to be useful in some worldly way.  Y Combinator itself was something I only did because it seemed interesting. So I seem to have some sort of internal compass that helps me out.  But I don't know what other people have in their heads.",
      " Maybe if I think more about this I can come up with heuristics for recognizing genuinely interesting problems, but for the moment the best I can offer is the hopelessly question-begging advice that if you have a taste for genuinely interesting problems, indulging it energetically is the best way to prepare yourself for a startup. And indeed, probably also the best way to live. [9]But although I can't explain in the general case what counts as an interesting problem, I can tell you about a large subset of them. If you think of technology as something that's spreading like a sort of fractal stain, every moving point on the edge represents an interesting problem.  So one guaranteed way to turn your mind into the type that has good startup ideas is to get yourself to the leading edge of some technology \u2014 to cause yourself, as Paul Buchheit put it, to \"live in the future.\" When you reach that point, ideas that will seem to other people uncannily prescient will seem obvious to you.  You may not realize they're startup ideas, but you'll know they're something that ought to exist.For example, back at Harvard in the mid 90s a fellow grad student of my friends Robert and Trevor wrote his own voice over IP software.",
      " He didn't mean it to be a startup, and he never tried to turn it into one.  He just wanted to talk to his girlfriend in Taiwan without paying for long distance calls, and since he was an expert on networks it seemed obvious to him that the way to do it was turn the sound into packets and ship it over the Internet. He never did any more with his software than talk to his girlfriend, but this is exactly the way the best startups get started.So strangely enough the optimal thing to do in college if you want to be a successful startup founder is not some sort of new, vocational version of college focused on \"entrepreneurship.\" It's the classic version of college as education for its own sake. If you want to start a startup after college, what you should do in college is learn powerful things.  And if you have genuine intellectual curiosity, that's what you'll naturally tend to do if you just follow your own inclinations. [10]The component of entrepreneurship that really matters is domain expertise.  The way to become Larry Page was to become an expert on search. And the way to become an expert on search was to be driven by genuine curiosity, not some ulterior motive.At its best, starting a startup is merely an ulterior motive for curiosity.",
      "  And you'll do it best if you introduce the ulterior motive toward the end of the process.So here is the ultimate advice for young would-be startup founders, boiled down to two words: just learn. Notes[1] Some founders listen more than others, and this tends to be a predictor of success. One of the things I remember about the Airbnbs during YC is how intently they listened.[2] In fact, this is one of the reasons startups are possible.  If big companies weren't plagued by internal inefficiencies, they'd be proportionately more effective, leaving less room for startups.[3] In a startup you have to spend a lot of time on schleps, but this sort of work is merely unglamorous, not bogus.[4] What should you do if your true calling is gaming the system? Management consulting.[5] The company may not be incorporated, but if you start to get significant numbers of users, you've started it, whether you realize it yet or not.[6] It shouldn't be that surprising that colleges can't teach students how to be good startup founders, because they can't teach them how to be good employees either.The way universities \"teach\" students how to be employees is to hand off the task to companies via internship programs.",
      "  But you couldn't do the equivalent thing for startups, because by definition if the students did well they would never come back.[7] Charles Darwin was 22 when he received an invitation to travel aboard the HMS Beagle as a naturalist.  It was only because he was otherwise unoccupied, to a degree that alarmed his family, that he could accept it. And yet if he hadn't we probably would not know his name.[8] Parents can sometimes be especially conservative in this department.  There are some whose definition of important problems includes only those on the critical path to med school.[9] I did manage to think of a heuristic for detecting whether you have a taste for interesting ideas: whether you find known boring ideas intolerable.  Could you endure studying literary theory, or working in middle management at a large company?[10] In fact, if your goal is to start a startup, you can stick even more closely to the ideal of a liberal education than past generations have. Back when students focused mainly on getting a job after college, they thought at least a little about how the courses they took might look to an employer.  And perhaps even worse, they might shy away from taking a difficult class lest they get a low grade, which would harm their all-important GPA.",
      "  Good news: users don't care what your GPA was.  And I've never heard of investors caring either.  Y Combinator certainly never asks what classes you took in college or what grades you got in them. Thanks to Sam Altman, Paul Buchheit, John Collison, Patrick Collison, Jessica Livingston, Robert Morris, Geoff Ralston, and Fred Wilson for reading drafts of this.July 2010What hard liquor, cigarettes, heroin, and crack have in common is that they're all more concentrated forms of less addictive predecessors. Most if not all the things we describe as addictive are.  And the scary thing is, the process that created them is accelerating.We wouldn't want to stop it.  It's the same process that cures diseases: technological progress.  Technological progress means making things do more of what we want.  When the thing we want is something we want to want, we consider technological progress good. If some new technique makes solar cells x% more efficient, that seems strictly better.  When progress concentrates something we don't want to want\u2014when it transforms opium into heroin\u2014it seems bad.  But it's the same process at work. [1]No one doubts this process is accelerating, which means increasing numbers of things we like will be transformed into things we like too much.",
      " [2]As far as I know there's no word for something we like too much. The closest is the colloquial sense of \"addictive.\" That usage has become increasingly common during my lifetime.  And it's clear why: there are an increasing number of things we need it for.  At the extreme end of the spectrum are crack and meth.  Food has been transformed by a combination of factory farming and innovations in food processing into something with way more immediate bang for the buck, and you can see the results in any town in America.  Checkers and solitaire have been replaced by World of Warcraft and FarmVille. TV has become much more engaging, and even so it can't compete with Facebook.The world is more addictive than it was 40 years ago.   And unless the forms of technological progress that produced these things are subject to different laws than technological progress in general, the world will get more addictive in the next 40 years than it did in the last 40.The next 40 years will bring us some wonderful things.  I don't mean to imply they're all to be avoided.  Alcohol is a dangerous drug, but I'd rather live in a world with wine than one without. Most people can coexist with alcohol;",
      " but you have to be careful. More things we like will mean more things we have to be careful about.Most people won't, unfortunately.  Which means that as the world becomes more addictive, the two senses in which one can live a normal life will be driven ever further apart.  One sense of \"normal\" is statistically normal: what everyone else does.  The other is the sense we mean when we talk about the normal operating range of a piece of machinery: what works best.These two senses are already quite far apart.  Already someone trying to live well would seem eccentrically abstemious in most of the US.  That phenomenon is only going to become more pronounced. You can probably take it as a rule of thumb from now on that if people don't think you're weird, you're living badly.Societies eventually develop antibodies to addictive new"
    ]
  },
  {
    "id": 82,
    "question": "What is the best way to learn about different cultures?",
    "answer": "by traveling and interacting with locals.",
    "docs": [
      "Want to start a startup?  Get funded by Y Combinator.     October 2010After barely changing at all for decades, the startup funding business is now in what could, at least by comparison, be called turmoil.  At Y Combinator we've seen dramatic changes in the funding environment for startups.  Fortunately one of them is much higher valuations.The trends we've been seeing are probably not YC-specific.  I wish I could say they were, but the main cause is probably just that we see trends first\u2014partly because the startups we fund are very plugged into the Valley and are quick to take advantage of anything new, and partly because we fund so many that we have enough data points to see patterns clearly.What we're seeing now, everyone's probably going to be seeing in the next couple years.  So I'm going to explain what we're seeing, and what that will mean for you if you try to raise money.Super-AngelsLet me start by describing what the world of startup funding used to look like.  There used to be two sharply differentiated types of investors: angels and venture capitalists.  Angels are individual rich people who invest small amounts of their own money, while VCs are employees of funds that invest large amounts of other people's.For decades there were just those two types of investors,",
      " but now a third type has appeared halfway between them: the so-called super-angels.  [1]   And VCs have been provoked by their arrival into making a lot of angel-style investments themselves.  So the previously sharp line between angels and VCs has become hopelessly blurred.There used to be a no man's land between angels and VCs.  Angels would invest $20k to $50k apiece, and VCs usually a million or more. So an angel round meant a collection of angel investments that combined to maybe $200k, and a VC round meant a series A round in which a single VC fund (or occasionally two) invested $1-5 million.The no man's land between angels and VCs was a very inconvenient one for startups, because it coincided with the amount many wanted to raise.  Most startups coming out of Demo Day wanted to raise around $400k.  But it was a pain to stitch together that much out of angel investments, and most VCs weren't interested in investments so small.  That's the fundamental reason the super-angels have appeared.  They're responding to the market.The arrival of a new type of investor is big news for startups, because there used to be only two and they rarely competed with one another.",
      "  Super-angels compete with both angels and VCs.  That's going to change the rules about how to raise money.  I don't know yet what the new rules will be, but it looks like most of the changes will be for the better.A super-angel has some of the qualities of an angel, and some of the qualities of a VC.  They're usually individuals, like angels. In fact many of the current super-angels were initially angels of the classic type.  But like VCs, they invest other people's money. This allows them to invest larger amounts than angels:  a typical super-angel investment is currently about $100k.  They make investment decisions quickly, like angels.  And they make a lot more investments per partner than VCs\u2014up to 10 times as many.The fact that super-angels invest other people's money makes them doubly alarming to VCs. They don't just compete for startups; they also compete for investors.  What super-angels really are is a new form of fast-moving, lightweight VC fund.   And those of us in the technology world know what usually happens when something comes along that can be described in terms like that.  Usually it's the replacement.Will it be?",
      "  As of now, few of the startups that take money from super-angels are ruling out taking VC money.  They're just postponing it.  But that's still a problem for VCs.  Some of the startups that postpone raising VC money may do so well on the angel money they raise that they never bother to raise more.  And those who do raise VC rounds will be able to get higher valuations when they do.  If the best startups get 10x higher valuations when they raise series A rounds, that would cut VCs' returns from winners at least tenfold. [2]So I think VC funds are seriously threatened by the super-angels. But one thing that may save them to some extent is the uneven distribution of startup outcomes: practically all the returns are concentrated in a few big successes.  The expected value of a startup is the percentage chance it's Google.  So to the extent that winning is a matter of absolute returns, the super-angels could win practically all the battles for individual startups and yet lose the war, if they merely failed to get those few big winners.  And there's a chance that could happen, because the top VC funds have better brands, and can also do more for their portfolio companies.",
      "   [3]Because super-angels make more investments per partner, they have less partner per investment.  They can't pay as much attention to you as a VC on your board could.  How much is that extra attention worth?  It will vary enormously from one partner to another.  There's no consensus yet in the general case.  So for now this is something startups are deciding individually.Till now, VCs' claims about how much value they added were sort of like the government's.  Maybe they made you feel better, but you had no choice in the matter, if you needed money on the scale only VCs could supply.  Now that VCs have competitors, that's going to put a market price on the help they offer.  The interesting thing is, no one knows yet what it will be.Do startups that want to get really big need the sort of advice and connections only the top VCs can supply?  Or would super-angel money do just as well?  The VCs will say you need them, and the super-angels will say you don't.  But the truth is, no one knows yet, not even the VCs and super-angels themselves.   All the super-angels know is that their new model seems promising enough to be worth trying,",
      " and all the VCs know is that it seems promising enough to worry about.RoundsWhatever the outcome, the conflict between VCs and super-angels is good news for founders.  And not just for the obvious reason that more competition for deals means better terms.  The whole shape of deals is changing.One of the biggest differences between angels and VCs is the amount of your company they want.  VCs want a lot.  In a series A round they want a third of your company, if they can get it.  They don't care much how much they pay for it, but they want a lot because the number of series A investments they can do is so small.  In a traditional series A investment, at least one partner from the VC fund takes a seat on your board.   [4]  Since board seats last about 5 years and each partner can't handle more than about 10 at once, that means a VC fund can only do about 2 series A deals per partner per year. And that means they need to get as much of the company as they can in each one.  You'd have to be a very promising startup indeed to get a VC to use up one of his 10 board seats for only a few percent of you.Since angels generally don't take board seats,",
      " they don't have this constraint.  They're happy to buy only a few percent of you.  And although the super-angels are in most respects mini VC funds, they've retained this critical property of angels.  They don't take board seats, so they don't need a big percentage of your company.Though that means you'll get correspondingly less attention from them, it's good news in other respects.  Founders never really liked giving up as much equity as VCs wanted.  It was a lot of the company to give up in one shot.  Most founders doing series A deals would prefer to take half as much money for half as much stock, and then see what valuation they could get for the second half of the stock after using the first half of the money to increase its value.  But VCs never offered that option.Now startups have another alternative.  Now it's easy to raise angel rounds about half the size of series A rounds.  Many of the startups we fund are taking this route, and I predict that will be true of startups in general.A typical big angel round might be $600k on a convertible note with a valuation cap of $4 million premoney.  Meaning that when the note converts into stock (in a later round,",
      " or upon acquisition), the investors in that round will get.6 / 4.6, or 13% of the company. That's a lot less than the 30 to 40% of the company you usually give up in a series A round if you do it so early.   [5]But the advantage of these medium-sized rounds is not just that they cause less dilution.  You also lose less control.  After an angel round, the founders almost always still have control of the company, whereas after a series A round they often don't.  The traditional board structure after a series A round is two founders, two VCs, and a (supposedly) neutral fifth person.  Plus series A terms usually give the investors a veto over various kinds of important decisions, including selling the company.  Founders usually have a lot of de facto control after a series A, as long as things are going well.  But that's not the same as just being able to do what you want, like you could before.A third and quite significant advantage of angel rounds is that they're less stressful to raise.  Raising a traditional series A round has in the past taken weeks, if not months.  When a VC firm can only do 2 deals per partner per year,",
      " they're careful about which they do.  To get a traditional series A round you have to go through a series of meetings, culminating in a full partner meeting where the firm as a whole says yes or no.  That's the really scary part for founders: not just that series A rounds take so long, but at the end of this long process the VCs might still say no.  The chance of getting rejected after the full partner meeting averages about 25%.  At some firms it's over 50%.Fortunately for founders, VCs have been getting a lot faster. Nowadays Valley VCs are more likely to take 2 weeks than 2 months. But they're still not as fast as angels and super-angels, the most decisive of whom sometimes decide in hours.Raising an angel round is not only quicker, but you get feedback as it progresses.  An angel round is not an all or nothing thing like a series A.  It's composed of multiple investors with varying degrees of seriousness, ranging from the upstanding ones who commit unequivocally to the jerks who give you lines like \"come back to me to fill out the round.\" You usually start collecting money from the most committed investors and work your way out toward the ambivalent ones,",
      " whose interest increases as the round fills up.But at each point you know how you're doing.  If investors turn cold you may have to raise less, but when investors in an angel round turn cold the process at least degrades gracefully, instead of blowing up in your face and leaving you with nothing, as happens if you get rejected by a VC fund after a full partner meeting. Whereas if investors seem hot, you can not only close the round faster, but now that convertible notes are becoming the norm, actually raise the price to reflect demand.ValuationHowever, the VCs have a weapon they can use against the super-angels, and they have started to use it.   VCs have started making angel-sized investments too.  The term \"angel round\" doesn't mean that all the investors in it are angels; it just describes the structure of the round.  Increasingly the participants include VCs making investments of a hundred thousand or two.  And when VCs invest in angel rounds they can do things that super-angels don't like.  VCs are quite valuation-insensitive in angel rounds\u2014partly because they are in general, and partly because they don't care that much about the returns on angel rounds, which they still view mostly as a way to recruit startups for series A rounds later.",
      "  So VCs who invest in angel rounds can blow up the valuations for angels and super-angels who invest in them.  [6]Some super-angels seem to care about valuations.  Several turned down YC-funded startups after Demo Day because their valuations were too high.  This was not a problem for the startups; by definition a high valuation means enough investors were willing to accept it. But it was mysterious to me that the super-angels would quibble about valuations.  Did they not understand that the big returns come from a few big successes, and that it therefore mattered far more which startups you picked than how much you paid for them?After thinking about it for a while and observing certain other signs, I have a theory that explains why the super-angels may be smarter than they seem.  It would make sense for super-angels to want low valuations if they're hoping to invest in startups that get bought early.  If you're hoping to hit the next Google, you shouldn't care if the valuation is 20 million.  But if you're looking for companies that are going to get bought for 30 million, you care. If you invest at 20 and the company gets bought for 30,",
      " you only get 1.5x.  You might as well buy Apple.So if some of the super-angels were looking for companies that could get acquired quickly, that would explain why they'd care about valuations.  But why would they be looking for those?   Because depending on the meaning of \"quickly,\" it could actually be very profitable.  A company that gets acquired for 30 million is a failure to a VC, but it could be a 10x return for an angel, and moreover, a quick 10x return.  Rate of return is what matters in investing\u2014not the multiple you get, but the multiple per year. If a super-angel gets 10x in one year, that's a higher rate of return than a VC could ever hope to get from a company that took 6 years to go public.  To get the same rate of return, the VC would have to get a multiple of 10^6\u2014one million x.  Even Google didn't come close to that.So I think at least some super-angels are looking for companies that will get bought.  That's the only rational explanation for focusing on getting the right valuations, instead of the right companies.  And if so they'll be different to deal with than VCs.",
      " They'll be tougher on valuations, but more accommodating if you want to sell early.PrognosisWho will win, the super-angels or the VCs?  I think the answer to that is, some of each.  They'll each become more like one another. The super-angels will start to invest larger amounts, and the VCs will gradually figure out ways to make more, smaller investments faster.  A decade from now the players will be hard to tell apart, and there will probably be survivors from each group.What does that mean for founders?  One thing it means is that the high valuations startups are presently getting may not last forever. To the extent that valuations are being driven up by price-insensitive VCs, they'll fall again if VCs become more like super-angels and start to become more miserly about valuations.  Fortunately if this does happen it will take years.The short term forecast is more competition between investors, which is good news for you.  The super-angels will try to undermine the VCs by acting faster, and the VCs will try to undermine the super-angels by driving up valuations.  Which for founders will result in the perfect combination: funding rounds that close fast,",
      " with high valuations.But remember that to get that combination, your startup will have to appeal to both super-angels and VCs.  If you don't seem like you have the potential to go public, you won't be able to use VCs to drive up the valuation of an angel round.There is a danger of having VCs in an angel round: the so-called signalling risk.  If VCs are only doing it in the hope of investing more later, what happens if they don't?  That's a signal to everyone else that they think you're lame.How much should you worry about that?  The seriousness of signalling risk depends on how far along you are.  If by the next time you need to raise money, you have graphs showing rising revenue or traffic month after month, you don't have to worry about any signals your existing investors are sending.  Your results will speak for themselves.   [7]Whereas if the next time you need to raise money you won't yet have concrete results, you may need to think more about the message your investors might send if they don't invest more.  I'm not sure yet how much you have to worry, because this whole phenomenon of VCs doing angel investments is so new. But my instincts tell me you don't have to worry much.",
      "  Signalling risk smells like one of those things founders worry about that's not a real problem.  As a rule, the only thing that can kill a good startup is the startup itself. Startups hurt themselves way more often than competitors hurt them, for example.  I suspect signalling risk is in this category too.One thing YC-funded startups have been doing to mitigate the risk of taking money from VCs in angel rounds is not to take too much from any one VC.  Maybe that will help, if you have the luxury of turning down money.Fortunately, more and more startups will.  After decades of competition that could best be described as intramural, the startup funding business is finally getting some real competition.  That should last several years at least, and maybe a lot longer. Unless there's some huge market crash, the next couple years are going to be a good time for startups to raise money.  And that's exciting because it means lots more startups will happen. Notes[1] I've also heard them called \"Mini-VCs\" and \"Micro-VCs.\" I don't know which name will stick.There were a couple predecessors.  Ron Conway had angel funds starting in the 1990s, and in some ways First Round Capital is closer to a super-",
      "angel than a VC fund.[2] It wouldn't cut their overall returns tenfold, because investing later would probably (a) cause them to lose less on investments that failed, and (b) not allow them to get as large a percentage of startups as they do now.  So it's hard to predict precisely what would happen to their returns.[3] The brand of an investor derives mostly from the success of their portfolio companies.  The top VCs thus have a big brand advantage over the super-angels.  They could make it self-perpetuating if they used it to get all the best new startups.  But I don't think they'll be able to.  To get all the best startups, you have to do more than make them want you.  You also have to want them; you have to recognize them when you see them, and that's much harder. Super-angels will snap up stars that VCs miss.  And that will cause the brand gap between the top VCs and the super-angels gradually to erode.[4] Though in a traditional series A round VCs put two partners on your board, there are signs now that VCs may begin to conserve board seats by switching to what used to be considered an angel-round board,",
      " consisting of two founders and one VC.  Which is also to the founders' advantage if it means they still control the company.[5] In a series A round, you usually have to give up more than the actual amount of stock the VCs buy, because they insist you dilute yourselves to set aside an \"option pool\" as well.  I predict this practice will gradually disappear though.[6] The best thing for founders, if they can get it, is a convertible note with no valuation cap at all.  In that case the money invested in the angel round just converts into stock at the valuation of the next round, no matter how large.  Angels and super-angels tend not to like uncapped notes. They have no idea how much of the company they're buying.  If the company does well and the valuation of the next round is high, they may end up with only a sliver of it.  So by agreeing to uncapped notes, VCs who don't care about valuations in angel rounds can make offers that super-angels hate to match.[7] Obviously signalling risk is also not a problem if you The best way to learn about different cultures is by traveling and interacting with locals.'ll never need to raise more money.  But startups are often mistaken about that.Thanks to Sam Altman,",
      " John Bautista, Patrick Collison, James Lindenbaum, Reid Hoffman, Jessica Livingston and Harj Taggar for reading drafts of this.December 2001 (rev. May 2002)  (This article came about in response to some questions on the LL1 mailing list.  It is now incorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s, it was a radical departure from existing languages, the most important of which was Fortran.Lisp embodied nine new ideas: 1. Conditionals.  A conditional is an if-then-else construct.  We take these for granted now.  They were  invented by McCarthy in the course of developing Lisp.  (Fortran at that time only had a conditional goto, closely based on the branch instruction in the  underlying hardware.)  McCarthy, who was on the Algol committee, got conditionals into Algol, whence they spread to most other languages.2. A function type. In Lisp, functions are first class  objects-- they're a data type just like integers, strings, etc, and have a literal representation, can be stored in variables, can be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept before Lisp of course,",
      " but Lisp was the first programming language to support it.  (It's arguably implicit in making functions first class objects.)4. A new concept of variables.  In Lisp, all variables are effectively pointers. Values are what have types, not variables, and assigning or binding variables means copying pointers, not what they point to.5. Garbage-collection.6. Programs composed of expressions. Lisp programs are  trees of expressions, each of which returns a value.   (In some Lisps expressions can return multiple values.)  This is in contrast to Fortran and most succeeding languages, which distinguish between expressions and statements.It was natural to have this distinction in Fortran because (not surprisingly in a language where the input format was punched cards) the language was line-oriented.  You could not nest statements.  And so while you needed expressions for math to work, there was no point in making anything else return a value, because there could not be anything waiting for it.This limitation went away with the arrival of block-structured languages, but by then it was too late. The distinction between expressions and statements was entrenched.  It spread from  Fortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can compose expressions however you want.",
      "  You can say either (using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that you can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.   There is no real distinction between read-time, compile-time, and runtime. You can compile or run code while reading, read or run code while compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax; running code at compile-time is the basis of macros; compiling at runtime is the basis of Lisp's use as an extension language in programs like Emacs; and reading at runtime enables programs to communicate using s-expressions, an idea recently reinvented as XML. When Lisp was first invented, all these ideas were far removed from ordinary programming practice, which was dictated largely by the hardware available in the late 1950s.Over time, the default language, embodied in a succession of popular languages, has gradually evolved toward Lisp.  1-5 are now widespread. 6 is starting to appear in the mainstream. Python has a form of 7,",
      " though there doesn't seem to be any syntax for it.   8, which (with 9) is what makes Lisp macros possible, is so far still unique to Lisp, perhaps because (a) it requires those parens, or something  just as bad, and (b) if you add that final increment of power,  you can no  longer claim to have invented a new language, but only to have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's strange to describe Lisp in terms of its variation from the random expedients other languages adopted.  That was not, probably, how McCarthy thought of it.  Lisp wasn't designed to fix the mistakes in Fortran; it came about more as the byproduct of an attempt to axiomatize computation.  Want to start a startup?  Get funded by Y Combinator.     November 2009I don't think Apple realizes how badly the App Store approval process is broken.  Or rather, I don't think they realize how much it matters that it's broken.The way Apple runs the App Store has harmed their reputation with programmers more than anything else they've ever done.  Their reputation with programmers used to be great. It used to be the most common complaint you heard about Apple was that their fans admired them too uncritically.",
      " The App Store has changed that.  Now a lot of programmers have started to see Apple as evil.How much of the goodwill Apple once had with programmers have they lost over the App Store?  A third?  Half?  And that's just so far. The App Store is an ongoing karma leak.* * *How did Apple get into this mess?  Their fundamental problem is that they don't understand software.They treat iPhone apps the way they treat the music they sell through iTunes.  Apple is the channel; they own the user; if you want to reach users, you do it on their terms. The record labels agreed, reluctantly.  But this model doesn't work for software.  It doesn't work for an intermediary to own the user.  The software business learned that in the early 1980s, when companies like VisiCorp showed that although the words \"software\" and \"publisher\" fit together, the underlying concepts don't.  Software isn't like music or books. It's too complicated for a third party to act as an intermediary between developer and user.   And yet that's what Apple is trying to be with the App Store: a software publisher.  And a particularly overreaching one at that, with fussy tastes and a rigidly enforced house style.If software publishing didn't work in 1980,",
      " it works even less now that software development has evolved from a small number of big releases to a constant stream of small ones.  But Apple doesn't understand that either.  Their model of product development derives from hardware.  They work on something till they think it's finished, then they release it.  You have to do that with hardware, but because software is so easy to change, its design can benefit from evolution. The standard way to develop applications now is to launch fast and iterate.  Which means it's a disaster to have long, random delays each time you release a new version.Apparently Apple's attitude is that developers should be more careful when they submit a new version to the App Store.  They would say that.  But powerful as they are, they're not powerful enough to turn back the evolution of technology.  Programmers don't use launch-fast-and-iterate out of laziness.  They use it because it yields the best results.  By obstructing that process, Apple is making them do bad work, and programmers hate that as much as Apple would.How would Apple like it if when they discovered a serious bug in OS\u00a0X, instead of releasing a software update immediately, they had to submit their code to an intermediary who sat on it for a month and then rejected it because it contained an icon they didn't like?",
      "By breaking software development, Apple gets the opposite of what they intended: the version of an app currently available in the App Store tends to be an old and buggy one.  One developer told me:    As a result of their process, the App Store is full of half-baked   applications. I make a new version almost every day that I release   to beta users. The version on the App Store feels old and crappy.   I'm sure that a lot of developers feel this way: One emotion is   \"I'm not really proud about what's in the App Store\", and it's   combined with the emotion \"Really, it's Apple's fault.\"  Another wrote:    I believe that they think their approval process helps users by   ensuring quality.  In reality, bugs like ours get through all the   time and then it can take 4-8 weeks to get that bug fix approved,   leaving users to think that iPhone apps sometimes just don't work.   Worse for Apple, these apps work just fine on other platforms   that have immediate approval processes.  Actually I suppose Apple has a third misconception: that all the complaints about App Store approvals are not a serious problem. They must hear developers complaining.  But partners and suppliers are always complaining.",
      "  It would be a bad sign if they weren't; it would mean you were being too easy on them.  Meanwhile the iPhone is selling better than ever.  So why do they need to fix anything?They get away with maltreating developers, in the short term, because they make such great hardware.  I just bought a new 27\" iMac a couple days ago.  It's fabulous.  The screen's too shiny, and the disk is surprisingly loud, but it's so beautiful that you can't make yourself care.So I bought it, but I bought it, for the first time, with misgivings. I felt the way I'd feel buying something made in a country with a bad human rights record.  That was new.  In the past when I bought things from Apple it was an unalloyed pleasure.  Oh boy!  They make such great stuff.  This time it felt like a Faustian bargain.  They make such great stuff, but they're such assholes.  Do I really want to support this company?* * *Should Apple care what people like me think?  What difference does it make if they alienate a small minority of their users?There are a couple reasons they should care.",
      "  One is that these users are the people they want as employees.  If your company seems evil, the best programmers won't work for you.  That hurt Microsoft a lot starting in the 90s.  Programmers started to feel sheepish about working there.  It seemed like selling out.  When people from Microsoft were talking to other programmers and they mentioned where they worked, there were a lot of self-deprecating jokes about having gone over to the dark side.  But the real problem for Microsoft wasn't the embarrassment of the people they hired.  It was the people they never got.  And you know who got them?  Google and Apple.  If Microsoft was the Empire, they were the Rebel Alliance. And it's largely because they got more of the best people that Google and Apple are doing so much better than Microsoft today.Why are programmers so fussy about their employers' morals?  Partly because they can afford to be.  The best programmers can work wherever they want.  They don't have to work for a company they have qualms about.But the other reason programmers are fussy, I think, is that evil begets stupidity.  An organization that wins by exercising power starts to lose the ability to win by doing better work.",
      "  And it's not fun for a smart person to work in a place where the best ideas aren't the ones that win.  I think the reason Google embraced \"Don't be evil\" so eagerly was not so much to impress the outside world as to inoculate themselves against arrogance. [1]That has worked for Google so far.  They've become more bureaucratic, but otherwise they seem to have held true to their original principles. With Apple that seems less the case.  When you look at the famous  1984 ad  now, it's easier to imagine Apple as the dictator on the screen than the woman with the hammer. [2] In fact, if you read the dictator's speech it sounds uncannily like a prophecy of the App Store.    We have triumphed over the unprincipled dissemination of facts.We have created, for the first time in all history, a garden of   pure ideology, where each worker may bloom secure from the pests   of contradictory and confusing truths.  The other reason Apple should care what programmers think of them is that when you sell a platform, developers make or break you.  If anyone should know this, Apple should.  VisiCalc made the Apple II.And programmers build applications for the platforms they use.",
      "  Most applications\u2014most startups, probably\u2014grow out of personal projects. Apple itself did.  Apple made microcomputers because that's what Steve Wozniak wanted for himself.  He couldn't have afforded a minicomputer.  [3]  Microsoft likewise started out making interpreters for little microcomputers because Bill Gates and Paul Allen were interested in using them.  It's a rare startup that doesn't build something the founders use.The main reason there are so many iPhone apps is that so many programmers have iPhones.  They may know, because they read it in an article, that Blackberry has such and such market share.  But in practice it's as if RIM didn't exist. If they're going to build something, they want to be able to use it themselves, and that means building an iPhone app.So programmers continue to develop iPhone apps, even though Apple continues to maltreat them.  They're like someone stuck in an abusive relationship.  They're so attracted to the iPhone that they can't leave.  But they're looking for a way out.  One wrote:    While I did enjoy developing for the iPhone, the control they   place on the App Store does not give me the drive to develop   applications as I would like.",
      " In fact I don't intend to make any   more iPhone applications unless absolutely necessary. [4]  Can anything break this cycle?  No device I've seen so far could. Palm and RIM haven't a hope.  The only credible contender is Android. But Android is an orphan; Google doesn't really care about it, not the way Apple cares about the iPhone.  Apple cares about the iPhone the way Google cares about search.* * *Is the future of handheld devices one locked down by Apple?  It's a worrying prospect.  It would be a bummer to have another grim monoculture like we had in the 1990s.  In 1995, writing software for end users was effectively identical with writing Windows applications.  Our horror at that prospect was the single biggest thing that drove us to start building web apps.At least we know now what it would take to break Apple's lock. You'd have to get iPhones out of programmers' hands.  If programmers used some other device for mobile web access, they'd start to develop apps for that instead.How could you make a device programmers liked better than the iPhone? It's unlikely you could make something better designed.  Apple leaves no room there.  So this alternative device probably couldn't win on general appeal.",
      "  It would have to win by virtue of some appeal it had to programmers specifically.One way to appeal to programmers is with software.  If you could think of an application programmers had to have, but that would be impossible in the circumscribed world of the iPhone,  you could presumably get them to switch.That would definitely happen if programmers started to use handhelds as development machines\u2014if handhelds displaced laptops the way laptops displaced desktops.  You need more control of a development machine than Apple will let you have over an iPhone.Could anyone make a device that you'd carry around in your pocket like a phone, and yet would also work as a development machine? It's hard to imagine what it would look like.  But I've learned never to say never about technology.  A phone-sized device that would work as a development machine is no more miraculous by present standards than the iPhone itself would have seemed by the standards of 1995.My current development machine is a MacBook Air, which I use with an external monitor and keyboard in my office, and by itself when traveling.  If there was a version half the size I'd prefer it. That still wouldn't be small enough to carry around everywhere like a phone, but we're within a factor of 4 or so.",
      "  Surely that gap is bridgeable.  In fact, let's make it an RFS. Wanted:  Woman with hammer.Notes[1] When Google adopted \"Don't be evil,\" they were still so small that no one would have expected them to be, yet. [2] The dictator in the 1984 ad isn't Microsoft, incidentally; it's IBM.  IBM seemed a lot more frightening in those days, but they were friendlier to developers than Apple is now.[3] He couldn't even afford a monitor.  That's why the Apple I used a TV as a monitor.[4] Several people I talked to mentioned how much they liked the iPhone SDK.  The problem is not Apple's products but their policies. Fortunately policies are software; Apple can change them instantly if they want to.  Handy that, isn't it?Thanks to Sam Altman, Trevor Blackwell, Ross Boucher,  James Bracy, Gabor Cselle, Patrick Collison, Jason Freedman, John Gruber, Joe Hewitt, Jessica Livingston, Robert Morris, Teng Siong Ong, Nikhil Pandit, Savraj Singh, and Jared Tame for reading drafts of this.February 2020What should an essay be?",
      " Many people would say persuasive. That's what a lot of us were taught essays should be. But I think we can aim for something more ambitious: that an essay should be useful.To start with, that means it should be correct. But it's not enough merely to be correct. It's easy to make a statement correct by making it vague. That's a common flaw in academic writing, for example. If you know nothing at all about an issue, you can't go wrong by saying that the issue is a complex one, that there are many factors to be considered, that it's a mistake to take too simplistic a view of it, and so on.Though no doubt correct, such statements tell the reader nothing. Useful writing makes claims that are as strong as they can be made without becoming false.For example, it's more useful to say that Pike's Peak is near the middle of Colorado than merely somewhere in Colorado. But if I say it's in the exact middle of Colorado, I've now gone too far, because it's a bit east of the middle.Precision and correctness are like opposing forces. It's easy to satisfy one if you ignore the other. The converse of vaporous academic writing is the bold, but false, rhetoric of demagogues.",
      " Useful writing is bold, but true.It's also two other things: it tells people something important, and that at least some of them didn't already know.Telling people something they didn't know doesn't always mean surprising them. Sometimes it means telling them something they knew unconsciously but had never put into words. In fact those may be the more valuable insights, because they tend to be more fundamental.Let's put them all together. Useful writing tells people something true and important that they didn't already know, and tells them as unequivocally as possible.Notice these are all a matter of degree. For example, you can't expect an idea to be novel to everyone. Any insight that you have will probably have already been had by at least one of the world's 7 billion people. But it's sufficient if an idea is novel to a lot of readers.Ditto for correctness, importance, and strength. In effect the four components are like numbers you can multiply together to get a score for usefulness. Which I realize is almost awkwardly reductive, but nonetheless true._____ How can you ensure that the things you say are true and novel and important? Believe it or not, there is a trick for doing this. I learned it from my friend Robert Morris,",
      " who has a horror of saying anything dumb. His trick is not to say anything unless he's sure it's worth hearing. This makes it hard to get opinions out of him, but when you do, they're usually right.Translated into essay writing, what this means is that if you write a bad sentence, you don't publish it. You delete it and try again. Often you abandon whole branches of four or five paragraphs. Sometimes a whole essay.You can't ensure that every idea you have is good, but you can ensure that every one you publish is, by simply not publishing the ones that aren't.In the sciences, this is called publication bias, and is considered bad. When some hypothesis you're exploring gets inconclusive results, you're supposed to tell people about that too. But with essay writing, publication bias is the way to go.My strategy is loose, then tight. I write the first draft of an essay fast, trying out all kinds of ideas. Then I spend days rewriting it very carefully.I've never tried to count how many times I proofread essays, but I'm sure there are sentences I've read 100 times before publishing them. When I proofread an essay, there are usually passages that stick out in an annoying way, sometimes because they're clumsily written,",
      " and sometimes because I'm not sure they're true. The annoyance starts out unconscious, but after the tenth reading or so I'm saying \"Ugh, that part\" each time I hit it. They become like briars that catch your sleeve as you walk past. Usually I won't publish an essay till they're all gone \u0097 till I can read through the whole thing without the feeling of anything catching.I'll sometimes let through a sentence that seems clumsy, if I can't think of a way to rephrase it, but I will never knowingly let through one that doesn't seem correct. You never have to. If a sentence doesn't seem right, all you have to do is ask why it doesn't, and you've usually got the replacement right there in your head.This is where essayists have an advantage over journalists. You don't have a deadline. You can work for as long on an essay as you need to get it right. You don't have to publish the essay at all, if you can't get it right. Mistakes seem to lose courage in the face of an enemy with unlimited resources. Or that's what it feels like. What's really going on is that you have different expectations for yourself. You're like a parent saying to a child \"we can sit here all night till you eat your vegetables.\" Except you're the child too.I'm not saying no mistake gets through.",
      " For example, I added condition (c) in \"A Way to Detect Bias\"  after readers pointed out that I'd omitted it. But in practice you can catch nearly all of them.There's a trick for getting importance too. It's like the trick I suggest to young founders for getting startup ideas: to make something you yourself want. You can use yourself as a proxy for the reader. The reader is not completely unlike you, so if you write about topics that seem important to you, they'll probably seem important to a significant number of readers as well.Importance has two factors. It's the number of people something matters to, times how much it matters to them. Which means of course that it's not a rectangle, but a sort of ragged comb, like a Riemann sum.The way to get novelty is to write about topics you've thought about a lot. Then you can use yourself as a proxy for the reader in this department too. Anything you notice that surprises you, who've thought about the topic a lot, will probably also surprise a significant number of readers. And here, as with correctness and importance, you can use the Morris technique to ensure that you will. If you don't learn anything from writing an essay, don't publish it.You need humility to measure novelty,",
      " because acknowledging the novelty of an idea means acknowledging your previous ignorance of it. Confidence and humility are often seen as opposites, but in this case, as in many others, confidence helps you to be humble. If you know you're an expert on some topic, you can freely admit when you learn something you didn't know, because you can be confident that most other people wouldn't know it either.The fourth component of useful writing, strength, comes from two things: thinking well, and the skillful use of qualification. These two counterbalance each other, like the accelerator and clutch in a car with a manual transmission. As you try to refine the expression of an idea, you adjust the qualification accordingly. Something you're sure of, you can state baldly with no qualification at all, as I did the four components of useful writing. Whereas points that seem dubious have to be held at arm's length with perhapses.As you refine an idea, you're pushing in the direction of less qualification. But you can rarely get it down to zero. Sometimes you don't even want to, if it's a side point and a fully refined version would be too long.Some say that qualifications weaken writing. For example, that you should never begin a sentence in an essay with \"I think,\" because if you're saying it,",
      " then of course you think it. And it's true that \"I think x\" is a weaker statement than simply \"x.\" Which is exactly why you need \"I think.\" You need it to express your degree of certainty.But qualifications are not scalars. They're not just experimental error. There must be 50 things they can express: how broadly something applies, how you know it, how happy you are it's so, even how it could be falsified. I'm not going to try to explore the structure of qualification here. It's probably more complex than the whole topic of writing usefully. Instead I'll just give you a practical tip: Don't underestimate qualification. It's an important skill in its own right, not just a sort of tax you have to pay in order to avoid saying things that are false. So learn and use its full range. It may not be fully half of having good ideas, but it's part of having them.There's one other quality I aim for in essays: to say things as simply as possible. But I don't think this is a component of usefulness. It's more a matter of consideration for the reader. And it's a practical aid in getting things right; a mistake is more obvious when expressed in simple language.",
      " But I'll admit that the main reason I write simply is not for the reader's sake or because it helps get things right, but because it bothers me to use more or fancier words than I need to. It seems inelegant, like a program that's too long.I realize florid writing works for some people. But unless you're sure you're one of them, the best advice is to write as simply as you can._____ I believe the formula I've given you, importance + novelty + correctness + strength, is the recipe for a good essay. But I should warn you that it's also a recipe for making people mad.The root of the problem is novelty. When you tell people something they didn't know, they don't always thank you for it. Sometimes the reason people don't know something is because they don't want to know it. Usually because it contradicts some cherished belief. And indeed, if you're looking for novel ideas, popular but mistaken beliefs are a good place to find them. Every popular mistaken belief creates a dead zone of ideas around  it that are relatively unexplored because they contradict it.The strength component just makes things worse. If there's anything that annoys people more than having their cherished assumptions contradicted, it's having them flatly contradicted.Plus if you've used the Morris technique,",
      " your writing will seem quite confident. Perhaps offensively confident, to people who disagree with you. The reason you'll seem confident is that you are confident: you've cheated, by only publishing the things you're sure of.  It will seem to people who try to disagree with you that you never admit you're wrong. In fact you constantly admit you're wrong. You just do it before publishing instead of after.And if your writing is as simple as possible, that just makes things worse. Brevity is the diction of command. If you watch someone delivering unwelcome news from a position of inferiority, you'll notice they tend to use lots of words, to soften the blow. Whereas to be short with someone is more or less to be rude to them.It can sometimes work to deliberately phrase statements more weakly than you mean. To put \"perhaps\" in front of something you're actually quite sure of. But you'll notice that when writers do this, they usually do it with a wink.I don't like to do this too much. It's cheesy to adopt an ironic tone for a whole essay. I think we just have to face the fact that elegance and curtness are two names for the same thing.You might think that if you work sufficiently hard to ensure that an essay is correct,",
      " it will be invulnerable to attack. That's sort of true. It will be invulnerable to valid attacks. But in practice that's little consolation.In fact, the strength component of useful writing will make you particularly vulnerable to misrepresentation. If you've stated an idea as strongly as you could without making it false, all anyone has to do is to exaggerate slightly what you said, and now it is false.Much of the time they're not even doing it deliberately. One of the most surprising things you'll discover, if you start writing essays, is that people who disagree with you rarely disagree with what you've actually written. Instead they make up something you said and disagree with that.For what it's worth, the countermove is to ask someone who does this to quote a specific sentence or passage you wrote that they believe is false, and explain why. I say \"for what it's worth\" because they never do. So although it might seem that this could get a broken discussion back on track, the truth is that it was never on track in the first place.Should you explicitly forestall likely misinterpretations? Yes, if they're misinterpretations a reasonably smart and well-intentioned person might make. In fact it's sometimes better to say something slightly misleading and then add the correction than to try to get an idea right in one shot.",
      " That can be more efficient, and can also model the way such an idea would be discovered.But I don't think you should explicitly forestall intentional misinterpretations in the body of an essay. An essay is a place to meet honest readers. You don't want to spoil your house by putting bars on the windows to protect against dishonest ones. The place to protect against intentional misinterpretations is in end-notes. But don't think you can predict them all. People are as ingenious at misrepresenting you when you say something they don't want to hear as they are at coming up with rationalizations for things they want to do but know they shouldn't. I suspect it's the same skill._____ As with most other things, the way to get better at writing essays is to practice. But how do you start? Now that we've examined the structure of useful writing, we can rephrase that question more precisely. Which constraint do you relax initially? The answer is, the first component of importance: the number of people who care about what you write.If you narrow the topic sufficiently, you can probably find something you're an expert on. Write about that to start with. If you only have ten readers who care, that's fine. You're helping them, and you're writing.",
      " Later you can expand the breadth of topics you write about.The other constraint you can relax is a little surprising: publication. Writing essays doesn't have to mean publishing them. That may seem strange now that the trend is to publish every random thought, but it worked for me. I wrote what amounted to essays in notebooks for about 15 years. I never published any of them and never expected to. I wrote them as a way of figuring things out. But when the web came along I'd had a lot of practice.Incidentally,  Steve  Wozniak did the same thing. In high school he designed computers on paper for fun. He couldn't build them because he couldn't afford the components. But when Intel launched 4K DRAMs in 1975, he was ready._____ How many essays are there left to write though? The answer to that question is probably the most exciting thing I've learned about essay writing. Nearly all of them are left to write.Although the essay  is an old form, it hasn't been assiduously cultivated. In the print era, publication was expensive, and there wasn't enough demand for essays to publish that many. You could publish essays if you were already well known for writing something else, like novels.",
      " Or you could write book reviews that you took over to express your own ideas. But there was not really a direct path to becoming an essayist. Which meant few essays got written, and those that did tended to be about a narrow range of subjects.Now, thanks to the internet, there's a path. Anyone can publish essays online. You start in obscurity, perhaps, but at least you can start. You don't need anyone's permission.It sometimes happens that an area of knowledge sits quietly for years, till some change makes it explode. Cryptography did this to number theory. The internet is doing it to the essay.The exciting thing is not that there's a lot left to write, but that there's a lot left to discover. There's a certain kind of idea that's best discovered by writing essays. If most essays are still unwritten, most such ideas are still undiscovered.Notes[1] Put railings on the balconies, but don't put bars on the windows.[2] Even now I sometimes write essays that are not meant for publication. I wrote several to figure out what Y Combinator should do, and they were really helpful.Thanks to Trevor Blackwell, Daniel Gackle, Jessica Livingston, and Robert Morris for reading drafts of this.April 2012A palliative care nurse called Bronnie Ware made a list of the biggest regrets of the dying.",
      "  Her list seems plausible.  I could see myself \u2014 can see myself \u2014 making at least 4 of these 5 mistakes.If you had to compress them into a single piece of advice, it might be: don't be a cog.  The 5 regrets paint a portrait of post-industrial man, who shrinks himself into a shape that fits his circumstances, then turns dutifully till he stops.The alarming thing is, the mistakes that produce these regrets are all errors of omission.  You forget your dreams, ignore your family, suppress your feelings, neglect your friends, and forget to be happy.  Errors of omission are a particularly dangerous type of mistake, because you make them by default.I would like to avoid making these mistakes.  But how do you avoid mistakes you make by default?  Ideally you transform your life so it has other defaults.  But it may not be possible to do that completely. As long as these mistakes happen by default, you probably have to be reminded not to make them.  So I inverted the 5 regrets, yielding a list of 5 commands     Don't ignore your dreams; don't work too much; say what you    think; cultivate friendships; be happy.  which I then put at the top of the file I use as a todo list.",
      "  Want to start a startup?  Get funded by Y Combinator.     October 2014(This essay is derived from a guest lecture in Sam Altman's startup class at Stanford.  It's intended for college students, but much of it is applicable to potential founders at other ages.)One of the advantages of having kids is that when you have to give advice, you can ask yourself \"what would I tell my own kids?\"  My kids are little, but I can imagine what I'd tell them about startups if they were in college, and that's what I'm going to tell you.Startups are very counterintuitive.  I'm not sure why.  Maybe it's just because knowledge about them hasn't permeated our culture yet. But whatever the reason, starting a startup is a task where you can't always trust your instincts.It's like skiing in that way.  When you first try skiing and you want to slow down, your instinct is to lean back.  But if you lean back on skis you fly down the hill out of control.  So part of learning to ski is learning to suppress that impulse.  Eventually you get new habits, but at first it takes a conscious effort.  At first there's a list of things you're trying to remember as you start down the hill.Startups are as unnatural as skiing,",
      " so there's a similar list for startups. Here I'm going to give you the first part of it \u2014 the things to remember if you want to prepare yourself to start a startup. CounterintuitiveThe first item on it is the fact I already mentioned: that startups are so weird that if you trust your instincts, you'll make a lot of mistakes.  If you know nothing more than this, you may at least pause before making them.When I was running Y Combinator I used to joke that our function was to tell founders things they would ignore.  It's really true. Batch after batch, the YC partners warn founders about mistakes they're about to make, and the founders ignore them, and then come back a year later and say \"I wish we'd listened.\"Why do the founders ignore the partners' advice?  Well, that's the thing about counterintuitive ideas: they contradict your intuitions. They seem wrong.  So of course your first impulse is to disregard them.  And in fact my joking description is not merely the curse of Y Combinator but part of its raison d'etre. If founders' instincts already gave them the right answers, they wouldn't need us.  You only need other people to give you advice that surprises you.",
      " That's why there are a lot of ski instructors and not many running instructors. [1]You can, however, trust your instincts about people.  And in fact one of the most common mistakes young founders make is not to do that enough.  They get involved with people who seem impressive, but about whom they feel some misgivings personally.  Later when things blow up they say \"I knew there was something off about him, but I ignored it because he seemed so impressive.\"If you're thinking about getting involved with someone \u2014 as a cofounder, an employee, an investor, or an acquirer \u2014 and you have misgivings about them, trust your gut.  If someone seems slippery, or bogus, or a jerk, don't ignore it.This is one case where it pays to be self-indulgent. Work with people you genuinely like, and you've known long enough to be sure. ExpertiseThe second counterintuitive point is that it's not that important to know a lot about startups.  The way to succeed in a startup is not to be an expert on startups, but to be an expert on your users and the problem you're solving for them. Mark Zuckerberg didn't succeed because he was an expert on startups.",
      " He succeeded despite being a complete noob at startups, because he understood his users really well.If you don't know anything about, say, how to raise an angel round, don't feel bad on that account.  That sort of thing you can learn when you need to, and forget after you've done it.In fact, I worry it's not merely unnecessary to learn in great detail about the mechanics of startups, but possibly somewhat dangerous.  If I met an undergrad who knew all about convertible notes and employee agreements and (God forbid) class FF stock, I wouldn't think \"here is someone who is way ahead of their peers.\" It would set off alarms.  Because another of the characteristic mistakes of young founders is to go through the motions of starting a startup.  They make up some plausible-sounding idea, raise money at a good valuation, rent a cool office, hire a bunch of people. From the outside that seems like what startups do.  But the next step after rent a cool office and hire a bunch of people is: gradually realize how completely fucked they are, because while imitating all the outward forms of a startup they have neglected the one thing that's actually essential: making something people want. GameWe saw this happen so often that we made up a name for it:",
      " playing house.  Eventually I realized why it was happening.  The reason young founders go through the motions of starting a startup is because that's what they've been trained to do for their whole lives up to that point.  Think about what you have to do to get into college, for example.  Extracurricular activities, check.  Even in college classes most of the work is as artificial as running laps.I'm not attacking the educational system for being this way. There will always be a certain amount of fakeness in the work you do when you're being taught something, and if you measure their performance it's inevitable that people will exploit the difference to the point where much of what you're measuring is artifacts of the fakeness.I confess I did it myself in college. I found that in a lot of classes there might only be 20 or 30 ideas that were the right shape to make good exam questions.  The way I studied for exams in these classes was not (except incidentally) to master the material taught in the class, but to make a list of potential exam questions and work out the answers in advance. When I walked into the final, the main thing I'd be feeling was curiosity about which of my questions would turn up on the exam.",
      "  It was like a game.It's not surprising that after being trained for their whole lives to play such games, young founders' first impulse on starting a startup is to try to figure out the tricks for winning at this new game. Since fundraising appears to be the measure of success for startups (another classic noob mistake), they always want to know what the tricks are for convincing investors.  We tell them the best way to convince investors is to make a startup that's actually doing well, meaning growing fast, and then simply tell investors so.  Then they want to know what the tricks are for growing fast.  And we have to tell them the best way to do that is simply to make something people want.So many of the conversations YC partners have with young founders begin with the founder asking \"How do we...\" and the partner replying \"Just...\"Why do the founders always make things so complicated?  The reason, I realized, is that they're looking for the trick.So this is the third counterintuitive thing to remember about startups: starting a startup is where gaming the system stops working.  Gaming the system may continue to work if you go to work for a big company. Depending on how broken the company is, you can succeed by sucking up to the right people,",
      " giving the impression of productivity, and so on.  [2] But that doesn't work with startups. There is no boss to trick, only users, and all users care about is whether your product does what they want. Startups are as impersonal as physics.  You have to make something people want, and you prosper only to the extent you do.The dangerous thing is, faking does work to some degree on investors. If you're super good at sounding like you know what you're talking about, you can fool investors for at least one and perhaps even two rounds of funding.  But it's not in your interest to.  The company is ultimately doomed.  All you're doing is wasting your own time riding it down.So stop looking for the trick. There are tricks in startups, as there are in any domain, but they are an order of magnitude less important than solving the real problem. A founder who knows nothing about fundraising but has made something users love will have an easier time raising money than one who knows every trick in the book but has a flat usage graph. And more importantly, the founder who has made something users love is the one who will go on to succeed after raising the money.Though in a sense it's bad news in that you're deprived of one of your most powerful weapons,",
      " I think it's exciting that gaming the system stops working when you start a startup.  It's exciting that there even exist parts of the world where you win by doing good work.  Imagine how depressing the world would be if it were all like school and big companies, where you either have to spend a lot of time on bullshit things or lose to people who do. [3] I would have been delighted if I'd realized in college that there were parts of the real world where gaming the system mattered less than others, and a few where it hardly mattered at all.  But there are, and this variation is one of the most important things to consider when you're thinking about your future.  How do you win in each type of work, and what would you like to win by doing? [4] All-ConsumingThat brings us to our fourth counterintuitive point: startups are all-consuming.  If you start a startup, it will take over your life to a degree you cannot imagine.  And if your startup succeeds, it will take over your life for a long time: for several years at the very least, maybe for a decade, maybe for the rest of your working life.  So there is a real opportunity cost here.Larry Page may seem to have an enviable life,",
      " but there are aspects of it that are unenviable.  Basically at 25 he started running as fast as he could and it must seem to him that he hasn't stopped to catch his breath since.  Every day new shit happens in the Google empire that only the CEO can deal with, and he, as CEO, has to deal with it.  If he goes on vacation for even a week, a whole week's backlog of shit accumulates.  And he has to bear this uncomplainingly, partly because as the company's daddy he can never show fear or weakness, and partly because billionaires get less than zero sympathy if they talk about having difficult lives.  Which has the strange side effect that the difficulty of being a successful startup founder is concealed from almost everyone except those who've done it.Y Combinator has now funded several companies that can be called big successes, and in every single case the founders say the same thing.  It never gets any easier.  The nature of the problems change. You're worrying about construction delays at your London office instead of the broken air conditioner in your studio apartment. But the total volume of worry never decreases; if anything it increases.Starting a successful startup is similar to having kids in that it's like a button you push that changes your life irrevocably.",
      " And while it's truly wonderful having kids, there are a lot of things that are easier to do before you have them than after.  Many of which will make you a better parent when you do have kids. And since you can delay pushing the button for a while, most people in rich countries do.Yet when it comes to startups, a lot of people seem to think they're supposed to start them while they're still in college.  Are you crazy?  And what are the universities thinking?  They go out of their way to ensure their students are well supplied with contraceptives, and yet they're setting up entrepreneurship programs and startup incubators left and right.To be fair, the universities have their hand forced here.  A lot of incoming students are interested in startups.  Universities are, at least de facto, expected to prepare them for their careers.  So students who want to start startups hope universities can teach them about startups.  And whether universities can do this or not, there's some pressure to claim they can, lest they lose applicants to other universities that do.Can universities teach students about startups?  Yes and no.  They can teach students about startups, but as I explained before, this is not what you need to know.  What you need to learn about are the needs of your own users,",
      " and you can't do that until you actually start the company. [5] So starting a startup is intrinsically something you can only really learn by doing it.  And it's impossible to do that in college, for the reason I just explained: startups take over your life.  You can't start a startup for real as a student, because if you start a startup for real you're not a student anymore. You may be nominally a student for a bit, but you won't even be that for long. [6]Given this dichotomy, which of the two paths should you take?  Be a real student and not start a startup, or start a real startup and not be a student?  I can answer that one for you. Do not start a startup in college.  How to start a startup is just a subset of a bigger problem you're trying to solve: how to have a good life. And though starting a startup can be part of a good life for a lot of ambitious people, age 20 is not the optimal time to do it. Starting a startup is like a brutally fast depth-first search.  Most people should still be searching breadth-first at 20.You can do things in your early 20s that you can't do as well before or after,",
      " like plunge deeply into projects on a whim and travel super cheaply with no sense of a deadline.  For unambitious people, this sort of thing is the dreaded \"failure to launch,\" but for the ambitious ones it can be an incomparably valuable sort of exploration. If you start a startup at 20 and you're sufficiently successful, you'll never get to do it. [7]Mark Zuckerberg will never get to bum around a foreign country.  He can do other things most people can't, like charter jets to fly him to foreign countries. But success has taken a lot of the serendipity out of his life. Facebook is running him as much as he's running Facebook. And while it can be very cool to be in the grip of a project you consider your life's work, there are advantages to serendipity too, especially early in life.  Among other things it gives you more options to choose your life's work from.There's not even a tradeoff here. You're not sacrificing anything if you forgo starting a startup at 20, because you're more likely to succeed if you wait.  In the unlikely case that you're 20 and one of your side projects takes off like Facebook did, you'll face a choice of running with it or not,",
      " and it may be reasonable to run with it.  But the usual way startups take off is for the founders to make them take off, and it's gratuitously stupid to do that at 20. TryShould you do it at any age?  I realize I've made startups sound pretty hard.  If I haven't, let me try again: starting a startup is really hard.  What if it's too hard?  How can you tell if you're up to this challenge?The answer is the fifth counterintuitive point: you can't tell. Your life so far may have given you some idea what your prospects might be if you tried to become a mathematician, or a professional football player.  But unless you've had a very strange life you haven't done much that was like being a startup founder. Starting a startup will change you a lot.  So what you're trying to estimate is not just what you are, but what you could grow into, and who can do that?For the past 9 years it was my job to predict whether people would have what it took to start successful startups.  It was easy to tell how smart they were, and most people reading this will be over that threshold.  The hard part was predicting how tough and ambitious they would become.",
      "  There may be no one who has more experience at trying to predict that, so I can tell you how much an expert can know about it, and the answer is: not much.  I learned to keep a completely open mind about which of the startups in each batch would turn out to be the stars.The founders sometimes think they know. Some arrive feeling sure they will ace Y Combinator just as they've aced every one of the (few, artificial, easy) tests they've faced in life so far.  Others arrive wondering how they got in, and hoping YC doesn't discover whatever mistake caused it to accept them.  But there is little correlation between founders' initial attitudes and how well their companies do.I've read that the same is true in the military \u2014 that the swaggering recruits are no more likely to turn out to be really tough than the quiet ones. And probably for the same reason: that the tests involved are so different from the ones in their previous lives.If you're absolutely terrified of starting a startup, you probably shouldn't do it.  But if you're merely unsure whether you're up to it, the only way to find out is to try.  Just not now. IdeasSo if you want to start a startup one day,",
      " what should you do in college?  There are only two things you need initially: an idea and cofounders.  And the m.o. for getting both is the same.  Which leads to our sixth and last counterintuitive point: that the way to get startup ideas is not to try to think of startup ideas.I've written a whole essay on this, so I won't repeat it all here.  But the short version is that if you make a conscious effort to think of startup ideas, the ideas you come up with will not merely be bad, but bad and plausible-sounding, meaning you'll waste a lot of time on them before realizing they're bad.The way to come up with good startup ideas is to take a step back. Instead of making a conscious effort to think of startup ideas, turn your mind into the type that startup ideas form in without any conscious effort.  In fact, so unconsciously that you don't even realize at first that they're startup ideas.This is not only possible, it's how Apple, Yahoo, Google, and Facebook all got started.  None of these companies were even meant to be companies at first.  They were all just side projects.  The best startups almost have to start as side projects, because great ideas tend to be such outliers that your conscious mind would reject them as ideas for companies.Ok,",
      " so how do you turn your mind into the type that startup ideas form in unconsciously?  (1) Learn a lot about things that matter, then (2) work on problems that interest you (3) with people you like and respect.  The third part, incidentally, is how you get cofounders at the same time as the idea.The first time I wrote that paragraph, instead of \"learn a lot about things that matter,\" I wrote \"become good at some technology.\" But that prescription, though sufficient, is too narrow.  What was special about Brian Chesky and Joe Gebbia was not that they were experts in technology.  They were good at design, and perhaps even more importantly, they were good at organizing groups and making projects happen.  So you don't have to work on technology per se, so long as you work on problems demanding enough to stretch you.What kind of problems are those?  That is very hard to answer in the general case.  History is full of examples of young people who were working on important problems that no one else at the time thought were important, and in particular that their parents didn't think were important.  On the other hand, history is even fuller of examples of parents who thought their kids were wasting their time and who were right.",
      "  So how do you know when you're working on real stuff? [8]I know how I know.  Real problems are interesting, and I am self-indulgent in the sense that I always want to work on interesting things, even if no one else cares about them (in fact, especially if no one else cares about them), and find it very hard to make myself work on boring things, even if they're supposed to be important.My life is full of case after case where I worked on something just because it seemed interesting, and it turned out later to be useful in some worldly way.  Y Combinator itself was something I only did because it seemed interesting. So I seem to have some sort of internal compass that helps me out.  But I don't know what other people have in their heads. Maybe if I think more about this I can come up with heuristics for recognizing genuinely interesting problems, but for the moment the best I can offer is the hopelessly question-begging advice that if you have a taste for genuinely interesting problems, indulging it energetically is the best way to prepare yourself for a startup. And indeed, probably also the best way to live. [9]But although I can't explain in the general case what counts as an interesting problem,",
      " I can tell you about a large subset of them. If you think of technology as something that's spreading like a sort of fractal stain, every moving point on the edge represents an interesting problem.  So one guaranteed way to turn your mind into the type that has good startup ideas is to get yourself to the leading edge of some technology \u2014 to cause yourself, as Paul Buchheit put it, to \"live in the future.\" When you reach that point, ideas that will seem to other people uncannily prescient will seem obvious to you.  You may not realize they're startup ideas, but you'll know they're something that ought to exist.For example, back at Harvard in the mid 90s a fellow grad student of my friends Robert and Trevor wrote his own voice over IP software. He didn't mean it to be a startup, and he never tried to turn it into one.  He just wanted to talk to his girlfriend in Taiwan without paying for long distance calls, and since he was an expert on networks it seemed obvious to him that the way to do it was turn the sound into packets and ship it over the Internet. He never did any more with his software than talk to his girlfriend, but this is exactly the way the best startups get started.So strangely enough the optimal thing to do in college if you want to be a successful startup founder is not some sort of new,",
      " vocational version of college focused on \"entrepreneurship.\" It's the classic version of college as education for its own sake. If you want to start a startup after college, what you should do in college is learn powerful things.  And if you have genuine intellectual curiosity, that's what you'll naturally tend to do if you just follow your own inclinations. [10]The component of entrepreneurship that really matters is domain expertise.  The way to become Larry Page was to become an expert on search. And the way to become an expert on search was to be driven by genuine curiosity, not some ulterior motive.At its best, starting a startup is merely an ulterior motive for curiosity.  And you'll do it best if you introduce the ulterior motive toward the end of the process.So here is the ultimate advice for young would-be startup founders, boiled down to two words: just learn. Notes[1] Some founders listen more than others, and this tends to be a predictor of success. One of the things I remember about the Airbnbs during YC is how intently they listened.[2] In fact, this is one of the reasons startups are possible.  If big companies weren't plagued by internal inefficiencies, they'd be proportionately more effective,",
      " leaving less room for startups.[3] In a startup you have to spend a lot of time on schleps, but this sort of work is merely unglamorous, not bogus.[4] What should you do if your true calling is gaming the system? Management consulting.[5] The company may not be incorporated, but if you start to get significant numbers of users, you've started it, whether you realize it yet or not.[6] It shouldn't be that surprising that colleges can't teach students how to be good startup founders, because they can't teach them how to be good employees either.The way universities \"teach\" students how to be employees is to hand off the task to companies via internship programs.  But you couldn't do the equivalent thing for startups, because by definition if the students did well they would never come back.[7] Charles Darwin was 22 when he received an invitation to travel aboard the HMS Beagle as a naturalist.  It was only because he was otherwise unoccupied, to a degree that alarmed his family, that he could accept it. And yet if he hadn't we probably would not know his name.[8] Parents can sometimes be especially conservative in this department.  There are some whose definition of important problems includes only those on the critical path to med school.[9]",
      " I did manage to think of a heuristic for detecting whether you have a taste for interesting ideas: whether you find known boring ideas intolerable.  Could you endure studying literary theory, or working in middle management at a large company?[10] In fact, if your goal is to start a startup, you can stick even more closely to the ideal of a liberal education than past generations have. Back when students focused mainly on getting a job after college, they thought at least a little about how the courses they took might look to an employer.  And perhaps even worse, they might shy away from taking a difficult class lest they get a low grade, which would harm their all-important GPA.  Good news: users don't care what your GPA was.  And I've never heard of investors caring either.  Y Combinator certainly never asks what classes you took in college or what grades you got in them. Thanks to Sam Altman, Paul Buchheit, John Collison, Patrick Collison, Jessica Livingston, Robert Morris, Geoff Ralston, and Fred Wilson for reading drafts of this.May 2021There's one kind of opinion I'd be very afraid to express publicly. If someone I knew to be both a domain expert and a reasonable person proposed an idea that sounded preposterous,",
      " I'd be very reluctant to say \"That will never work.\"Anyone who has studied the history of ideas, and especially the history of science, knows that's how big things start. Someone proposes an idea that sounds crazy, most people dismiss it, then it gradually takes over the world.Most implausible-sounding ideas are in fact bad and could be safely dismissed. But not when they're proposed by reasonable domain experts. If the person proposing the idea is reasonable, then they know how implausible it sounds. And yet they're proposing it anyway. That suggests they know something you don't. And if they have deep domain expertise, that's probably the source of it. [1]Such ideas are not merely unsafe to dismiss, but disproportionately likely to be interesting. When the average person proposes an implausible-sounding idea, its implausibility is evidence of their incompetence. But when a reasonable domain expert does it, the situation is reversed. There's something like an efficient market here: on average the ideas that seem craziest will, if correct, have the biggest effect. So if you can eliminate the theory that the person proposing an implausible-sounding idea is incompetent, its implausibility switches from evidence that it's boring to evidence that it's exciting.",
      " [2]Such ideas are not guaranteed to work. But they don't have to be. They just have to be sufficiently good bets \u2014 to have sufficiently high expected value. And I think on average they do. I think if you bet on the entire set of implausible-sounding ideas proposed by reasonable domain experts, you'd end up net ahead.The reason is that everyone is too conservative. The word \"paradigm\" is overused, but this is a case where it's warranted. Everyone is too much in the grip of the current paradigm. Even the people who have the new ideas undervalue them initially. Which means that before they reach the stage of proposing them publicly, they've already subjected them to an excessively strict filter. [3]The wise response to such an idea is not to make statements, but to ask questions, because there's a real mystery here. Why has this smart and reasonable person proposed an idea that seems so wrong? Are they mistaken, or are you? One of you has to be. If you're the one who's mistaken, that would be good to know, because it means there's a hole in your model of the world. But even if they're mistaken, it should be interesting to learn why. A trap that an expert falls into is one you have to worry about too.This all seems pretty obvious.",
      " And yet there are clearly a lot of people who don't share my fear of dismissing new ideas. Why do they do it? Why risk looking like a jerk now and a fool later, instead of just reserving judgement?One reason they do it is envy. If you propose a radical new idea and it succeeds, your reputation (and perhaps also your wealth) will increase proportionally. Some people would be envious if that happened, and this potential envy propagates back into a conviction that you must be wrong.Another reason people dismiss new ideas is that it's an easy way to seem sophisticated. When a new idea first emerges, it usually seems pretty feeble. It's a mere hatchling. Received wisdom is a full-grown eagle by comparison. So it's easy to launch a devastating attack on a new idea, and anyone who does will seem clever to those who don't understand this asymmetry.This phenomenon is exacerbated by the difference between how those working on new ideas and those attacking them are rewarded. The rewards for working on new ideas are weighted by the value of the outcome. So it's worth working on something that only has a 10% chance of succeeding if it would make things more than 10x better. Whereas the rewards for attacking new ideas are roughly constant;",
      " such attacks seem roughly equally clever regardless of the target.People will also attack new ideas when they have a vested interest in the old ones. It's not surprising, for example, that some of Darwin's harshest critics were churchmen. People build whole careers on some ideas. When someone claims they're false or obsolete, they feel threatened.The lowest form of dismissal is mere factionalism: to automatically dismiss any idea associated with the opposing faction. The lowest form of all is to dismiss an idea because of who proposed it.But the main thing that leads reasonable people to dismiss new ideas is the same thing that holds people back from proposing them: the sheer pervasiveness of the current paradigm. It doesn't just affect the way we think; it is the Lego blocks we build thoughts out of. Popping out of the current paradigm is something only a few people can do. And even they usually have to suppress their intuitions at first, like a pilot flying through cloud who has to trust his instruments over his sense of balance. [4]Paradigms don't just define our present thinking. They also vacuum up the trail of crumbs that led to them, making our standards for new ideas impossibly high. The current paradigm seems so perfect to us, its offspring,",
      " that we imagine it must have been accepted completely as soon as it was discovered \u2014 that whatever the church thought of the heliocentric model, astronomers must have been convinced as soon as Copernicus proposed it. Far, in fact, from it. Copernicus published the heliocentric model in 1532, but it wasn't till the mid seventeenth century that the balance of scientific opinion shifted in its favor. [5]Few understand how feeble new ideas look when they first appear. So if you want to have new ideas yourself, one of the most valuable things you can do is to learn what they look like when they're born. Read about how new ideas happened, and try to get yourself into the heads of people at the time. How did things look to them, when the new idea was only half-finished, and even the person who had it was only half-convinced it was right?But you don't have to stop at history. You can observe big new ideas being born all around you right now. Just look for a reasonable domain expert proposing something that sounds wrong.If you're nice, as well as wise, you won't merely resist attacking such people, but encourage them. Having new ideas is a lonely business. Only those who've tried it know how lonely.",
      " These people need your help. And if you help them, you'll probably learn something in the process.Notes[1] This domain expertise could be in another field. Indeed, such crossovers tend to be particularly promising.[2] I'm not claiming this principle extends much beyond math, engineering, and the hard sciences. In politics, for example, crazy-sounding ideas generally are as bad as they sound. Though arguably this is not an exception, because the people who propose them are not in fact domain experts; politicians are domain experts in political tactics, like how to get elected and how to get legislation passed, but not in the world that policy acts upon. Perhaps no one could be.[3] This sense of \"paradigm\" was defined by Thomas Kuhn in his Structure of Scientific Revolutions, but I also recommend his Copernican Revolution, where you can see him at work developing the idea.[4] This is one reason people with a touch of Asperger's may have an advantage in discovering new ideas. They're always flying on instruments.[5] Hall, Rupert. From Galileo to Newton. Collins, 1963. This book is particularly good at getting into contemporaries' heads.Thanks to Trevor Blackwell, Patrick Collison,",
      " Suhail Doshi, Daniel Gackle, Jessica Livingston, and Robert Morris for reading drafts of this.Aaron Swartz created a scraped feed of the essays page.December 2014I've read Villehardouin's chronicle of the Fourth Crusade at least two times, maybe three.  And yet if I had to write down everything I remember from it, I doubt it would amount to much more than a page.  Multiply this times several hundred, and I get an uneasy feeling when I look at my bookshelves. What use is it to read all these books if I remember so little from them?A few months ago, as I was reading Constance Reid's excellent biography of Hilbert, I figured out if not the answer to this question, at least something that made me feel better about it. She writes:    Hilbert had no patience with mathematical lectures which filled   the students with facts but did not teach them how to frame a   problem and solve it. He often used to tell them that \"a perfect   formulation of a problem is already half its solution.\"  That has always seemed to me an important point, and I was even more convinced of it after hearing it confirmed by Hilbert.But how had I come to believe in this idea in the first place?",
      "  A combination of my own experience and other things I'd read.  None of which I could at that moment remember!  And eventually I'd forget that Hilbert had confirmed it too.  But my increased belief in the importance of this idea would remain something I'd learned from this book, even after I'd forgotten I'd learned it.Reading and experience train your model of the world.  And even if you forget the experience or what you read, its effect on your model of the world persists.  Your mind is like a compiled program you've lost the source of.  It works, but you don't know why.The place to look for what I learned from Villehardouin's chronicle is not what I remember from it, but my mental models of the crusades, Venice, medieval culture, siege warfare, and so on.  Which doesn't mean I couldn't have read more attentively, but at least the harvest of reading is not so miserably small as it might seem.This is one of those things that seem obvious in retrospect.  But it was a surprise to me and presumably would be to anyone else who felt uneasy about (apparently) forgetting so much they'd read.Realizing it does more than make you feel a little better about forgetting,",
      " though.  There are specific implications.For example, reading and experience are usually \"compiled\" at the time they happen, using the state of your brain at that time.  The same book would get compiled differently at different points in your life.  Which means it is very much worth reading important books multiple times.  I always used to feel some misgivings about rereading books.  I unconsciously lumped reading together with work like carpentry, where having to do something again is a sign you did it wrong the first time.  Whereas now the phrase \"already read\" seems almost ill-formed.Intriguingly, this implication isn't limited to books.  Technology will increasingly make it possible to relive our experiences.  When people do that today it's usually to enjoy them again (e.g. when looking at pictures of a trip) or to find the origin of some bug in their compiled code (e.g. when Stephen Fry succeeded in remembering the childhood trauma that prevented him from singing).  But as technologies for recording and playing back your life improve, it may become common for people to relive experiences without any goal in mind, simply to learn from them again as one might when rereading a book.Eventually we may be able not just to play back experiences but also to index and even edit them.",
      " So although not knowing how you know things may seem part of being human, it may not be. Thanks to Sam Altman, Jessica Livingston, and Robert Morris for reading  drafts of this.September 2007In high school I decided I was going to study philosophy in college. I had several motives, some more honorable than others.  One of the less honorable was to shock people.  College was regarded as job training where I grew up, so studying philosophy seemed an impressively impractical thing to do.  Sort of like slashing holes in your clothes or putting a safety pin through your ear, which were other forms of impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying philosophy would be a shortcut straight to wisdom.  All the people majoring in other things would just end up with a bunch of domain knowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you wouldn't find those in our high school library.  But I tried to read Plato and Aristotle.  I doubt I believed I understood them, but they sounded like they were talking about something important. I assumed I'd learn what in college.The summer before senior year I took some college classes.",
      "  I learned a lot in the calculus class, but I didn't learn much in Philosophy 101.  And yet my plan to study philosophy remained intact.  It was my fault I hadn't learned anything.  I hadn't read the books we were assigned carefully enough.  I'd give Berkeley's Principles of Human Knowledge another shot in college.  Anything so admired and so difficult to read must have something in it, if one could only figure out what.Twenty-six years later, I still don't understand Berkeley.  I have a nice edition of his collected works.  Will I ever read it?  Seems unlikely.The difference between then and now is that now I understand why Berkeley is probably not worth trying to understand.  I think I see now what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It didn't work out as I'd hoped.  I didn't learn any magical truths compared to which everything else was mere domain knowledge.  But I do at least know now why I didn't.  Philosophy doesn't really have a subject matter in the way math or history or most other university subjects do.  There is no core of knowledge one must master.",
      "  The closest you come to that is a knowledge of what various individual philosophers have said about different topics over the years.  Few were sufficiently correct that people have forgotten who discovered what they discovered.Formal logic has some subject matter. I took several classes in logic.  I don't know if I learned anything from them. [1] It does seem to me very important to be able to flip ideas around in one's head: to see when two ideas don't fully cover the space of possibilities, or when one idea is the same as another but with a couple things changed.  But did studying logic teach me the importance of thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The most dramatic I learned immediately, in the first semester of freshman year, in a class taught by Sydney Shoemaker.  I learned that I don't exist.  I am (and you are) a collection of cells that lurches around driven by various forces, and calls itself I.  But there's no central, indivisible thing that your identity goes with. You could conceivably lose half your brain and live.  Which means your brain could conceivably be split into two halves and each transplanted into different bodies.",
      "  Imagine waking up after such an operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life are fuzzy, and break down if pushed too hard.  Even a concept as dear to us as I.  It took me a while to grasp this, but when I did it was fairly sudden, like someone in the nineteenth century grasping evolution and realizing the story of creation they'd been told as a child was all wrong.  [2] Outside of math there's a limit to how far you can push words; in fact, it would not be a bad definition of math to call it the study of terms that have precise meanings.  Everyday words are inherently imprecise.  They work well enough in everyday life that you don't notice.  Words seem to work, just as Newtonian physics seems to.  But you can always make them break if you push them far enough.I would say that this has been, unfortunately for philosophy, the central fact of philosophy.  Most philosophical debates are not merely afflicted by but driven by confusions over words.  Do we have free will?  Depends what you mean by \"free.\" Do abstract ideas exist?  Depends what you mean by \"exist.\"Wittgenstein is popularly credited with the idea that most philosophical controversies are due to confusions over language.",
      "  I'm not sure how much credit to give him.  I suspect a lot of people realized this, but reacted simply by not studying philosophy, rather than becoming philosophy professors.How did things get this way?  Can something people have spent thousands of years studying really be a waste of time?  Those are interesting questions.  In fact, some of the most interesting questions you can ask about philosophy.  The most valuable way to approach the current philosophical tradition may be neither to get lost in pointless speculations like Berkeley, nor to shut them down like Wittgenstein, but to study it as an example of reason gone wrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle. What we know of their predecessors comes from fragments and references in later works; their doctrines could be described as speculative cosmology that occasionally strays into analysis.  Presumably they were driven by whatever makes people in every other society invent cosmologies. [3]With Socrates, Plato, and particularly Aristotle, this tradition turned a corner.  There started to be a lot more analysis.  I suspect Plato and Aristotle were encouraged in this by progress in math. Mathematicians had by then shown that you could figure things out in a much more conclusive way than by making up fine sounding stories about them.",
      "   [4]People talk so much about abstractions now that we don't realize what a leap it must have been when they first started to.  It was presumably many thousands of years between when people first started describing things as hot or cold and when someone asked \"what is heat?\"  No doubt it was a very gradual process.  We don't know if Plato or Aristotle were the first to ask any of the questions they did.  But their works are the oldest we have that do this on a large scale, and there is a freshness (not to say naivete) about them that suggests some of the questions they asked were new to them, at least.Aristotle in particular reminds me of the phenomenon that happens when people discover something new, and are so excited by it that they race through a huge percentage of the newly discovered territory in one lifetime.  If so, that's evidence of how new this kind of thinking was.  [5]This is all to explain how Plato and Aristotle can be very impressive and yet naive and mistaken.  It was impressive even to ask the questions they did.  That doesn't mean they always came up with good answers.  It's not considered insulting to say that ancient Greek mathematicians were naive in some respects,",
      " or at least lacked some concepts that would have made their lives easier.  So I hope people will not be too offended if I propose that ancient philosophers were similarly naive.  In particular, they don't seem to have fully grasped what I earlier called the central fact of philosophy: that words break if you push them too far.\"Much to the surprise of the builders of the first digital computers,\" Rod Brooks wrote, \"programs written for them usually did not work.\" [6] Something similar happened when people first started trying to talk about abstractions.  Much to their surprise, they didn't arrive at answers they agreed upon.  In fact, they rarely seemed to arrive at answers at all.They were in effect arguing about artifacts induced by sampling at too low a resolution.The proof of how useless some of their answers turned out to be is how little effect they have.  No one after reading Aristotle's Metaphysics does anything differently as a result. [7]Surely I'm not claiming that ideas have to have practical applications to be interesting?  No, they may not have to.  Hardy's boast that number theory had no use whatsoever wouldn't disqualify it.  But he turned out to be mistaken.  In fact, it's suspiciously hard to find a field of math that truly has no practical use.",
      "  And Aristotle's explanation of the ultimate goal of philosophy in Book A of the Metaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles. The examples he gives are convincing: an ordinary worker builds things a certain way out of habit; a master craftsman can do more because he grasps the underlying principles.  The trend is clear: the more general the knowledge, the more admirable it is.  But then he makes a mistake\u2014possibly the most important mistake in the history of philosophy.  He has noticed that theoretical knowledge is often acquired for its own sake, out of curiosity, rather than for any practical need.  So he proposes there are two kinds of theoretical knowledge: some that's useful in practical matters and some that isn't.  Since people interested in the latter are interested in it for its own sake, it must be more noble.  So he sets as his goal in the Metaphysics the exploration of knowledge that has no practical use.  Which means no alarms go off when he takes on grand but vaguely understood questions and ends up getting lost in a sea of words.His mistake was to confuse motive and result.  Certainly, people who want a deep understanding of something are often driven by curiosity rather than any practical need.",
      "  But that doesn't mean what they end up learning is useless.  It's very valuable in practice to have a deep understanding of what you're doing; even if you're never called on to solve advanced problems, you can see shortcuts in the solution of simple ones, and your knowledge won't break down in edge cases, as it would if you were relying on formulas you didn't understand.  Knowledge is power.  That's what makes theoretical knowledge prestigious.  It's also what causes smart people to be curious about certain things and not others; our DNA is not so disinterested as we might think.So while ideas don't have to have immediate practical applications to be interesting, the kinds of things we find interesting will surprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was partly that he set off with contradictory aims: to explore the most abstract ideas, guided by the assumption that they were useless. He was like an explorer looking for a territory to the north of him, starting with the assumption that it was located to the south.And since his work became the map used by generations of future explorers, he sent them off in the wrong direction as well.  [8] Perhaps worst of all, he protected them from both the criticism of outsiders and the promptings of their own inner compass by establishing the principle that the most noble sort of theoretical knowledge had to be useless.The Metaphysics is mostly a failed experiment.",
      "  A few ideas from it turned out to be worth keeping; the bulk of it has had no effect at all.  The Metaphysics is among the least read of all famous books.  It's not hard to understand the way Newton's Principia is, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately that was not the conclusion Aristotle's successors derived from works like the Metaphysics.  [9] Soon after, the western world fell on intellectual hard times.  Instead of version 1s to be superseded, the works of Plato and Aristotle became revered texts to be mastered and discussed.  And so things remained for a shockingly long time.  It was not till around 1600 (in Europe, where the center of gravity had shifted by then) that one found people confident enough to treat Aristotle's work as a catalog of mistakes.  And even then they rarely said so outright.If it seems surprising that the gap was so long, consider how little progress there was in math between Hellenistic times and the Renaissance.In the intervening years an unfortunate idea took hold:  that it was not only acceptable to produce works like the Metaphysics, but that it was a particularly prestigious line of work,",
      " done by a class of people called philosophers.  No one thought to go back and debug Aristotle's motivating argument.  And so instead of correcting the problem Aristotle discovered by falling into it\u2014that you can easily get lost if you talk too loosely about very abstract ideas\u2014they  continued to fall into it.The SingularityCuriously, however, the works they produced continued to attract new readers.  Traditional philosophy occupies a kind of singularity in this respect.  If you write in an unclear way about big ideas, you produce something that seems tantalizingly attractive to inexperienced but intellectually ambitious students.  Till one knows better, it's hard to distinguish something that's hard to understand because the writer was unclear in his own mind from something like a mathematical proof that's hard to understand because the ideas it represents are hard to understand.  To someone who hasn't learned the difference, traditional philosophy seems extremely attractive: as hard (and therefore impressive) as math, yet broader in scope. That was what lured me in as a high school student.This singularity is even more singular in having its own defense built in.  When things are hard to understand, people who suspect they're nonsense generally keep quiet.  There's no way to prove a text is meaningless.  The closest you can get is to show that the official judges of some class of texts can't distinguish them from placebos.",
      "  [10]And so instead of denouncing philosophy, most people who suspected it was a waste of time just studied other things.  That alone is fairly damning evidence, considering philosophy's claims.  It's supposed to be about the ultimate truths. Surely all smart people would be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might have corrected them, they tended to be self-perpetuating.  Bertrand Russell wrote in a letter in 1912:    Hitherto the people attracted to philosophy have been mostly those   who loved the big generalizations, which were all wrong, so that   few people with exact minds have taken up the subject. [11]  His response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery that most previous philosophy was a waste of time, which judging from the circumstantial evidence must have been made by every smart person who studied a little philosophy and declined to pursue it further, but for how he acted in response. [12] Instead of quietly switching to another field, he made a fuss, from inside.  He was Gorbachev.The field of philosophy is still shaken from the fright Wittgenstein gave it.",
      "  [13] Later in life he spent a lot of time talking about how words worked.  Since that seems to be allowed, that's what a lot of philosophers do now.  Meanwhile, sensing a vacuum in the metaphysical speculation department, the people who used to do literary criticism have been edging Kantward, under new names like \"literary theory,\" \"critical theory,\" and when they're feeling ambitious, plain \"theory.\"  The writing is the familiar word salad:    Gender is not like some of the other grammatical modes which   express precisely a mode of conception without any reality that   corresponds to the conceptual mode, and consequently do not express   precisely something in reality by which the intellect could be   moved to conceive a thing the way it does, even where that motive   is not something in the thing as such.   [14]  The singularity I've described is not going away.  There's a market for writing that sounds impressive and can't be disproven. There will always be both supply and demand.  So if one group abandons this territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility. Perhaps we should do what Aristotle meant to do,",
      " instead of what he did.  The goal he announces in the Metaphysics seems one worth pursuing: to discover the most general truths.  That sounds good. But instead of trying to discover them because they're useless, let's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised criterion, applicability, as a guide to keep us from wondering off into a swamp of abstractions.  Instead of trying to answer the question:    What are the most general truths?  let's try to answer the question    Of all the useful things we can say, which are the most general?  The test of utility I propose is whether we cause people who read what we've written to do anything differently afterward.  Knowing we have to give definite (if implicit) advice will keep us from straying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a different direction.As an example of a useful, general idea, consider that of the controlled experiment.  There's an idea that has turned out to be widely applicable.  Some might say it's part of science, but it's not part of any specific science; it's literally meta-physics (in our sense of \"meta\").   The idea of evolution is another.",
      " It turns out to have quite broad applications\u2014for example, in genetic algorithms and even product design.  Frankfurt's distinction between lying and bullshitting seems a promising recent example. [15]These seem to me what philosophy should look like: quite general observations that would cause someone who understood them to do something differently.Such observations will necessarily be about things that are imprecisely defined.  Once you start using words with precise meanings, you're doing math.  So starting from utility won't entirely solve the problem I described above\u2014it won't flush out the metaphysical singularity.  But it should help.  It gives people with good intentions a new roadmap into abstraction.  And they may thereby produce things that make the writing of the people with bad intentions look bad by comparison.One drawback of this approach is that it won't produce the sort of writing that gets you tenure.  And not just because it's not currently the fashion.  In order to get tenure in any field you must not arrive at conclusions that members of tenure committees can disagree with.  In practice there are two kinds of solutions to this problem. In math and the sciences, you can prove what you're saying, or at any rate adjust your conclusions so you're not claiming anything false (\"6 of 8 subjects had lower blood pressure after the treatment\"). In the humanities you can either avoid drawing any definite conclusions (e.g.",
      " conclude that an issue is a complex one), or draw conclusions so narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either of these routes.  At best you'll be able to achieve the essayist's standard of proof, not the mathematician's or the experimentalist's. And yet you won't be able to meet the usefulness test without implying definite and fairly broadly applicable conclusions.  Worse still, the usefulness test will tend to produce results that annoy people: there's no use in telling people things they already believe, and people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting to general plus useful by starting with useful and cranking up the generality may be unsuitable for junior professors trying to get tenure, but it's better for everyone else, including professors who already have it.  This side of the mountain is a nice gradual slope. You can start by writing things that are useful but very specific, and then gradually make them more general.  Joe's has good burritos. What makes a good burrito?  What makes good food?  What makes anything good?  You can take as long as you want.",
      "  You don't have to get all the way to the top of the mountain.  You don't have to tell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an encouraging thought.  The field is a lot younger than it seems. Though the first philosophers in the western tradition lived about 2500 years ago, it would be misleading to say the field is 2500 years old, because for most of that time the leading practitioners weren't doing much more than writing commentaries on Plato or Aristotle while watching over their shoulders for the next invading army.  In the times when they weren't, philosophy was hopelessly intermingled with religion.  It didn't shake itself free till a couple hundred years ago, and even then was afflicted by the structural problems I've described above.  If I say this, some will say it's a ridiculously overbroad and uncharitable generalization, and others will say it's old news, but here goes: judging from their works, most philosophers up to the present have been wasting their time.  So in a sense the field is still at the first step.  [16]That sounds a preposterous claim to make.  It won't seem so preposterous in 10,",
      "000 years.  Civilization always seems old, because it's always the oldest it's ever been.  The only way to say whether something is really old or not is by looking at structural evidence, and structurally philosophy is young; it's still reeling from the unexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot more to discover.Notes [1] In practice formal logic is not much use, because despite some progress in the last 150 years we're still only able to formalize a small percentage of statements.  We may never do that much better, for the same reason 1980s-style \"knowledge representation\" could never have worked; many statements may have no representation more concise than a huge, analog brain state.[2] It was harder for Darwin's contemporaries to grasp this than we can easily imagine.  The story of creation in the Bible is not just a Judeo-Christian concept; it's roughly what everyone must have believed since before people were people.  The hard part of grasping evolution was to realize that species weren't, as they seem to be, unchanging, but had instead evolved from different, simpler organisms over unimaginably long periods of time.Now we don't have to make that leap.",
      "  No one in an industrialized country encounters the idea of evolution for the first time as an adult.  Everyone's taught about it as a child, either as truth or heresy.[3] Greek philosophers before Plato wrote in verse.  This must have affected what they said.  If you try to write about the nature of the world in verse, it inevitably turns into incantation.  Prose lets you be more precise, and more tentative.[4] Philosophy is like math's ne'er-do-well brother.  It was born when Plato and Aristotle looked at the works of their predecessors and said in effect \"why can't you be more like your brother?\"  Russell was still saying the same thing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy the imprecise half.  It's probably inevitable that philosophy will suffer by comparison, because there's no lower bound to its precision. Bad math is merely boring, whereas bad philosophy is nonsense.  And yet there are some good ideas in the imprecise half.[5] Aristotle's best work was in logic and zoology, both of which he can  be said to have invented.  But the most dramatic departure from his predecessors was a new, much more analytical style of thinking.",
      "  He was arguably the first scientist.[6] Brooks, Rodney, Programming in Common Lisp, Wiley, 1985, p. 94.[7] Some would say we depend on Aristotle more than we realize, because his ideas were one of the ingredients in our common culture. Certainly a lot of the words we use have a connection with Aristotle, but it seems a bit much to suggest that we wouldn't have the concept of the essence of something or the distinction between matter and form if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to diff European culture with Chinese: what ideas did European culture have in 1800 that Chinese culture didn't, in virtue of Aristotle's contribution?[8] The meaning of the word \"philosophy\" has changed over time. In ancient times it covered a broad range of topics, comparable in scope to our \"scholarship\" (though without the methodological implications).  Even as late as Newton's time it included what we now call \"science.\"  But core of the subject today is still what seemed to Aristotle the core: the attempt to discover the most general truths.Aristotle didn't call this \"metaphysics.\"  That name got assigned to it because the books we now call the Metaphysics came after (meta = after)",
      " the Physics in the standard edition of Aristotle's works compiled by Andronicus of Rhodes three centuries later.  What we call \"metaphysics\" Aristotle called \"first philosophy.\"[9] Some of Aristotle's immediate successors may have realized this, but it's hard to say because most of their works are lost.[10] Sokal, Alan, \"Transgressing the Boundaries: Toward a Transformative Hermeneutics of Quantum Gravity,\" Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's aligned with some axe the audience already has to grind.  If this is so we should find it's most popular with groups that are (or feel) weak.  The powerful don't need its reassurance.[11] Letter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991, p. 75.[12] A preliminary result, that all metaphysics between Aristotle and 1783 had been a waste of time, is due to I. Kant.[13] Wittgenstein asserted a sort of mastery to which the inhabitants of early 20th century Cambridge seem to have been peculiarly vulnerable\u2014perhaps partly because so many had been raised religious and then stopped believing,",
      " so had a vacant space in their heads for someone to tell them what to do (others chose Marx or Cardinal Newman), and partly because a quiet, earnest place like Cambridge in that era had no natural immunity to messianic figures, just as European politics then had no natural immunity to dictators.[14] This is actually from the Ordinatio of Duns Scotus (ca. 1300), with \"number\" replaced by \"gender.\"  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson, 1963, p. 92.[15] Frankfurt, Harry, On Bullshit,  Princeton University Press, 2005.[16] Some introductions to philosophy now take the line that philosophy is worth studying as a process rather than for any particular truths you'll learn.  The philosophers whose works they cover would be rolling in their graves at that.  They hoped they were doing more than serving as examples of how to argue: they hoped they were getting results.  Most were wrong, but it doesn't seem an impossible hope.This argument seems to me like someone in 1500 looking at the lack of results achieved by alchemy and saying its value was as a process. No,",
      " they were going about it wrong.  It turns out it is possible to transmute lead into gold (though not economically at current energy prices), but the route to that knowledge was to backtrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston,  Robert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.February 2021Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u0097 CPU, disk drives, printer, card reader \u0097 sitting up on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran.",
      " You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping.",
      "  [1]The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths.",
      " All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive,",
      " and programmers' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere. Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. It wasn't happening in a class, like it was supposed to, but that was ok. For the next couple years I was on a roll. I knew what I was going to do.For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my belief \u0097 hard to imagine now, but not unique in 1985 \u0097 that it was already climbing the lower slopes of intelligence.I had gotten into a program at Cornell that didn't make you choose a major. You could take whatever classes you liked, and choose whatever you liked to put on your degree. I of course chose \"Artificial Intelligence.\" When I got the actual physical diploma, I was dismayed to find that the quotes had been included, which made them read as scare-quotes. At the time this bothered me,",
      " but now it seems amusingly accurate, for reasons I was about to discover.I applied to 3 grad schools: MIT and Yale, which were renowned for AI at the time, and Harvard, which I'd visited because Rich Draves went there, and was also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU clone. Only Harvard accepted me, so that was where I went.I don't remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that's told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.What these programs really showed was that there's a subset of natural language that's a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. Its brokenness did,",
      " as so often happens, generate a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to get us Mike.So I looked around to see what I could salvage from the wreckage of my plans, and there was Lisp. I knew from experience that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp hacking. It's scary to think how little I knew about Lisp hacking when I started writing that book. But there's nothing like writing a book about something to help you learn it. The book, On Lisp, wasn't published till 1993, but I wrote much of it in grad school.Computer Science is an uneasy alliance between two halves, theory and systems. The theory people prove things, and the systems people build things. I wanted to build things. I had plenty of respect for theory \u0097 indeed, a sneaking suspicion that it was the more admirable of the two halves \u0097 but building things seemed so much more exciting.The problem with systems work, though, was that it didn't last. Any program you wrote today,",
      " no matter how good, would be obsolete in a couple decades at best. People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that, in its time, it had been good.There were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would last.In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.And moreover this was something you could make a living doing.",
      " Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.I had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art \u0097 that it didn't just appear spontaneously \u0097 but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.That fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.So now I was in a PhD program in computer science, yet planning to be an artist, yet also genuinely in love with Lisp hacking and working away at On Lisp. In other words, like many a grad student, I was working energetically on multiple projects that were not my thesis.I didn't see a way out of this situation.",
      " I didn't want to drop out of grad school, but how else was I going to get out? I remember when my friend Robert Morris got kicked out of Cornell for writing the internet worm of 1988, I was envious that he'd found such a spectacular way to get out of grad school.Then one day in April 1990 a crack appeared in the wall. I ran into professor Cheatham and he asked if I was far enough along to graduate that June. I didn't have a word of my dissertation written, but in what must have been the quickest bit of thinking in my life, I decided to take a shot at writing one in the 5 weeks or so that remained before the deadline, reusing parts of On Lisp where I could, and I was able to respond, with no perceptible delay \"Yes, I think so. I'll give you something to read in a few days.\"I picked applications of continuations as the topic. In retrospect I should have written about macros and embedded languages. There's a whole world there that's barely been explored. But all I wanted was to get out of grad school, and my rapidly written dissertation sufficed, just barely.Meanwhile I was applying to art schools. I applied to two: RISD in the US,",
      " and the Accademia di Belli Arti in Florence, which, because it was the oldest art school, I imagined would be good. RISD accepted me, and I never heard back from the Accademia, so off to Providence I went.I'd applied for the BFA program at RISD, which meant in effect that I had to go to college again. This was not as strange as it sounds, because I was only 25, and art schools are full of people of different ages. RISD counted me as a transfer sophomore and said I had to do the foundation that summer. The foundation means the classes that everyone has to take in fundamental subjects like drawing, color, and design.Toward the end of the summer I got a big surprise: a letter from the Accademia, which had been delayed because they'd sent it to Cambridge England instead of Cambridge Massachusetts, inviting me to take the entrance exam in Florence that fall. This was now only weeks away. My nice landlady let me leave my stuff in her attic. I had some money saved from consulting work I'd done in grad school; there was probably enough to last a year if I lived cheaply. Now all I had to do was learn Italian.Only stranieri (foreigners)",
      " had to take this entrance exam. In retrospect it may well have been a way of excluding them, because there were so many stranieri attracted by the idea of studying art in Florence that the Italian students would otherwise have been outnumbered. I was in decent shape at painting and drawing from the RISD foundation that summer, but I still don't know how I managed to pass the written exam. I remember that I answered the essay question by writing about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited vocabulary.  [2]I'm only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed. The students and faculty in the painting department at the Accademia were the nicest people you could imagine, but they had long since arrived at an arrangement whereby the students wouldn't require the faculty to teach anything, and in return the faculty wouldn't require the students to learn anything. And at the same time all involved would adhere outwardly to the conventions of a 19th century atelier. We actually had one of those little stoves,",
      " fed with kindling, that you see in 19th century studio paintings, and a nude model sitting as close to it as possible without getting burned. Except hardly anyone else painted her besides me. The rest of the students spent their time chatting or occasionally trying to imitate things they'd seen in American art magazines.Our model turned out to live just down the street from me. She made a living from a combination of modelling and making fakes for a local antique dealer. She'd copy an obscure old painting out of a book, and then he'd take the copy and maltreat it to make it look old.  [3]While I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny, because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still lives is different from painting people, because the subject, as its name suggests, can't move. People can't sit for more than about 15 minutes at a time, and when they do they don't sit very still. So the traditional m.o. for painting people is to know how to paint a generic person, which you then modify to match the specific person you're painting.",
      " Whereas a still life you can, if you want, copy pixel by pixel from what you're seeing. You don't want to stop there, of course, or you get merely photographic accuracy, and what makes a still life interesting is that it's been through a head. You want to emphasize the visual cues that tell you, for example, that the reason the color changes suddenly at a certain point is that it's the edge of an object. By subtly emphasizing such things you can make paintings that are more realistic than photographs not just in some metaphorical sense, but in the strict information-theoretic sense.  [4]I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren't consciously aware of much we're seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that's a water droplet\" without telling you details like where the lightest and darkest points are, or \"that's a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see.",
      " You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can  after days of trying to write an essay about something people usually take for granted.This is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.Our teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.I wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]Interleaf had done something pretty bold.",
      " Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.The good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that every hour, even when I was just sitting in a meeting.",
      " By living cheaply I not only managed to save enough to go back to RISD, but also paid off my college loans.I learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it's better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people, that cheap office space is no bargain if it's depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there's not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it.But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it's good to be the \"entry level\" option, even though that will be less prestigious, because if you're not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign.When I left to go back to RISD the next fall, I arranged to do freelance work for the group that did projects for customers,",
      " and this was how I survived for the next several years. When I came back to visit for a project later on, someone told me about a new thing called HTML, which was, as he described it, a derivative of SGML. Markup language enthusiasts were an occupational hazard at Interleaf and I ignored him, but this HTML thing later became a big part of my life.In the fall of 1992 I moved back to Providence to continue at RISD. The foundation had merely been intro stuff, and the Accademia had been a (very civilized) joke. Now I was going to see what real art school was like. But alas it was more like the Accademia than not. Better organized, certainly, and a lot more expensive, but it was now becoming clear that art school did not bear the same relationship to art that medical school bore to medicine. At least not the painting department. The textile department, which my next door neighbor belonged to, seemed to be pretty rigorous. No doubt illustration and architecture were too. But painting was post-rigorous. Painting students were supposed to express themselves, which to the more worldly ones meant to try to cook up some sort of distinctive signature style.A signature style is the visual equivalent of what in show business is known as a \"schtick\": something that immediately identifies the work as yours and no one else's.",
      " For example, when you see a painting that looks like a certain kind of cartoon, you know it's by Roy Lichtenstein. So if you see a big painting of this type hanging in the apartment of a hedge fund manager, you know he paid millions of dollars for it. That's not always why artists have a signature style, but it's usually why buyers pay a lot for such work. [6]There were plenty of earnest students too: kids who \"could draw\" in high school, and now had come to what was supposed to be the best art school in the country, to learn to draw even better. They tended to be confused and demoralized by what they found at RISD, but they kept going, because painting was what they did. I was not one of the kids who could draw in high school, but at RISD I was definitely closer to their tribe than the tribe of signature style seekers.I learned a lot in the color class I took at RISD, but otherwise I was basically teaching myself to paint, and I could do that for free. So in 1993 I dropped out. I hung around Providence for a bit, and then my college friend Nancy Parmet did me a big favor. A rent-controlled apartment in a building her mother owned in New York was becoming vacant.",
      " Did I want it? It wasn't much more than my current place, and New York was supposed to be where the artists were. So yes, I wanted it! [7]Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist \u0097 in the strictly technical sense of making paintings and living in New York.I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book,",
      " ANSI Common Lisp, is one that I painted around this time.)The best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.She liked to paint on big, square canvases, 4 to 5 feet on a side. One day in late 1994 as I was stretching one of these monsters there was something on the radio about a famous fund manager. He wasn't that much older than me, and was super rich. The thought suddenly occurred to me: why don't I become rich? Then I'll be able to work on whatever I want.Meanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me when I visited him in Cambridge, where he was now in grad school at Harvard. It seemed to me that the web would be a big deal. I'd seen what graphical user interfaces had done for the popularity of microcomputers. It seemed like the web would do the same for the internet.If I wanted to get rich,",
      " here was the next train leaving the station. I was right about that part. What I got wrong was the idea. I decided we should start a company to put art galleries online. I can't honestly say, after reading so many Y Combinator applications, that this was the worst startup idea ever, but it was up there. Art galleries didn't want to be online, and still don't, not the fancy ones. That's not how they sell. I wrote some software to generate web sites for galleries, and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up galleries. To call this a difficult sale would be an understatement. It was difficult to give away. A few galleries let us make sites for them for free, but none paid us.Then some online stores started to appear, and I realized that except for the order buttons they were identical to the sites we'd been generating for galleries. This impressive-sounding thing called an \"internet storefront\" was something we already knew how to build.So in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started trying to write software to build online stores. At first this was going to be normal desktop software,",
      " which in those days meant Windows software. That was an alarming prospect, because neither of us knew how to write Windows software or wanted to learn. We lived in the Unix world. But we decided we'd at least try writing a prototype store builder on Unix. Robert wrote a shopping cart, and I wrote a new site generator for stores \u0097 in Lisp, of course.We were working out of Robert's apartment in Cambridge. His roommate was away for big chunks of time, during which I got to sleep in his room. For some reason there was no bed frame or sheets, just a mattress on the floor. One morning as I was lying on this mattress I had an idea that made me sit up like a capital L. What if we ran the software on the server, and let users control it by clicking on links? Then we'd never have to write anything to run on users' computers. We could generate the sites on the same server we'd serve them from. Users wouldn't need anything more than a browser.This kind of software, known as a web app, is common now, but at the time it wasn't clear that it was even possible. To find out, we decided to try making a version of our store builder that you could control through the browser.",
      " A couple days later, on August 12, we had one that worked. The UI was horrible, but it proved you could build a whole store through the browser, without any client software or typing anything into the command line on the server.Now we felt like we were really onto something. I had visions of a whole new generation of software working this way. You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group called Release Engineering that seemed to be at least as big as the group that actually wrote the software. Now you could just update the software right on the server.We started a new company we called Viaweb, after the fact that our software worked via the web, and we got $10,000 in seed funding from Idelle's husband Julian. In return for that and doing the initial legal work and giving us business advice, we gave him 10% of the company. Ten years later this deal became the model for Y Combinator's. We knew founders needed something like this, because we'd needed it ourselves.At this stage I had a negative net worth, because the thousand dollars or so I had in the bank was more than counterbalanced by what I owed the government in taxes. (Had I diligently set aside the proper proportion of the money I'd made consulting for Interleaf?",
      " No, I had not.) So although Robert had his graduate student stipend, I needed that seed funding to live on.We originally hoped to launch in September, but we got more ambitious about the software as we worked on it. Eventually we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exactly like the static ones that would be generated later, except that instead of leading to static pages, the links all referred to closures stored in a hash table on the server.It helped to have studied art, because the main goal of an online store builder is to make users look legit, and the key to looking legit is high production values. If you get page layouts and fonts and colors right, you can make a guy running a store out of his bedroom look more legit than a big company.(If you're curious why my site looks so old-fashioned, it's because it's still made with this software. It may look clunky today, but in 1996 it was the last word in slick.)In September, Robert rebelled. \"We've been working on this for a month,\" he said, \"and it's still not done.\" This is funny in retrospect, because he would still be working on it almost 3 years later.",
      " But I decided it might be prudent to recruit more programmers, and I asked Robert who else in grad school with him was really good. He recommended Trevor Blackwell, which surprised me at first, because at that point I knew Trevor mainly for his plan to reduce everything in his life to a stack of notecards, which he carried around with him. But Rtm was right, as usual. Trevor turned out to be a frighteningly effective hacker.It was a lot of fun working with Robert and Trevor. They're the two most independent-minded people  I know, and in completely different ways. If you could see inside Rtm's brain it would look like a colonial New England church, and if you could see inside Trevor's it would look like the worst excesses of Austrian Rococo.We opened for business, with 6 stores, in January 1996. It was just as well we waited a few months, because although we worried we were late, we were actually almost fatally early. There was a lot of talk in the press then about ecommerce, but not many people actually wanted online stores. [8]There were three main parts to the software: the editor, which people used to build sites and which I wrote, the shopping cart, which Robert wrote,",
      " and the manager, which kept track of orders and statistics, and which Trevor wrote. In its time, the editor was one of the best general-purpose site builders. I kept the code tight and didn't have to integrate with any other software except Robert's and Trevor's, so it was quite fun to work on. If all I'd had to do was work on this software, the next 3 years would have been the easiest of my life. Unfortunately I had to do a lot more, all of it stuff I was worse at than programming, and the next 3 years were instead the most stressful.There were a lot of startups making ecommerce software in the second half of the 90s. We were determined to be the Microsoft Word, not the Interleaf. Which meant being easy to use and inexpensive. It was lucky for us that we were poor, because that caused us to make Viaweb even more inexpensive than we realized. We charged $100 a month for a small store and $300 a month for a big one. This low price was a big attraction, and a constant thorn in the sides of competitors, but it wasn't because of some clever insight that we set the price low. We had no idea what businesses paid for things. $300 a month seemed like a lot of money to us.We did a lot of things right by accident like that.",
      " For example, we did what's now called \"doing things that  don't scale,\" although at the time we would have described it as \"being so lame that we're driven to the most desperate measures to get users.\" The most common of which was building stores for them. This seemed particularly humiliating, since the whole raison d'etre of our software was that people could use it to make their own stores. But anything to get users.We learned a lot more about retail than we wanted to know. For example, that if you could only have a small image of a man's shirt (and all images were small then by present standards), it was better to have a closeup of the collar than a picture of the whole shirt. The reason I remember learning this was that it meant I had to rescan about 30 images of men's shirts. My first set of scans were so beautiful too.Though this felt wrong, it was exactly the right thing to be doing. Building stores for users taught us about retail, and about how it felt to use our software. I was initially both mystified and repelled by \"business\" and thought we needed a \"business person\" to be in charge of it, but once we started to get users, I was converted,",
      " in much the same way I was converted to  fatherhood once I had kids. Whatever users wanted, I was all theirs. Maybe one day we'd have so many users that I couldn't scan their images for them, but in the meantime there was nothing more important to do.Another thing I didn't get at the time is that  growth rate is the ultimate test of a startup. Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users. And that is the thing that matters in the sense that that's how much money you're making, and if you're not making enough, you might go out of business. But in the long term the growth rate takes care of the absolute number. If we'd been a startup I was advising at Y Combinator, I would have said: Stop being so stressed out, because you're doing fine. You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.Alas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble.",
      " A company with just a handful of employees would have seemed amateurish. So we didn't reach breakeven until about when Yahoo bought us in the summer of 1998. Which in turn meant we were at the mercy of investors for the entire life of the company. And since both we and our investors were noobs at startups, the result was a mess even by startup standards.It was a huge relief when Yahoo bought us. In principle our Viaweb stock was valuable. It was a share in a business that was profitable and growing rapidly. But it didn't feel very valuable to me; I had no idea how to value a business, but I was all too keenly aware of the near-death experiences we seemed to have every few months. Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb.",
      " For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.Yahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.When I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying.",
      " My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire. Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.But I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.So I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York,",
      " where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.When I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted yet).Meanwhile I looked for an apartment to buy. Now I could actually choose what neighborhood to live in. Where, I asked myself and various real estate agents,",
      " is the Cambridge of New York? Aided by occasional visits to actual Cambridge, I gradually realized there wasn't one. Huh.Around this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were the future. Why not build a web app for making web apps? Why not let people edit code on our server through the browser, and then host the resulting applications for them? [9] You could run all sorts of services on the servers that these applications could use just by making an API call: making and receiving phone calls, manipulating images, taking credit card payments, etc.I got so excited about this idea that I couldn't think about anything else. It seemed obvious that this was the future. I didn't particularly want to start another company, but it was clear that this idea would have to be embodied as one, so I decided to move to Cambridge and start it. I hoped to lure Robert into working on it with me, but there I ran into a hitch. Robert was now a postdoc at MIT, and though he'd made a lot of money the last time I'd lured him into working on one of my schemes, it had also been a huge time sink.",
      " So while he agreed that it sounded like a plausible idea, he firmly refused to work on it.Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted summer jobs, and we got to work trying to build what it's now clear is about twenty companies and several open source projects worth of software. The language for defining applications would of course be a dialect of Lisp. But I wasn't so naive as to assume I could spring an overt Lisp on a general audience; we'd hide the parentheses, like Dylan did.By then there was a name for the kind of company Viaweb was, an \"application service provider,\" or ASP. This name didn't last long before it was replaced by \"software as a service,\" but it was current for long enough that I named this new company after it: it was going to be called Aspra.I started working on the application builder, Dan worked on network infrastructure, and the two undergrads worked on the first two services (images and phone calls). But about halfway through the summer I realized I really didn't want to run a company \u0097 especially not a big one, which it was looking like this would have to be. I'd only started Viaweb because I needed the money.",
      " Now that I didn't need money anymore, why was I doing this? If this vision had to be realized as a company, then screw the vision. I'd build a subset that could be done as an open source project.Much to my surprise, the time I spent working on this stuff was not wasted after all. After we started Y Combinator, I would often encounter startups working on parts of this new architecture, and it was very useful to have spent so much time thinking about it and even trying to write some of it.The subset I would build as an open source project was the new Lisp, whose parentheses I now wouldn't even have to hide. A lot of Lisp hackers dream of building a new Lisp, partly because one of the distinctive features of the language is that it has dialects, and partly, I think, because we have in our minds a Platonic form of Lisp that all existing dialects fall short of. I certainly did. So at the end of the summer Dan and I switched to working on this new dialect of Lisp, which I called Arc, in a house I bought in Cambridge.The following spring, lightning struck. I was invited to give a talk at a Lisp conference, so I gave one about how we'd used Lisp at Viaweb.",
      " Afterward I put a postscript file of this talk online, on paulgraham.com, which I'd created years before using Viaweb but had never used for anything. In one day it got 30,000 page views. What on earth had happened? The referring urls showed that someone had posted it on Slashdot. [10]Wow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.This had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of  essays. [11]In the print era, the channel for publishing essays had been vanishingly small.",
      " Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]I've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.I knew that online essays would be a  marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.One of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm writing,",
      " and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious intellectually in something like the way Latin is, also seems about as hip.It's not that unprestigious types of work are good per se. But when you find yourself drawn to some kind of work despite its current lack of prestige, it's a sign both that there's something real to be discovered there, and that you have the right kind of motives. Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees you're not on the most common type of wrong one.Over the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later,",
      " twas said, porn studio), to use as an office.One night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.Jessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of  interviews with startup founders.When the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones,",
      " they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.One of my tricks for writing essays had always been to give talks. The prospect of having to stand up in front of a group of people and tell them something that won't waste their time is a great spur to the imagination. When the Harvard Computer Society, the undergrad computer club, asked me to give a talk, I decided I would tell them how to start a startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.So I gave this talk, in the course of which I told them that the best sources of seed funding were successful startup founders, because then they'd be sources of advice too. Whereupon it seemed they were all looking expectantly at me. Horrified at the prospect of having my inbox flooded by business plans (if I'd only known), I blurted out \"But not me!\" and went on with the talk. But afterward it occurred to me that I should really stop procrastinating about angel investing. I'd been meaning to since Yahoo bought us, and now it was 7 years later and I still hadn't done one angel investment.Meanwhile I had been scheming with Robert and Trevor about projects we could work on together.",
      " I missed working with them, and it seemed like there had to be something we could collaborate on.As Jessica and I were walking home from dinner on March 11, at the corner of Garden and Walker streets, these three threads converged. Screw the VCs who were taking so long to make up their minds. We'd start our own investment firm and actually implement the ideas we'd been talking about. I'd fund it, and Jessica could quit her job and work for it, and we'd get Robert and Trevor as partners too. [13]Once again, ignorance worked in our favor. We had no idea how to be angel investors, and in Boston in 2005 there were no Ron Conways to learn from. So we just made what seemed like the obvious choices, and some of the things we did turned out to be novel.There are multiple components to Y Combinator, and we didn't figure them all out at once. The part we got first was to be an angel firm. In those days, those two words didn't go together. There were VC firms, which were organized companies with people whose job it was to make investments, but they only did big, million dollar investments. And there were angels, who did smaller investments, but these were individuals who were usually focused on other things and made investments on the side.",
      " And neither of them helped founders enough in the beginning. We knew how helpless founders were in some respects, because we remembered how helpless we'd been. For example, one thing Julian had done for us that seemed to us like magic was to get us set up as a company. We were fine writing fairly difficult software, but actually getting incorporated, with bylaws and stock and all that stuff, how on earth did you do that? Our plan was not only to make seed investments, but to do for startups everything Julian had done for us.YC was not organized as a fund. It was cheap enough to run that we funded it with our own money. That went right by 99% of readers, but professional investors are thinking \"Wow, that means they got all the returns.\" But once again, this was not due to any particular insight on our part. We didn't know how VC firms were organized. It never occurred to us to try to raise a fund, and if it had, we wouldn't have known where to start. [14]The most distinctive thing about YC is the batch model: to fund a bunch of startups all at once, twice a year, and then to spend three months focusing intensively on trying to help them. That part we discovered by accident,",
      " not merely implicitly but explicitly due to our ignorance about investing. We needed to get experience as investors. What better way, we thought, than to fund a whole bunch of startups at once? We knew undergrads got temporary jobs at tech companies during the summer. Why not organize a summer program where they'd start startups instead? We wouldn't feel guilty for being in a sense fake investors, because they would in a similar sense be fake founders. So while we probably wouldn't make much money out of it, we'd at least get to practice being investors on them, and they for their part would probably have a more interesting summer than they would working at Microsoft.We'd use the building I owned in Cambridge as our headquarters. We'd all have dinner there once a week \u0097 on tuesdays, since I was already cooking for the thursday diners on thursdays \u0097 and after dinner we'd bring in experts on startups to give talks.We knew undergrads were deciding then about summer jobs, so in a matter of days we cooked up something we called the Summer Founders Program, and I posted an  announcement  on my site, inviting undergrads to apply. I had never imagined that writing essays would be a way to get \"deal flow,\" as investors call it,",
      " but it turned out to be the perfect source. [15] We got 225 applications for the Summer Founders Program, and we were surprised to find that a lot of them were from people who'd already graduated, or were about to that spring. Already this SFP thing was starting to feel more serious than we'd intended.We invited about 20 of the 225 groups to interview in person, and from those we picked 8 to fund. They were an impressive group. That first batch included reddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron Swartz, who had already helped write the RSS spec and would a few years later become a martyr for open access, and Sam Altman, who would later become the second president of YC. I don't think it was entirely luck that the first batch was so good. You had to be pretty bold to sign up for a weird thing like the Summer Founders Program instead of a summer job at a legit place like Microsoft or Goldman Sachs.The deal for startups was based on a combination of the deal we did with Julian ($10k for 10%) and what Robert said MIT grad students got for the summer ($6k). We invested $6k per founder, which in the typical two-founder case was $12k,",
      " in return for 6%. That had to be fair, because it was twice as good as the deal we ourselves had taken. Plus that first summer, which was really hot, Jessica brought the founders free air conditioners. [16]Fairly quickly I realized that we had stumbled upon the way to scale startup funding. Funding startups in batches was more convenient for us, because it meant we could do things for a lot of startups at once, but being part of a batch was better for the startups too. It solved one of the biggest problems faced by founders: the isolation. Now you not only had colleagues, but colleagues who understood the problems you were facing and could tell you how they were solving them.As YC grew, we started to notice other advantages of scale. The alumni became a tight community, dedicated to helping one another, and especially the current batch, whose shoes they remembered being in. We also noticed that the startups were becoming one another's customers. We used to refer jokingly to the \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots of startups get their initial set of customers almost entirely from among their batchmates.I had not originally intended YC to be a full-time job. I was going to do three things:",
      " hack, write essays, and work on YC. As YC grew, and I grew more excited about it, it started to take up a lot more than a third of my attention. But for the first few years I was still able to work on other things.In the summer of 2006, Robert and I started working on a new version of Arc. This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn't startup founders we wanted to reach. It was future startup founders. So I changed the name to Hacker News and the topic to whatever engaged one's intellectual curiosity.HN was no doubt good for YC, but it was also by far the biggest source of stress for me. If all I'd had to do was select and help founders, life would have been so easy. And that implies that HN was a mistake. Surely the biggest source of stress in one's work should at least be something close to the core of the work. Whereas I was like someone who was in pain while running a marathon not from the exertion of running,",
      " but because I had a blister from an ill-fitting shoe. When I was dealing with some urgent problem during YC, there was about a 60% chance it had to do with HN, and a 40% chance it had do with everything else combined. [17]As well as HN, I wrote all of YC's internal software in Arc. But while I continued to work a good deal in Arc, I gradually stopped working on Arc, partly because I didn't have time to, and partly because it was a lot less attractive to mess around with the language now that we had all this infrastructure depending on it. So now my three projects were reduced to two: writing essays and working on YC.YC was different from other kinds of work I've done. Instead of deciding for myself what to work on, the problems came to me. Every 6 months there was a new batch of startups, and their problems, whatever they were, became our problems. It was very engaging work, because their problems were quite varied, and the good founders were very effective. If you were trying to learn the most you could about startups in the shortest possible time, you couldn't have picked a better way to do it.There were parts of the job I didn't like.",
      " Disputes between cofounders, figuring out when people were lying to us, fighting with people who maltreated the startups, and so on. But I worked hard even at the parts I didn't like. I was haunted by something Kevin Hale once said about companies: \"No one works harder than the boss.\" He meant it both descriptively and prescriptively, and it was the second part that scared me. I wanted YC to be good, so if how hard I worked set the upper bound on how hard everyone else worked, I'd better work very hard.One day in 2010, when he was visiting California for interviews, Robert Morris did something astonishing: he offered me unsolicited advice. I can only remember him doing that once before. One day at Viaweb, when I was bent over double from a kidney stone, he suggested that it would be a good idea for him to take me to the hospital. That was what it took for Rtm to offer unsolicited advice. So I remember his exact words very clearly. \"You know,\" he said, \"you should make sure Y Combinator isn't the last cool thing you do.\"At the time I didn't understand what he meant, but gradually it dawned on me that he was saying I should quit.",
      " This seemed strange advice, because YC was doing great. But if there was one thing rarer than Rtm offering advice, it was Rtm being wrong. So this set me thinking. It was true that on my current trajectory, YC would be the last thing I did, because it was only taking up more of my attention. It had already eaten Arc, and was in the process of eating essays too. Either YC was my life's work or I'd have to leave eventually. And it wasn't, so I would.In the summer of 2012 my mother had a stroke, and the cause turned out to be a blood clot caused by colon cancer. The stroke destroyed her balance, and she was put in a nursing home, but she really wanted to get out of it and back to her house, and my sister and I were determined to help her do it. I used to fly up to Oregon to visit her regularly, and I had a lot of time to think on those flights. On one of them I realized I was ready to hand YC over to someone else.I asked Jessica if she wanted to be president, but she didn't, so we decided we'd try to recruit Sam Altman. We talked to Robert and Trevor and we agreed to make it a complete changing of the guard.",
      " Up till that point YC had been controlled by the original LLC we four had started. But we wanted YC to last for a long time, and to do that it couldn't be controlled by the founders. So if Sam said yes, we'd let him reorganize YC. Robert and I would retire, and Jessica and Trevor would become ordinary partners.When we asked Sam if he wanted to be president of YC, initially he said no. He wanted to start a startup to make nuclear reactors. But I kept at it, and in October 2013 he finally agreed. We decided he'd take over starting with the winter 2014 batch. For the rest of 2013 I left running YC more and more to Sam, partly so he could learn the job, and partly because I was focused on my mother, whose"
    ]
  },
  {
    "id": 107,
    "question": "What is the best way to improve your vocabulary?",
    "answer": "by reading extensively and using new words in conversation.",
    "docs": [
      "Aaron Swartz created a scraped feed of the essays page.February 2021Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines \u0097 CPU, disk drives, printer, card reader \u0097 sitting up on a raised floor under bright fluorescent lights.The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.I was puzzled by the 1401.",
      " I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping.  [1]The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it,",
      " typing programs right into the computer.Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.I couldn't have put this into words when I was 18.",
      " All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.There weren't any classes in AI at Cornell then, not even graduate classes, so I started trying to teach myself. Which meant learning Lisp, since in those days Lisp was regarded as the language of AI. The commonly used programming languages then were pretty primitive, and programmers' ideas correspondingly so. The default language at Cornell was a Pascal-like language called PL/I, and the situation was similar elsewhere.",
      " Learning Lisp expanded my concept of a program so fast that it was years before I started to have a sense of where the new limits were. This was more like it; this was what I had expected college to do. It wasn't happening in a class, like it was supposed to, but that was ok. For the next couple years I was on a roll. I knew what I was going to do.For my undergraduate thesis, I reverse-engineered SHRDLU. My God did I love working on that program. It was a pleasing bit of code, but what made it even more exciting was my belief \u0097 hard to imagine now, but not unique in 1985 \u0097 that it was already climbing the lower slopes of intelligence.I had gotten into a program at Cornell that didn't make you choose a major. You could take whatever classes you liked, and choose whatever you liked to put on your degree. I of course chose \"Artificial Intelligence.\" When I got the actual physical diploma, I was dismayed to find that the quotes had been included, which made them read as scare-quotes. At the time this bothered me, but now it seems amusingly accurate, for reasons I was about to discover.I applied to 3 grad schools: MIT and Yale,",
      " which were renowned for AI at the time, and Harvard, which I'd visited because Rich Draves went there, and was also home to Bill Woods, who'd invented the type of parser I used in my SHRDLU clone. Only Harvard accepted me, so that was where I went.I don't remember the moment it happened, or if there even was a specific moment, but during the first year of grad school I realized that AI, as practiced at the time, was a hoax. By which I mean the sort of AI in which a program that's told \"the dog is sitting on the chair\" translates this into some formal representation and adds it to the list of things it knows.What these programs really showed was that there's a subset of natural language that's a formal language. But a very proper subset. It was clear that there was an unbridgeable gap between what they could do and actually understanding natural language. It was not, in fact, simply a matter of teaching SHRDLU more words. That whole way of doing AI, with explicit data structures representing concepts, was not going to work. Its brokenness did, as so often happens, generate a lot of opportunities to write papers about various band-aids that could be applied to it, but it was never going to get us Mike.So I looked around to see what I could salvage from the wreckage of my plans,",
      " and there was Lisp. I knew from experience that Lisp was interesting for its own sake and not just for its association with AI, even though that was the main reason people cared about it at the time. So I decided to focus on Lisp. In fact, I decided to write a book about Lisp hacking. It's scary to think how little I knew about Lisp hacking when I started writing that book. But there's nothing like writing a book about something to help you learn it. The book, On Lisp, wasn't published till 1993, but I wrote much of it in grad school.Computer Science is an uneasy alliance between two halves, theory and systems. The theory people prove things, and the systems people build things. I wanted to build things. I had plenty of respect for theory \u0097 indeed, a sneaking suspicion that it was the more admirable of the two halves \u0097 but building things seemed so much more exciting.The problem with systems work, though, was that it didn't last. Any program you wrote today, no matter how good, would be obsolete in a couple decades at best. People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that,",
      " in its time, it had been good.There were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.I wanted not just to build things, but to build things that would last.In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.And moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss,",
      " or even need to get research funding.I had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art \u0097 that it didn't just appear spontaneously \u0097 but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.That fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.So now I was in a PhD program in computer science, yet planning to be an artist, yet also genuinely in love with Lisp hacking and working away at On Lisp. In other words, like many a grad student, I was working energetically on multiple projects that were not my thesis.I didn't see a way out of this situation. I didn't want to drop out of grad school, but how else was I going to get out? I remember when my friend Robert Morris got kicked out of Cornell for writing the internet worm of 1988,",
      " I was envious that he'd found such a spectacular way to get out of grad school.Then one day in April 1990 a crack appeared in the wall. I ran into professor Cheatham and he asked if I was far enough along to graduate that June. I didn't have a word of my dissertation written, but in what must have been the quickest bit of thinking in my life, I decided to take a shot at writing one in the 5 weeks or so that remained before the deadline, reusing parts of On Lisp where I could, and I was able to respond, with no perceptible delay \"Yes, I think so. I'll give you something to read in a few days.\"I picked applications of continuations as the topic. In retrospect I should have written about macros and embedded languages. There's a whole world there that's barely been explored. But all I wanted was to get out of grad school, and my rapidly written dissertation sufficed, just barely.Meanwhile I was applying to art schools. I applied to two: RISD in the US, and the Accademia di Belli Arti in Florence, which, because it was the oldest art school, I imagined would be good. RISD accepted me, and I never heard back from the Accademia,",
      " so off to Providence I went.I'd applied for the BFA program at RISD, which meant in effect that I had to go to college again. This was not as strange as it sounds, because I was only 25, and art schools are full of people of different ages. RISD counted me as a transfer sophomore and said I had to do the foundation that summer. The foundation means the classes that everyone has to take in fundamental subjects like drawing, color, and design.Toward the end of the summer I got a big surprise: a letter from the Accademia, which had been delayed because they'd sent it to Cambridge England instead of Cambridge Massachusetts, inviting me to take the entrance exam in Florence that fall. This was now only weeks away. My nice landlady let me leave my stuff in her attic. I had some money saved from consulting work I'd done in grad school; there was probably enough to last a year if I lived cheaply. Now all I had to do was learn Italian.Only stranieri (foreigners) had to take this entrance exam. In retrospect it may well have been a way of excluding them, because there were so many stranieri attracted by the idea of studying art in Florence that the Italian students would otherwise have been outnumbered.",
      " I was in decent shape at painting and drawing from the RISD foundation that summer, but I still don't know how I managed to pass the written exam. I remember that I answered the essay question by writing about Cezanne, and that I cranked up the intellectual level as high as I could to make the most of my limited vocabulary.  [2]I'm only up to age 25 and already there are such conspicuous patterns. Here I was, yet again about to attend some august institution in the hopes of learning about some prestigious subject, and yet again about to be disappointed. The students and faculty in the painting department at the Accademia were the nicest people you could imagine, but they had long since arrived at an arrangement whereby the students wouldn't require the faculty to teach anything, and in return the faculty wouldn't require the students to learn anything. And at the same time all involved would adhere outwardly to the conventions of a 19th century atelier. We actually had one of those little stoves, fed with kindling, that you see in 19th century studio paintings, and a nude model sitting as close to it as possible without getting burned. Except hardly anyone else painted her besides me. The rest of the students spent their time chatting or occasionally trying to imitate things they'd seen in American art magazines.Our model turned out to live just down the street from me.",
      " She made a living from a combination of modelling and making fakes for a local antique dealer. She'd copy an obscure old painting out of a book, and then he'd take the copy and maltreat it to make it look old.  [3]While I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny, because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still lives is different from painting people, because the subject, as its name suggests, can't move. People can't sit for more than about 15 minutes at a time, and when they do they don't sit very still. So the traditional m.o. for painting people is to know how to paint a generic person, which you then modify to match the specific person you're painting. Whereas a still life you can, if you want, copy pixel by pixel from what you're seeing. You don't want to stop there, of course, or you get merely photographic accuracy, and what makes a still life interesting is that it's been through a head. You want to emphasize the visual cues that tell you, for example, that the reason the color changes suddenly at a certain point is that it's the edge of an object.",
      " By subtly emphasizing such things you can make paintings that are more realistic than photographs not just in some metaphorical sense, but in the strict information-theoretic sense.  [4]I liked painting still lives because I was curious about what I was seeing. In everyday life, we aren't consciously aware of much we're seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that's a water droplet\" without telling you details like where the lightest and darkest points are, or \"that's a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can  after days of trying to write an essay about something people usually take for granted.This is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.Our teacher, professor Ulivi, was a nice guy.",
      " He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.I wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]Interleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it,",
      " I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.The good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that every hour, even when I was just sitting in a meeting. By living cheaply I not only managed to save enough to go back to RISD, but also paid off my college loans.I learned some useful things at Interleaf, though they were mostly about what not to do. I learned that it's better for technology companies to be run by product people than sales people (though sales is a real skill and people who are good at it are really good at it), that it leads to bugs when code is edited by too many people,",
      " that cheap office space is no bargain if it's depressing, that planned meetings are inferior to corridor conversations, that big, bureaucratic customers are a dangerous source of money, and that there's not much overlap between conventional office hours and the optimal time for hacking, or conventional offices and the optimal place for it.But the most important thing I learned, and which I used in both Viaweb and Y Combinator, is that the low end eats the high end: that it's good to be the \"entry level\" option, even though that will be less prestigious, because if you're not, someone else will be, and will squash you against the ceiling. Which in turn means that prestige is a danger sign.When I left to go back to RISD the next fall, I arranged to do freelance work for the group that did projects for customers, and this was how I survived for the next several years. When I came back to visit for a project later on, someone told me about a new thing called HTML, which was, as he described it, a derivative of SGML. Markup language enthusiasts were an occupational hazard at Interleaf and I ignored him, but this HTML thing later became a big part of my life.In the fall of 1992 I moved back to Providence to continue at RISD.",
      " The foundation had merely been intro stuff, and the Accademia had been a (very civilized) joke. Now I was going to see what real art school was like. But alas it was more like the Accademia than not. Better organized, certainly, and a lot more expensive, but it was now becoming clear that art school did not bear the same relationship to art that medical school bore to medicine. At least not the painting department. The textile department, which my next door neighbor belonged to, seemed to be pretty rigorous. No doubt illustration and architecture were too. But painting was post-rigorous. Painting students were supposed to express themselves, which to the more worldly ones meant to try to cook up some sort of distinctive signature style.A signature style is the visual equivalent of what in show business is known as a \"schtick\": something that immediately identifies the work as yours and no one else's. For example, when you see a painting that looks like a certain kind of cartoon, you know it's by Roy Lichtenstein. So if you see a big painting of this type hanging in the apartment of a hedge fund manager, you know he paid millions of dollars for it. That's not always why artists have a signature style, but it's usually why buyers pay a lot for such work.",
      " [6]There were plenty of earnest students too: kids who \"could draw\" in high school, and now had come to what was supposed to be the best art school in the country, to learn to draw even better. They tended to be confused and demoralized by what they found at RISD, but they kept going, because painting was what they did. I was not one of the kids who could draw in high school, but at RISD I was definitely closer to their tribe than the tribe of signature style seekers.I learned a lot in the color class I took at RISD, but otherwise I was basically teaching myself to paint, and I could do that for free. So in 1993 I dropped out. I hung around Providence for a bit, and then my college friend Nancy Parmet did me a big favor. A rent-controlled apartment in a building her mother owned in New York was becoming vacant. Did I want it? It wasn't much more than my current place, and New York was supposed to be where the artists were. So yes, I wanted it! [7]Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City:",
      " if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist \u0097 in the strictly technical sense of making paintings and living in New York.I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around this time.)The best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her,",
      " including me. After I moved to New York I became her de facto studio assistant.She liked to paint on big, square canvases, 4 to 5 feet on a side. One day in late 1994 as I was stretching one of these monsters there was something on the radio about a famous fund manager. He wasn't that much older than me, and was super rich. The thought suddenly occurred to me: why don't I become rich? Then I'll be able to work on whatever I want.Meanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me when I visited him in Cambridge, where he was now in grad school at Harvard. It seemed to me that the web would be a big deal. I'd seen what graphical user interfaces had done for the popularity of microcomputers. It seemed like the web would do the same for the internet.If I wanted to get rich, here was the next train leaving the station. I was right about that part. What I got wrong was the idea. I decided we should start a company to put art galleries online. I can't honestly say, after reading so many Y Combinator applications, that this was the worst startup idea ever,",
      " but it was up there. Art galleries didn't want to be online, and still don't, not the fancy ones. That's not how they sell. I wrote some software to generate web sites for galleries, and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up galleries. To call this a difficult sale would be an understatement. It was difficult to give away. A few galleries let us make sites for them for free, but none paid us.Then some online stores started to appear, and I realized that except for the order buttons they were identical to the sites we'd been generating for galleries. This impressive-sounding thing called an \"internet storefront\" was something we already knew how to build.So in the summer of 1995, after I submitted the camera-ready copy of ANSI Common Lisp to the publishers, we started trying to write software to build online stores. At first this was going to be normal desktop software, which in those days meant Windows software. That was an alarming prospect, because neither of us knew how to write Windows software or wanted to learn. We lived in the Unix world. But we decided we'd at least try writing a prototype store builder on Unix. Robert wrote a shopping cart,",
      " and I wrote a new site generator for stores \u0097 in Lisp, of course.We were working out of Robert's apartment in Cambridge. His roommate was away for big chunks of time, during which I got to sleep in his room. For some reason there was no bed frame or sheets, just a mattress on the floor. One morning as I was lying on this mattress I had an idea that made me sit up like a capital L. What if we ran the software on the server, and let users control it by clicking on links? Then we'd never have to write anything to run on users' computers. We could generate the sites on the same server we'd serve them from. Users wouldn't need anything more than a browser.This kind of software, known as a web app, is common now, but at the time it wasn't clear that it was even possible. To find out, we decided to try making a version of our store builder that you could control through the browser. A couple days later, on August 12, we had one that worked. The UI was horrible, but it proved you could build a whole store through the browser, without any client software or typing anything into the command line on the server.Now we felt like we were really onto something.",
      " I had visions of a whole new generation of software working this way. You wouldn't need versions, or ports, or any of that crap. At Interleaf there had been a whole group called Release Engineering that seemed to be at least as big as the group that actually wrote the software. Now you could just update the software right on the server.We started a new company we called Viaweb, after the fact that our software worked via the web, and we got $10,000 in seed funding from Idelle's husband Julian. In return for that and doing the initial legal work and giving us business advice, we gave him 10% of the company. Ten years later this deal became the model for Y Combinator's. We knew founders needed something like this, because we'd needed it ourselves.At this stage I had a negative net worth, because the thousand dollars or so I had in the bank was more than counterbalanced by what I owed the government in taxes. (Had I diligently set aside the proper proportion of the money I'd made consulting for Interleaf? No, I had not.) So although Robert had his graduate student stipend, I needed that seed funding to live on.We originally hoped to launch in September, but we got more ambitious about the software as we worked on it.",
      " Eventually we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exactly like the static ones that would be generated later, except that instead of leading to static pages, the links all referred to closures stored in a hash table on the server.It helped to have studied art, because the main goal of an online store builder is to make users look legit, and the key to looking legit is high production values. If you get page layouts and fonts and colors right, you can make a guy running a store out of his bedroom look more legit than a big company.(If you're curious why my site looks so old-fashioned, it's because it's still made with this software. It may look clunky today, but in 1996 it was the last word in slick.)In September, Robert rebelled. \"We've been working on this for a month,\" he said, \"and it's still not done.\" This is funny in retrospect, because he would still be working on it almost 3 years later. But I decided it might be prudent to recruit more programmers, and I asked Robert who else in grad school with him was really good. He recommended Trevor Blackwell, which surprised me at first, because at that point I knew Trevor mainly for his plan to reduce everything in his life to a stack of notecards,",
      " which he carried around with him. But Rtm was right, as usual. Trevor turned out to be a frighteningly effective hacker.It was a lot of fun working with Robert and Trevor. They're the two most independent-minded people  I know, and in completely different ways. If you could see inside Rtm's brain it would look like a colonial New England church, and if you could see inside Trevor's it would look like the worst excesses of Austrian Rococo.We opened for business, with 6 stores, in January 1996. It was just as well we waited a few months, because although we worried we were late, we were actually almost fatally early. There was a lot of talk in the press then about ecommerce, but not many people actually wanted online stores. [8]There were three main parts to the software: the editor, which people used to build sites and which I wrote, the shopping cart, which Robert wrote, and the manager, which kept track of orders and statistics, and which Trevor wrote. In its time, the editor was one of the best general-purpose site builders. I kept the code tight and didn't have to integrate with any other software except Robert's and Trevor's, so it was quite fun to work on.",
      " If all I'd had to do was work on this software, the next 3 years would have been the easiest of my life. Unfortunately I had to do a lot more, all of it stuff I was worse at than programming, and the next 3 years were instead the most stressful.There were a lot of startups making ecommerce software in the second half of the 90s. We were determined to be the Microsoft Word, not the Interleaf. Which meant being easy to use and inexpensive. It was lucky for us that we were poor, because that caused us to make Viaweb even more inexpensive than we realized. We charged $100 a month for a small store and $300 a month for a big one. This low price was a big attraction, and a constant thorn in the sides of competitors, but it wasn't because of some clever insight that we set the price low. We had no idea what businesses paid for things. $300 a month seemed like a lot of money to us.We did a lot of things right by accident like that. For example, we did what's now called \"doing things that  don't scale,\" although at the time we would have described it as \"being so lame that we're driven to the most desperate measures to get users.\" The most common of which was building stores for them.",
      " This seemed particularly humiliating, since the whole raison d'etre of our software was that people could use it to make their own stores. But anything to get users.We learned a lot more about retail than we wanted to know. For example, that if you could only have a small image of a man's shirt (and all images were small then by present standards), it was better to have a closeup of the collar than a picture of the whole shirt. The reason I remember learning this was that it meant I had to rescan about 30 images of men's shirts. My first set of scans were so beautiful too.Though this felt wrong, it was exactly the right thing to be doing. Building stores for users taught us about retail, and about how it felt to use our software. I was initially both mystified and repelled by \"business\" and thought we needed a \"business person\" to be in charge of it, but once we started to get users, I was converted, in much the same way I was converted to  fatherhood once I had kids. Whatever users wanted, I was all theirs. Maybe one day we'd have so many users that I couldn't scan their images for them, but in the meantime there was nothing more important to do.Another thing I didn't get at the time is that  growth rate is the ultimate test of a startup.",
      " Our growth rate was fine. We had about 70 stores at the end of 1996 and about 500 at the end of 1997. I mistakenly thought the thing that mattered was the absolute number of users. And that is the thing that matters in the sense that that's how much money you're making, and if you're not making enough, you might go out of business. But in the long term the growth rate takes care of the absolute number. If we'd been a startup I was advising at Y Combinator, I would have said: Stop being so stressed out, because you're doing fine. You're growing 7x a year. Just don't hire too many more people and you'll soon be profitable, and then you'll control your own destiny.Alas I hired lots more people, partly because our investors wanted me to, and partly because that's what startups did during the Internet Bubble. A company with just a handful of employees would have seemed amateurish. So we didn't reach breakeven until about when Yahoo bought us in the summer of 1998. Which in turn meant we were at the mercy of investors for the entire life of the company. And since both we and our investors were noobs at startups, the result was a mess even by startup standards.It was a huge relief when Yahoo bought us.",
      " In principle our Viaweb stock was valuable. It was a share in a business that was profitable and growing rapidly. But it didn't feel very valuable to me; I had no idea how to value a business, but I was all too keenly aware of the near-death experiences we seemed to have every few months. Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.Yahoo had given us a lot of options when they bought us.",
      " At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.When I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying. My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire.",
      " Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.But I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.So I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York, where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.When I got back to New York I resumed my old life, except now I was rich.",
      " It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted yet).Meanwhile I looked for an apartment to buy. Now I could actually choose what neighborhood to live in. Where, I asked myself and various real estate agents, is the Cambridge of New York? Aided by occasional visits to actual Cambridge, I gradually realized there wasn't one. Huh.Around this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were the future. Why not build a web app for making web apps? Why not let people edit code on our server through the browser,",
      " and then host the resulting applications for them? [9] You could run all sorts of services on the servers that these applications could use just by making an API call: making and receiving phone calls, manipulating images, taking credit card payments, etc.I got so excited about this idea that I couldn't think about anything else. It seemed obvious that this was the future. I didn't particularly want to start another company, but it was clear that this idea would have to be embodied as one, so I decided to move to Cambridge and start it. I hoped to lure Robert into working on it with me, but there I ran into a hitch. Robert was now a postdoc at MIT, and though he'd made a lot of money the last time I'd lured him into working on one of my schemes, it had also been a huge time sink. So while he agreed that it sounded like a plausible idea, he firmly refused to work on it.Hmph. Well, I'd do it myself then. I recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted summer jobs, and we got to work trying to build what it's now clear is about twenty companies and several open source projects worth of software. The language for defining applications would of course be a dialect of Lisp.",
      " But I wasn't so naive as to assume I could spring an overt Lisp on a general audience; we'd hide the parentheses, like Dylan did.By then there was a name for the kind of company Viaweb was, an \"application service provider,\" or ASP. This name didn't last long before it was replaced by \"software as a service,\" but it was current for long enough that I named this new company after it: it was going to be called Aspra.I started working on the application builder, Dan worked on network infrastructure, and the two undergrads worked on the first two services (images and phone calls). But about halfway through the summer I realized I really didn't want to run a company \u0097 especially not a big one, which it was looking like this would have to be. I'd only started Viaweb because I needed the money. Now that I didn't need money anymore, why was I doing this? If this vision had to be realized as a company, then screw the vision. I'd build a subset that could be done as an open source project.Much to my surprise, the time I spent working on this stuff was not wasted after all. After we started Y Combinator, I would often encounter startups working on parts of this new architecture,",
      " and it was very useful to have spent so much time thinking about it and even trying to write some of it.The subset I would build as an open source project was the new Lisp, whose parentheses I now wouldn't even have to hide. A lot of Lisp hackers dream of building a new Lisp, partly because one of the distinctive features of the language is that it has dialects, and partly, I think, because we have in our minds a Platonic form of Lisp that all existing dialects fall short of. I certainly did. So at the end of the summer Dan and I switched to working on this new dialect of Lisp, which I called Arc, in a house I bought in Cambridge.The following spring, lightning struck. I was invited to give a talk at a Lisp conference, so I gave one about how we'd used Lisp at Viaweb. Afterward I put a postscript file of this talk online, on paulgraham.com, which I'd created years before using Viaweb but had never used for anything. In one day it got 30,000 page views. What on earth had happened? The referring urls showed that someone had posted it on Slashdot. [10]Wow, I thought, there's an audience. If I write something and put it on the web,",
      " anyone can read it. That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.This had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of  essays. [11]In the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]I've worked on several different things, but to the extent there was a turning point where I figured out what to work on,",
      " it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.I knew that online essays would be a  marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.One of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm writing, and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious intellectually in something like the way Latin is, also seems about as hip.It's not that unprestigious types of work are good per se. But when you find yourself drawn to some kind of work despite its current lack of prestige, it's a sign both that there's something real to be discovered there, and that you have the right kind of motives.",
      " Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees you're not on the most common type of wrong one.Over the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.One night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot:",
      " a woman called Jessica Livingston. A couple days later I asked her out.Jessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of  interviews with startup founders.When the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital. They should make a larger number of smaller investments instead of a handful of giant ones, they should be funding younger, more technical founders instead of MBAs, they should let the founders remain as CEO, and so on.One of my tricks for writing essays had always been to give talks. The prospect of having to stand up in front of a group of people and tell them something that won't waste their time is a great spur to the imagination. When the Harvard Computer Society, the undergrad computer club, asked me to give a talk,",
      " I decided I would tell them how to start a startup. Maybe they'd be able to avoid the worst of the mistakes we'd made.So I gave this talk, in the course of which I told them that the best sources of seed funding were successful startup founders, because then they'd be sources of advice too. Whereupon it seemed they were all looking expectantly at me. Horrified at the prospect of having my inbox flooded by business plans (if I'd only known), I blurted out \"But not me!\" and went on with the talk. But afterward it occurred to me that I should really stop procrastinating about angel investing. I'd been meaning to since Yahoo bought us, and now it was 7 years later and I still hadn't done one angel investment.Meanwhile I had been scheming with Robert and Trevor about projects we could work on together. I missed working with them, and it seemed like there had to be something we could collaborate on.As Jessica and I were walking home from dinner on March 11, at the corner of Garden and Walker streets, these three threads converged. Screw the VCs who were taking so long to make up their minds. We'd start our own investment firm and actually implement the ideas we'd been talking about.",
      " I'd fund it, and Jessica could quit her job and work for it, and we'd get Robert and Trevor as partners too. [13]Once again, ignorance worked in our favor. We had no idea how to be angel investors, and in Boston in 2005 there were no Ron Conways to learn from. So we just made what seemed like the obvious choices, and some of the things we did turned out to be novel.There are multiple components to Y Combinator, and we didn't figure them all out at once. The part we got first was to be an angel firm. In those days, those two words didn't go together. There were VC firms, which were organized companies with people whose job it was to make investments, but they only did big, million dollar investments. And there were angels, who did smaller investments, but these were individuals who were usually focused on other things and made investments on the side. And neither of them helped founders enough in the beginning. We knew how helpless founders were in some respects, because we remembered how helpless we'd been. For example, one thing Julian had done for us that seemed to us like magic was to get us set up as a company. We were fine writing fairly difficult software, but actually getting incorporated,",
      " with bylaws and stock and all that stuff, how on earth did you do that? Our plan was not only to make seed investments, but to do for startups everything Julian had done for us.YC was not organized as a fund. It was cheap enough to run that we funded it with our own money. That went right by 99% of readers, but professional investors are thinking \"Wow, that means they got all the returns.\" But once again, this was not due to any particular insight on our part. We didn't know how VC firms were organized. It never occurred to us to try to raise a fund, and if it had, we wouldn't have known where to start. [14]The most distinctive thing about YC is the batch model: to fund a bunch of startups all at once, twice a year, and then to spend three months focusing intensively on trying to help them. That part we discovered by accident, not merely implicitly but explicitly due to our ignorance about investing. We needed to get experience as investors. What better way, we thought, than to fund a whole bunch of startups at once? We knew undergrads got temporary jobs at tech companies during the summer. Why not organize a summer program where they'd start startups instead?",
      " We wouldn't feel guilty for being in a sense fake investors, because they would in a similar sense be fake founders. So while we probably wouldn't make much money out of it, we'd at least get to practice being investors on them, and they for their part would probably have a more interesting summer than they would working at Microsoft.We'd use the building I owned in Cambridge as our headquarters. We'd all have dinner there once a week \u0097 on tuesdays, since I was already cooking for the thursday diners on thursdays \u0097 and after dinner we'd bring in experts on startups to give talks.We knew undergrads were deciding then about summer jobs, so in a matter of days we cooked up something we called the Summer Founders Program, and I posted an  announcement  on my site, inviting undergrads to apply. I had never imagined that writing essays would be a way to get \"deal flow,\" as investors call it, but it turned out to be the perfect source. [15] We got 225 applications for the Summer Founders Program, and we were surprised to find that a lot of them were from people who'd already graduated, or were about to that spring. Already this SFP thing was starting to feel more serious than we'd intended.We invited about 20 of the 225 groups to interview in person,",
      " and from those we picked 8 to fund. They were an impressive group. That first batch included reddit, Justin Kan and Emmett Shear, who went on to found Twitch, Aaron Swartz, who had already helped write the RSS spec and would a few years later become a martyr for open access, and Sam Altman, who would later become the second president of YC. I don't think it was entirely luck that the first batch was so good. You had to be pretty bold to sign up for a weird thing like the Summer Founders Program instead of a summer job at a legit place like Microsoft or Goldman Sachs.The deal for startups was based on a combination of the deal we did with Julian ($10k for 10%) and what Robert said MIT grad students got for the summer ($6k). We invested $6k per founder, which in the typical two-founder case was $12k, in return for 6%. That had to be fair, because it was twice as good as the deal we ourselves had taken. Plus that first summer, which was really hot, Jessica brought the founders free air conditioners. [16]Fairly quickly I realized that we had stumbled upon the way to scale startup funding. Funding startups in batches was more convenient for us,",
      " because it meant we could do things for a lot of startups at once, but being part of a batch was better for the startups too. It solved one of the biggest problems faced by founders: the isolation. Now you not only had colleagues, but colleagues who understood the problems you were facing and could tell you how they were solving them.As YC grew, we started to notice other advantages of scale. The alumni became a tight community, dedicated to helping one another, and especially the current batch, whose shoes they remembered being in. We also noticed that the startups were becoming one another's customers. We used to refer jokingly to the \"YC GDP,\" but as YC grows this becomes less and less of a joke. Now lots of startups get their initial set of customers almost entirely from among their batchmates.I had not originally intended YC to be a full-time job. I was going to do three things: hack, write essays, and work on YC. As YC grew, and I grew more excited about it, it started to take up a lot more than a third of my attention. But for the first few years I was still able to work on other things.In the summer of 2006, Robert and I started working on a new version of Arc.",
      " This one was reasonably fast, because it was compiled into Scheme. To test this new Arc, I wrote Hacker News in it. It was originally meant to be a news aggregator for startup founders and was called Startup News, but after a few months I got tired of reading about nothing but startups. Plus it wasn't startup founders we wanted to reach. It was future startup founders. So I changed the name to Hacker News and the topic to whatever engaged one's intellectual curiosity.HN was no doubt good for YC, but it was also by far the biggest source of stress for me. If all I'd had to do was select and help founders, life would have been so easy. And that implies that HN was a mistake. Surely the biggest source of stress in one's work should at least be something close to the core of the work. Whereas I was like someone who was in pain while running a marathon not from the exertion of running, but because I had a blister from an ill-fitting shoe. When I was dealing with some urgent problem during YC, there was about a 60% chance it had to do with HN, and a 40% chance it had do with everything else combined. [17]As well as HN, I wrote all of YC's internal software in Arc.",
      " But while I continued to work a good deal in Arc, I gradually stopped working on Arc, partly because I didn't have time to, and partly because it was a lot less attractive to mess around with the language now that we had all this infrastructure depending on it. So now my three projects were reduced to two: writing essays and working on YC.YC was different from other kinds of work I've done. Instead of deciding for myself what to work on, the problems came to me. Every 6 months there was a new batch of startups, and their problems, whatever they were, became our problems. It was very engaging work, because their problems were quite varied, and the good founders were very effective. If you were trying to learn the most you could about startups in the shortest possible time, you couldn't have picked a better way to do it.There were parts of the job I didn't like. Disputes between cofounders, figuring out when people were lying to us, fighting with people who maltreated the startups, and so on. But I worked hard even at the parts I didn't like. I was haunted by something Kevin Hale once said about companies: \"No one works harder than the boss.\" He meant it both descriptively and prescriptively,",
      " and it was the second part that scared me. I wanted YC to be good, so if how hard I worked set the upper bound on how hard everyone else worked, I'd better work very hard.One day in 2010, when he was visiting California for interviews, Robert Morris did something astonishing: he offered me unsolicited advice. I can only remember him doing that once before. One day at Viaweb, when I was bent over double from a kidney stone, he suggested that it would be a good idea for him to take me to the hospital. That was what it took for Rtm to offer unsolicited advice. So I remember his exact words very clearly. \"You know,\" he said, \"you should make sure Y Combinator isn't the last cool thing you do.\"At the time I didn't understand what he meant, but gradually it dawned on me that he was saying I should quit. This seemed strange advice, because YC was doing great. But if there was one thing rarer than Rtm offering advice, it was Rtm being wrong. So this set me thinking. It was true that on my current trajectory, YC would be the last thing I did, because it was only taking up more of my attention.",
      " It had already eaten Arc, and was in the process of eating essays too. Either YC was my life's work or I'd have to leave eventually. And it wasn't, so I would.In the summer of 2012 my mother had a stroke, and the cause turned out to be a blood clot caused by colon cancer. The stroke destroyed her balance, and she was put in a nursing home, but she really wanted to get out of it and back to her house, and my sister and I were determined to help her do it. I used to fly up to Oregon to visit her regularly, and I had a lot of time to think on those flights. On one of them I realized I was ready to hand YC over to someone else.I asked Jessica if she wanted to be president, but she didn't, so we decided we'd try to recruit Sam Altman. We talked to Robert and Trevor and we agreed to make it a complete changing of the guard. Up till that point YC had been controlled by the original LLC we four had started. But we wanted YC to last for a long time, and to do that it couldn't be controlled by the founders. So if Sam said yes, we'd let him reorganize YC.",
      " Robert and I would retire, and Jessica and Trevor would become ordinary partners.When we asked Sam if he wanted to be president of YC, initially he said no. He wanted to start a startup to make nuclear reactors. But I kept at it, and in October 2013 he finally agreed. We decided he'd take over starting with the winter 2014 batch. For the rest of 2013 I left running YC more and more to Sam, partly so he could learn the job, and partly because I was focused on my mother, whose cancer had returned.She died on January 15, 2014. We knew this was coming, but it was still hard when it did.I kept working on YC till March, to help get that batch of startups through Demo Day, then I checked out pretty completely. (I still talk to alumni and to new startups working on things I'm interested in, but that only takes a few hours a week.)What should I do next? Rtm's advice hadn't included anything about that. I wanted to do something completely different, so I decided I'd paint. I wanted to see how good I could get if I really focused on it. So the day after I stopped working on YC, I started painting.",
      " I was rusty and it took a while to get back into shape, but it was at least completely engaging. [18]I spent most of the rest of 2014 painting. I'd never been able to work so uninterruptedly before, and I got to be better than I had been. Not good enough, but better. Then in November, right in the middle of a painting, I ran out of steam. Up till that point I'd always been curious to see how the painting I was working on would turn out, but suddenly finishing this one seemed like a chore. So I stopped working on it and cleaned my brushes and haven't painted since. So far anyway.I realize that sounds rather wimpy. But attention is a zero sum game. If you can choose what to work on, and you choose a project that's not the best one (or at least a good one) for you, then it's getting in the way of another project that is. And at 50 there was some opportunity cost to screwing around.I started writing essays again, and wrote a bunch of new ones over the next few months. I even wrote a couple that  weren't about startups. Then in March 2015 I started working on Lisp again.The distinctive thing about Lisp is that its core is a language defined by writing an interpreter in itself.",
      " It wasn't originally intended as a programming language in the ordinary sense. It was meant to be a formal model of computation, an alternative to the Turing machine. If you want to write an interpreter for a language in itself, what's the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question. [19]McCarthy didn't realize this Lisp could even be used to program computers till his grad student Steve Russell suggested it. Russell translated McCarthy's interpreter into IBM 704 machine language, and from that point Lisp started also to be a programming language in the ordinary sense. But its origins as a model of computation gave it a power and elegance that other languages couldn't match. It was this that attracted me in college, though I didn't understand why at the time.McCarthy's 1960 Lisp did nothing more than interpret Lisp expressions. It was missing a lot of things you'd want in a programming language. So these had to be added, and when they were, they weren't defined using McCarthy's original axiomatic approach. That wouldn't have been feasible at the time. McCarthy tested his interpreter by hand-simulating the execution of programs. But it was already getting close to the limit of interpreters you could test that way \u0097 indeed,",
      " there was a bug in it that McCarthy had overlooked. To test a more complicated interpreter, you'd have had to run it, and computers then weren't powerful enough.Now they are, though. Now you could continue using McCarthy's axiomatic approach till you'd defined a complete programming language. And as long as every change you made to McCarthy's Lisp was a discoveredness-preserving transformation, you could, in principle, end up with a complete language that had this quality. Harder to do than to talk about, of course, but if it was possible in principle, why not try? So I decided to take a shot at it. It took 4 years, from March 26, 2015 to October 12, 2019. It was fortunate that I had a precisely defined goal, or it would have been hard to keep at it for so long.I wrote this new Lisp, called Bel,  in itself in Arc. That may sound like a contradiction, but it's an indication of the sort of trickery I had to engage in to make this work. By means of an egregious collection of hacks I managed to make something close enough to an interpreter written in itself that could actually run. Not fast, but fast enough to test.I had to ban myself from writing essays during most of this time,",
      " or I'd never have finished. In late 2015 I spent 3 months writing essays, and when I went back to working on Bel I could barely understand the code. Not so much because it was badly written as because the problem is so convoluted. When you're working on an interpreter written in itself, it's hard to keep track of what's happening at what level, and errors can be practically encrypted by the time you get them.So I said no more essays till Bel was done. But I told few people about Bel while I was working on it. So for years it must have seemed that I was doing nothing, when in fact I was working harder than I'd ever worked on anything. Occasionally after wrestling for hours with some gruesome bug I'd check Twitter or HN and see someone asking \"Does Paul Graham still code?\"Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt.",
      " The good news is that I had more moments like this over the next few years.In the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.In the fall of 2019, Bel was finally finished. Like McCarthy's original Lisp, it's a spec rather than an implementation, although like McCarthy's Lisp it's a spec expressed as code.Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read,",
      " and this is the last sentence of it. Notes[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It's the everyday words that differ. So if you string together a lot of abstract concepts with a few simple verbs, you can make a little Italian go a long way.[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight down the spine of old Florence: past the Pitti, across the bridge, past Orsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli to Piazza San Marco. I saw Florence at street level in every possible condition, from empty dark winter evenings to sweltering summer days when the streets were packed with tourists.[4] You can of course paint people like still lives if you want to, and they're willing. That sort of portrait is arguably the apex of still life painting, though the long sitting does tend to produce pained expressions in the sitters.[5]",
      " Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly coupled. Anything expensive comes to be seen as cool, and anything seen as cool will soon become equally expensive.[7] Technically the apartment wasn't rent-controlled but rent-stabilized, but this is a refinement only New Yorkers would know or care about. The point is that it was really cheap, less than half market price.[8] Most software you can launch as soon as it's done. But when the software is an online store builder and you're hosting the stores, if you don't have any users yet, that fact will be painfully obvious. So before we could launch publicly we had to launch privately, in the sense of recruiting an initial set of users and making sure they had decent-looking stores.[9] We'd had a code editor in Viaweb for users to define their own page styles. They didn't know it,",
      " but they were editing Lisp expressions underneath. But this wasn't an app editor, because the code ran when the merchants' sites were generated, not when shoppers visited them.[10] This was the first instance of what is now a familiar experience, and so was what happened next, when I read the comments and found they were full of angry people. How could I claim that Lisp was better than other languages? Weren't they all Turing complete? People who see the responses to essays I write sometimes tell me how sorry they feel for me, but I'm not exaggerating when I reply that it has always been like this, since the very beginning. It comes with the territory. An essay must tell readers things they don't already know, and some  people dislike being told such things.[11] People put plenty of stuff on the internet in the 90s of course, but putting something online is not the same as publishing it online. Publishing online means you treat the online version as the (or at least a) primary version.[12] There is a general lesson here that our experience with Y Combinator also teaches: Customs continue to constrain you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs about publishing essays,",
      " been based on real constraints. Startups had once been much more expensive to start, and proportionally rare. Now they could be cheap and common, but the VCs' customs still reflected the old world, just as customs about writing essays still reflected the constraints of the print era.Which in turn implies that people who are independent-minded (i.e. less influenced by custom) will have an advantage in fields affected by rapid change (where customs are more likely to be obsolete).Here's an interesting point, though: you can't always predict which fields will be affected by rapid change. Obviously software and venture capital will be, but who would have predicted that essay writing would be?[13] Y Combinator was not the original name. At first we were called Cambridge Seed. But we didn't want a regional name, in case someone copied us in Silicon Valley, so we renamed ourselves after one of the coolest tricks in the lambda calculus, the Y combinator.I picked orange as our color partly because it's the warmest, and partly because no VC used it. In 2005 all the VCs used staid colors like maroon, navy blue, and forest green, because they were trying to appeal to LPs, not founders. The YC logo itself is an inside joke:",
      " the Viaweb logo had been a white V on a red circle, so I made the YC logo a white Y on an orange square.[14] YC did become a fund for a couple years starting in 2009, because it was getting so big I could no longer afford to fund it personally. But after Heroku got bought we had enough money to go back to being self-funded.[15] I've never liked the term \"deal flow,\" because it implies that the number of new startups at any given time is fixed. This is not only false, but it's the purpose of YC to falsify it, by causing startups to be founded that would not otherwise have existed.[16] She reports that they were all different shapes and sizes, because there was a run on air conditioners and she had to get whatever she could, but that they were all heavier than she could carry now.[17] Another problem with HN was a bizarre edge case that occurs when you both write essays and run a forum. When you run a forum, you're assumed to see if not every conversation, at least every conversation involving you. And when you write essays, people post highly imaginative misinterpretations of them on forums. Individually these two phenomena are tedious but bearable,",
      " but the combination is disastrous. You actually have to respond to the misinterpretations, because the assumption that you're present in the conversation means that not responding to any sufficiently upvoted misinterpretation reads as a tacit admission that it's correct. But that in turn encourages more; anyone who wants to pick a fight with you senses that now is their chance.[18] The worst thing about leaving YC was not working with Jessica anymore. We'd been working on YC almost the whole time we'd known each other, and we'd neither tried nor wanted to separate it from our personal lives, so leaving was like pulling up a deeply rooted tree.[19] One way to get more precise about the concept of invented vs discovered is to talk about space aliens. Any sufficiently advanced alien civilization would certainly know about the Pythagorean theorem, for example. I believe, though with less certainty, that they would also know about the Lisp in McCarthy's 1960 paper.But if so there's no reason to suppose that this is the limit of the language that might be known to them. Presumably aliens need numbers and errors and I/O too. So it seems likely there exists at least one path out of McCarthy's Lisp along which discoveredness is preserved.Thanks to Trevor Blackwell,",
      " John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Jessica Livingston, Robert Morris, and Harj Taggar for reading drafts of this.May 2001  (I wrote this article to help myself understand exactly what McCarthy discovered.  You don't need to know this stuff to program in Lisp, but it should be helpful to  anyone who wants to understand the essence of Lisp \u0097 both in the sense of its origins and its semantic core.  The fact that it has such a core is one of Lisp's distinguishing features, and the reason why, unlike other languages, Lisp has dialects.)In 1960, John  McCarthy published a remarkable paper in which he did for programming something like what Euclid did for geometry. He showed how, given a handful of simple operators and a notation for functions, you can build a whole programming language. He called this language Lisp, for \"List Processing,\" because one of his key ideas was to use a simple data structure called a list for both code and data.It's worth understanding what McCarthy discovered, not just as a landmark in the history of computers, but as a model for what programming is tending to become in our own time.  It seems to me that there have been two really clean,",
      " consistent models of programming so far: the C model and the Lisp model. These two seem points of high ground, with swampy lowlands between them.  As computers have grown more powerful, the new languages being developed have been moving steadily toward the Lisp model.  A popular recipe for new programming languages in the past 20 years  has been to take the C model of computing and add to it, piecemeal, parts taken from the Lisp model, like runtime typing and garbage collection.In this article I'm going to try to explain in the simplest possible terms what McCarthy discovered. The point is not just to learn about an interesting theoretical result someone figured out forty years ago, but to show where languages are heading. The unusual thing about Lisp \u0097 in fact, the defining quality of Lisp \u0097 is that it can be written in itself.  To understand what McCarthy meant by this, we're going to retrace his steps, with his mathematical notation translated into running Common Lisp code.May 2006(This essay is derived from a keynote at Xtech.)Could you reproduce Silicon Valley elsewhere, or is there something unique about it?It wouldn't be surprising if it were hard to reproduce in other countries, because you couldn't reproduce it in most of the US either.",
      "  What does it take to make a silicon valley even here?What it takes is the right people.  If you could get the right ten thousand people to move from Silicon Valley to Buffalo, Buffalo would become Silicon Valley.   [1]That's a striking departure from the past.  Up till a couple decades ago, geography was destiny for cities.  All great cities were located on waterways, because cities made money by trade, and water was the only economical way to ship.Now you could make a great city anywhere, if you could get the right people to move there.  So the question of how to make a silicon valley becomes: who are the right people, and how do you get them to move?Two TypesI think you only need two kinds of people to create a technology hub: rich people and nerds.  They're the limiting reagents in the reaction that produces startups, because they're the only ones present when startups get started.  Everyone else will move.Observation bears this out: within the US, towns have become startup hubs if and only if they have both rich people and nerds.  Few startups happen in Miami, for example, because although it's full of rich people, it has few nerds.  It's not the kind of place nerds like.Whereas Pittsburgh has the opposite problem:",
      " plenty of nerds, but no rich people.  The top US Computer Science departments are said to be MIT, Stanford, Berkeley, and Carnegie-Mellon.  MIT yielded Route 128.  Stanford and Berkeley yielded Silicon Valley.  But Carnegie-Mellon?  The record skips at that point.  Lower down the list, the University of Washington yielded a high-tech community in Seattle, and the University of Texas at Austin yielded one in Austin.  But what happened in Pittsburgh?  And in Ithaca, home of Cornell, which is also high on the list?I grew up in Pittsburgh and went to college at Cornell, so I can answer for both.  The weather is terrible,  particularly in winter, and there's no interesting old city to make up for it, as there is in Boston.  Rich people don't want to live in Pittsburgh or Ithaca. So while there are plenty of hackers who could start startups, there's no one to invest in them.Not BureaucratsDo you really need the rich people?  Wouldn't it work to have the government invest in the nerds?  No, it would not.  Startup investors are a distinct type of rich people.  They tend to have a lot of experience themselves in the technology business.",
      "  This (a) helps them pick the right startups, and (b) means they can supply advice and connections as well as money.  And the fact that they have a personal stake in the outcome makes them really pay attention.Bureaucrats by their nature are the exact opposite sort of people from startup investors. The idea of them making startup investments is comic.  It would be like mathematicians running Vogue-- or perhaps more accurately, Vogue editors running a math journal. [2]Though indeed, most things bureaucrats do, they do badly.   We just don't notice usually, because they only have to compete against other bureaucrats.  But as startup investors they'd have to compete against pros with a great deal more experience and motivation.Even corporations that have in-house VC groups generally forbid them to make their own investment decisions.  Most are only allowed to invest in deals where some reputable private VC firm is willing to act as lead investor.Not BuildingsIf you go to see Silicon Valley, what you'll see are buildings. But it's the people that make it Silicon Valley, not the buildings. I read occasionally about attempts to set up \"technology parks\" in other places, as if the active ingredient of Silicon Valley were the office space.  An article about Sophia Antipolis bragged that companies there included Cisco,",
      " Compaq, IBM, NCR, and Nortel.  Don't the French realize these aren't startups?Building office buildings for technology companies won't get you a silicon valley, because the key stage in the life of a startup happens before they want that kind of space.  The key stage is when they're three guys operating out of an apartment.  Wherever the startup is when it gets funded, it will stay.  The defining quality of Silicon Valley is not that Intel or Apple or Google have offices there, but that they were started there.So if you want to reproduce Silicon Valley, what you need to reproduce is those two or three founders sitting around a kitchen table deciding to start a company.  And to reproduce that you need those people.UniversitiesThe exciting thing is, all you need are the people.  If you could attract a critical mass of nerds and investors to live somewhere, you could reproduce Silicon Valley.  And both groups are highly mobile.  They'll go where life is good.  So what makes a place good to them?What nerds like is other nerds.  Smart people will go wherever other smart people are.  And in particular, to great universities.  In theory there could be other ways to attract them,",
      " but so far universities seem to be indispensable.  Within the US, there are no technology hubs without first-rate universities-- or at least, first-rate computer science departments.So if you want to make a silicon valley, you not only need a university, but one of the top handful in the world.  It has to be good enough to act as a magnet, drawing the best people from thousands of miles away.  And that means it has to stand up to existing magnets like MIT and Stanford.This sounds hard.  Actually it might be easy.  My professor friends, when they're deciding where they'd like to work, consider one thing above all: the quality of the other faculty.  What attracts professors is good colleagues.  So if you managed to recruit, en masse, a significant number of the best young researchers, you could create a first-rate university from nothing overnight.  And you could do that for surprisingly little.  If you paid 200 people hiring bonuses of $3 million apiece, you could put together a faculty that would bear comparison with any in the world.  And from that point the chain reaction would be self-sustaining.  So whatever it costs to establish a mediocre university, for an additional half billion or so you could have a great one.",
      "   [3]PersonalityHowever, merely creating a new university would not be enough to start a silicon valley. The university is just the seed.  It has to be planted in the right soil, or it won't germinate.  Plant it in the wrong place, and you just create Carnegie-Mellon.To spawn startups, your university has to be in a town that has attractions other than the university.  It has to be a place where investors want to live, and students want to stay after they graduate.The two like much the same things, because most startup investors are nerds themselves.  So what do nerds look for in a town?  Their tastes aren't completely different from other people's, because a lot of the towns they like most in the US are also big tourist destinations: San Francisco, Boston, Seattle.   But their tastes can't be quite mainstream either, because they dislike other big tourist destinations, like New York, Los Angeles, and Las Vegas.There has been a lot written lately about the \"creative class.\" The thesis seems to be that as wealth derives increasingly from ideas, cities will prosper only if they attract those who have them.  That is certainly true; in fact it was the basis of Amsterdam's prosperity 400 years ago.A lot of nerd tastes they share with the creative class in general.",
      " For example, they like well-preserved old neighborhoods instead of cookie-cutter suburbs, and locally-owned shops and restaurants instead of national chains.  Like the rest of the creative class, they want to live somewhere with personality.What exactly is personality?  I think it's the feeling that each building is the work of a distinct group of people.  A town with personality is one that doesn't feel mass-produced.  So if you want to make a startup hub-- or any town to attract the \"creative class\"-- you probably have to ban large development projects. When a large tract has been developed by a single organization, you can always tell.  [4]Most towns with personality are old, but they don't have to be. Old towns have two advantages: they're denser, because they were laid out before cars, and they're more varied, because they were built one building at a time.  You could have both now.  Just have building codes that ensure density, and ban large scale developments.A corollary is that you have to keep out the biggest developer of all: the government.  A government that asks \"How can we build a silicon valley?\" has probably ensured failure by the way they framed the question.  You don't build a silicon valley;",
      " you let one grow.NerdsIf you want to attract nerds, you need more than a town with personality.  You need a town with the right personality.  Nerds are a distinct subset of the creative class, with different tastes from the rest.  You can see this most clearly in New York, which attracts a lot of creative people, but few nerds.  [5]What nerds like is the kind of town where people walk around smiling. This excludes LA, where no one walks at all, and also New York, where people walk, but not smiling. When I was in grad school in Boston, a friend came to visit from New York.  On the subway back from the airport she asked \"Why is everyone smiling?\"  I looked and they weren't smiling.  They just looked like they were compared to the facial expressions she was used to.If you've lived in New York, you know where these facial expressions come from.  It's the kind of place where your mind may be excited, but your body knows it's having a bad time.  People don't so much enjoy living there as endure it for the sake of the excitement. And if you like certain kinds of excitement, New York is incomparable. It's a hub of glamour,",
      " a magnet for all the shorter half-life isotopes of style and fame.Nerds don't care about glamour, so to them the appeal of New York is a mystery.  People who like New York will pay a fortune for a small, dark, noisy apartment in order to live in a town where the cool people are really cool.  A nerd looks at that deal and sees only: pay a fortune for a small, dark, noisy apartment.Nerds will pay a premium to live in a town where the smart people are really smart, but you don't have to pay as much for that.  It's supply and demand: glamour is popular, so you have to pay a lot for it.Most nerds like quieter pleasures.  They like cafes instead of clubs; used bookshops instead of fashionable clothing shops; hiking instead of dancing; sunlight instead of tall buildings.  A nerd's idea of paradise is Berkeley or Boulder.YouthIt's the young nerds who start startups, so it's those specifically the city has to appeal to.  The startup hubs in the US are all young-feeling towns.  This doesn't mean they have to be new. Cambridge has the oldest town plan in America, but it feels young because it's full of students.What you can't have,",
      " if you want to create a silicon valley, is a large, existing population of stodgy people.  It would be a waste of time to try to reverse the fortunes of a declining industrial town like Detroit or Philadelphia by trying to encourage startups.  Those places have too much momentum in the wrong direction.  You're better off starting with a blank slate in the form of a small town.  Or better still, if there's a town young people already flock to, that one.The Bay Area was a magnet for the young and optimistic for decades before it was associated with technology.  It was a place people went in search of something new.  And so it became synonymous with California nuttiness.  There's still a lot of that there.  If you wanted to start a new fad-- a new way to focus one's \"energy,\" for example, or a new category of things not to eat-- the Bay Area would be the place to do it.  But a place that tolerates oddness in the search for the new is exactly what you want in a startup hub, because economically that's what startups are.  Most good startup ideas seem a little crazy; if they were obviously good ideas, someone would have done them already.(How many people are going to want computers in their houses?",
      " What, another search engine?)That's the connection between technology and liberalism.  Without exception the high-tech cities in the US are also the most liberal. But it's not because liberals are smarter that this is so.  It's because liberal cities tolerate odd ideas, and smart people by definition have odd ideas.Conversely, a town that gets praised for being \"solid\" or representing \"traditional values\" may be a fine place to live, but it's never going to succeed as a startup hub.  The 2004 presidential election, though a disaster in other respects, conveniently supplied us with a county-by-county  map of such places.   [6]To attract the young, a town must have an intact center.  In most American cities the center has been abandoned, and the growth, if any, is in the suburbs.  Most American cities have been turned inside out. But none of the startup hubs has: not San Francisco, or Boston, or Seattle.  They all have intact centers. [7] My guess is that no city with a dead center could be turned into a startup hub.  Young people don't want to live in the suburbs.Within the US, the two cities I think could most easily be turned into new silicon valleys are Boulder and Portland.",
      "  Both have the kind of effervescent feel that attracts the young.  They're each only a great university short of becoming a silicon valley, if they wanted to.TimeA great university near an attractive town.  Is that all it takes? That was all it took to make the original Silicon Valley.  Silicon Valley traces its origins to William Shockley, one of the inventors of the transistor.  He did the research that won him the Nobel Prize at Bell Labs, but when he started his own company in 1956 he moved to Palo Alto to do it.   At the time that was an odd thing to do. Why did he?  Because he had grown up there and remembered how nice it was.  Now Palo Alto is suburbia, but then it was a charming college town-- a charming college town with perfect weather and San Francisco only an hour away.The companies that rule Silicon Valley now are all descended in various ways from Shockley Semiconductor.  Shockley was a difficult man, and in 1957 his top people-- \"the traitorous eight\"-- left to start a new company, Fairchild Semiconductor.  Among them were Gordon Moore and Robert Noyce, who went on to found Intel, and Eugene Kleiner, who founded the VC firm Kleiner Perkins.",
      "  Forty-two years later, Kleiner Perkins funded Google, and the partner responsible for the deal was John Doerr, who came to Silicon Valley in 1974 to work for Intel.So although a lot of the newest companies in Silicon Valley don't make anything out of silicon, there always seem to be multiple links back to Shockley.  There's a lesson here: startups beget startups. People who work for startups start their own.  People who get rich from startups fund new ones.  I suspect this kind of organic growth is the only way to produce a startup hub, because it's the only way to grow the expertise you need.That has two important implications.  The first is that you need time to grow a silicon valley.  The university you could create in a couple years, but the startup community around it has to grow organically.   The cycle time is limited by the time it takes a company to succeed, which probably averages about five years.The other implication of the organic growth hypothesis is that you can't be somewhat of a startup hub.  You either have a self-sustaining chain reaction, or not.  Observation confirms this too: cities either have a startup scene, or they don't.  There is no middle ground.  Chicago has the third largest metropolitan area in America.",
      " As source of startups it's negligible compared to Seattle, number 15.The good news is that the initial seed can be quite small.  Shockley Semiconductor, though itself not very successful, was big enough. It brought a critical mass of experts in an important new technology together in a place they liked enough to stay.CompetingOf course, a would-be silicon valley faces an obstacle the original one didn't: it has to compete with Silicon Valley.  Can that be done?  Probably.One of Silicon Valley's biggest advantages is its venture capital firms.  This was not a factor in Shockley's day, because VC funds didn't exist.  In fact, Shockley Semiconductor and Fairchild Semiconductor were not startups at all in our sense.  They were subsidiaries-- of Beckman Instruments and Fairchild Camera and Instrument respectively.  Those companies were apparently willing to establish subsidiaries wherever the experts wanted to live.Venture investors, however, prefer to fund startups within an hour's drive.  For one, they're more likely to notice startups nearby. But when they do notice startups in other towns they prefer them to move.  They don't want to have to travel to attend board meetings, and in any case the odds of succeeding are higher in a startup hub.The centralizing effect of venture firms is a double one:",
      " they cause startups to form around them, and those draw in more startups through acquisitions.  And although the first may be weakening because it's now so cheap to start some startups, the second seems as strong as ever. Three of the most admired \"Web 2.0\" companies were started outside the usual startup hubs, but two of them have already been reeled in through acquisitions.Such centralizing forces make it harder for new silicon valleys to get started.  But by no means impossible.  Ultimately power rests with the founders.  A startup with the best people will beat one with funding from famous VCs, and a startup that was sufficiently successful would never have to move.  So a town that could exert enough pull over the right people could resist and perhaps even surpass Silicon Valley.For all its power, Silicon Valley has a great weakness: the paradise Shockley found in 1956 is now one giant parking lot.  San Francisco and Berkeley are great, but they're forty miles away.  Silicon Valley proper is soul-crushing suburban sprawl.  It has fabulous weather, which makes it significantly better than the soul-crushing sprawl of most other American cities.  But a competitor that managed to avoid sprawl would have real leverage.  All a city needs is to be the kind of place the next traitorous eight look at and say \"I want to stay here,\" and that would be enough to get the chain reaction started.Notes[",
      "1] It's interesting to consider how low this number could be made.  I suspect five hundred would be enough, even if they could bring no assets with them.  Probably just thirty, if I could pick them,  would be enough to turn Buffalo into a significant startup hub.[2] Bureaucrats manage to allocate research funding moderately well, but only because (like an in-house VC fund) they outsource most of the work of selection.  A professor at a famous university who is highly regarded by his peers will get funding, pretty much regardless of the proposal.  That wouldn't work for startups, whose founders aren't sponsored by organizations, and are often unknowns.[3] You'd have to do it all at once, or at least a whole department at a time, because people would be more likely to come if they knew their friends were.  And you should probably start from scratch, rather than trying to upgrade an existing university, or much energy would be lost in friction.[4] Hypothesis: Any plan in which multiple independent buildings are gutted or demolished to be \"redeveloped\" as a single project is a net loss of personality for the city, with the exception of the conversion of buildings not previously public, like warehouses.[5]",
      " A few startups get started in New York, but less than a tenth as many per capita as in Boston, and mostly in less nerdy fields like finance and media.[6] Some blue counties are false positives (reflecting the remaining power of Democractic party machines), but there are no false negatives.  You can safely write off all the red counties.[7] Some \"urban renewal\" experts took a shot at destroying Boston's in the 1960s, leaving the area around city hall a bleak wasteland, but most neighborhoods successfully resisted them.Thanks to Chris Anderson, Trevor Blackwell, Marc Hedlund, Jessica Livingston, Robert Morris, Greg Mcadoo, Fred Wilson, and Stephen Wolfram for reading drafts of this, and to Ed Dumbill for inviting me to speak.(The second part of this talk became Why Startups Condense in America.)  Want to start a startup?  Get funded by Y Combinator.     January 2006To do something well you have to like it.   That idea is not exactly novel.  We've got it down to four words: \"Do what you love.\"  But it's not enough just to tell people that.  Doing what you love is complicated.The very idea is foreign to what most of us learn as kids.",
      "  When I was a kid, it seemed as if work and fun were opposites by definition. Life had two states: some of the time adults were making you do things, and that was called work; the rest of the time you could do what you wanted, and that was called playing.  Occasionally the things adults made you do were fun, just as, occasionally, playing wasn't\u2014for example, if you fell and hurt yourself.  But except for these few anomalous cases, work was pretty much defined as not-fun.And it did not seem to be an accident. School, it was implied, was tedious because it was preparation for grownup work.The world then was divided into two groups, grownups and kids. Grownups, like some kind of cursed race, had to work.  Kids didn't, but they did have to go to school, which was a dilute version of work meant to prepare us for the real thing.  Much as we disliked school, the grownups all agreed that grownup work was worse, and that we had it easy.Teachers in particular all seemed to believe implicitly that work was not fun.  Which is not surprising: work wasn't fun for most of them.  Why did we have to memorize state capitals instead of playing dodgeball?",
      "  For the same reason they had to watch over a bunch of kids instead of lying on a beach.  You couldn't just do what you wanted.I'm not saying we should let little kids do whatever they want. They may have to be made to work on certain things.  But if we make kids work on dull stuff, it might be wise to tell them that tediousness is not the defining quality of work, and indeed that the reason they have to work on dull stuff now is so they can work on more interesting stuff later. [1]Once, when I was about 9 or 10, my father told me I could be whatever I wanted when I grew up, so long as I enjoyed it.  I remember that precisely because it seemed so anomalous.  It was like being told to use dry water.  Whatever I thought he meant, I didn't think he meant work could literally be fun\u2014fun like playing.  It took me years to grasp that.JobsBy high school, the prospect of an actual job was on the horizon. Adults would sometimes come to speak to us about their work, or we would go to see them at work.  It was always understood that they enjoyed what they did.  In retrospect I think one may have:",
      " the private jet pilot.  But I don't think the bank manager really did.The main reason they all acted as if they enjoyed their work was presumably the upper-middle class convention that you're supposed to.  It would not merely be bad for your career to say that you despised your job, but a social faux-pas.Why is it conventional to pretend to like what you do?  The first sentence of this essay explains that.  If you have to like something to do it well, then the most successful people will all like what they do.  That's where the upper-middle class tradition comes from. Just as houses all over America are full of  chairs that are, without the owners even knowing it, nth-degree imitations of chairs designed 250 years ago for French kings, conventional attitudes about work are, without the owners even knowing it, nth-degree imitations of the attitudes of people who've done great things.What a recipe for alienation.  By the time they reach an age to think about what they'd like to do, most kids have been thoroughly misled about the idea of loving one's work.  School has trained them to regard work as an unpleasant duty.  Having a job is said to be even more onerous than schoolwork.",
      "  And yet all the adults claim to like what they do.  You can't blame kids for thinking \"I am not like these people; I am not suited to this world.\"Actually they've been told three lies: the stuff they've been taught to regard as work in school is not real work; grownup work is not (necessarily) worse than schoolwork; and many of the adults around them are lying when they say they like what they do.The most dangerous liars can be the kids' own parents.  If you take a boring job to give your family a high standard of living, as so many people do, you risk infecting your kids with the idea that work is boring.  [2] Maybe it would be better for kids in this one case if parents were not so unselfish.  A parent who set an example of loving their work might help their kids more than an expensive house. [3]It was not till I was in college that the idea of work finally broke free from the idea of making a living.  Then the important question became not how to make money, but what to work on.  Ideally these coincided, but some spectacular boundary cases (like Einstein in the patent office) proved they weren't identical.The definition of work was now to make some original contribution to the world,",
      " and in the process not to starve.  But after the habit of so many years my idea of work still included a large component of pain.  Work still seemed to require discipline, because only hard problems yielded grand results, and hard problems couldn't literally be fun.   Surely one had to force oneself to work on them.If you think something's supposed to hurt, you're less likely to notice if you're doing it wrong.  That about sums up my experience of graduate school.BoundsHow much are you supposed to like what you do?  Unless you know that, you don't know when to stop searching. And if, like most people, you underestimate it, you'll tend to stop searching too early.  You'll end up doing something chosen for you by your parents, or the desire to make money, or prestige\u2014or sheer inertia.Here's an upper bound: Do what you love doesn't mean, do what you would like to do most this second.  Even Einstein probably had moments when he wanted to have a cup of coffee, but told himself he ought to finish what he was working on first.It used to perplex me when I read about people who liked what they did so much that there was nothing they'd rather do.  There didn't seem to be any sort of work I liked that much.",
      "  If I had a choice of (a) spending the next hour working on something or (b) be teleported to Rome and spend the next hour wandering about, was there any sort of work I'd prefer?  Honestly, no.But the fact is, almost anyone would rather, at any given moment, float about in the Carribbean, or have sex, or eat some delicious food, than work on hard problems.  The rule about doing what you love assumes a certain length of time.  It doesn't mean, do what will make you happiest this second, but what will make you happiest over some longer period, like a week or a month.Unproductive pleasures pall eventually.  After a while you get tired of lying on the beach.  If you want to stay happy, you have to do something.As a lower bound, you have to like your work more than any unproductive pleasure.  You have to like what you do enough that the concept of \"spare time\" seems mistaken.  Which is not to say you have to spend all your time working.  You can only work so much before you get tired and start to screw up.  Then you want to do something else\u2014even something mindless.  But you don't regard this time as the prize and the time you spend working as the pain you endure to earn it.I put the lower bound there for practical reasons.",
      "  If your work is not your favorite thing to do, you'll have terrible problems with procrastination.  You'll have to force yourself to work,  and when you resort to that the results are distinctly inferior.To be happy I think you have to be doing something you not only enjoy, but admire.  You have to be able to say, at the end, wow, that's pretty cool.  This doesn't mean you have to make something. If you learn how to hang glide, or to speak a foreign language fluently, that will be enough to make you say, for a while at least, wow, that's pretty cool.  What there has to be is a test.So one thing that falls just short of the standard, I think, is reading books.  Except for some books in math and the hard sciences, there's no test of how well you've read a book, and that's why merely reading books doesn't quite feel like work.  You have to do something with what you've read to feel productive.I think the best test is one Gino Lee taught me: to try to do things that would make your friends say wow.  But it probably wouldn't start to work properly till about age 22, because most people haven't had a big enough sample to pick friends from before then.SirensWhat you should not do,",
      " I think, is worry about the opinion of anyone beyond your friends.  You shouldn't worry about prestige. Prestige is the opinion of the rest of the world.  When you can ask the opinions of people whose judgement you respect, what does it add to consider the opinions of people you don't even know?  [4]This is easy advice to give.  It's hard to follow, especially when you're young.   [5] Prestige is like a powerful magnet that warps even your beliefs about what you enjoy.  It causes you to work not on what you like, but what you'd like to like.That's what leads people to try to write novels, for example.  They like reading novels.  They notice that people who write them win Nobel prizes.  What could be more wonderful, they think, than to be a novelist?  But liking the idea of being a novelist is not enough; you have to like the actual work of novel-writing if you're going to be good at it; you have to like making up elaborate lies.Prestige is just fossilized inspiration.  If you do anything well enough, you'll make it prestigious.  Plenty of things we now consider prestigious were anything but at first.  Jazz comes to mind\u2014though almost any established art form would do.",
      "   So just do what you like, and let prestige take care of itself.Prestige is especially dangerous to the ambitious.  If you want to make ambitious people waste their time on errands, the way to do it is to bait the hook with prestige.  That's the recipe for getting people to give talks, write forewords, serve on committees, be department heads, and so on.  It might be a good rule simply to avoid any prestigious task. If it didn't suck, they wouldn't have had to make it prestigious.Similarly, if you admire two kinds of work equally, but one is more prestigious, you should probably choose the other.  Your opinions about what's admirable are always going to be slightly influenced by prestige, so if the two seem equal to you, you probably have more genuine admiration for the less prestigious one.The other big force leading people astray is money.  Money by itself is not that dangerous.  When something pays well but is regarded with contempt, like telemarketing, or prostitution, or personal injury litigation, ambitious people aren't tempted by it.  That kind of work ends up being done by people who are \"just trying to make a living.\"  (Tip: avoid any field whose practitioners say this.)  The danger is when money is combined with prestige,",
      " as in, say, corporate law, or medicine.  A comparatively safe and prosperous career with some automatic baseline prestige is dangerously tempting to someone young, who hasn't thought much about what they really like.The test of whether people love what they do is whether they'd do it even if they weren't paid for it\u2014even if they had to work at another job to make a living.  How many corporate lawyers would do their current work if they had to do it for free, in their spare time, and take day jobs as waiters to support themselves?This test is especially helpful in deciding between different kinds of academic work, because fields vary greatly in this respect.  Most good mathematicians would work on math even if there were no jobs as math professors, whereas in the departments at the other end of the spectrum, the availability of teaching jobs is the driver: people would rather be English professors than work in ad agencies, and publishing papers is the way you compete for such jobs.  Math would happen without math departments, but it is the existence of English majors, and therefore jobs teaching them, that calls into being all those thousands of dreary papers about gender and identity in the novels of Conrad.  No one does  that  kind of thing for fun.The advice of parents will tend to err on the side of money.",
      "  It seems safe to say there are more undergrads who want to be novelists and whose parents want them to be doctors than who want to be doctors and whose parents want them to be novelists.  The kids think their parents are \"materialistic.\" Not necessarily.  All parents tend to be more conservative for their kids than they would for themselves, simply because, as parents, they share risks more than rewards.  If your eight year old son decides to climb a tall tree, or your teenage daughter decides to date the local bad boy, you won't get a share in the excitement, but if your son falls, or your daughter gets pregnant, you'll have to deal with the consequences.DisciplineWith such powerful forces leading us astray, it's not surprising we find it so hard to discover what we like to work on.  Most people are doomed in childhood by accepting the axiom that work = pain. Those who escape this are nearly all lured onto the rocks by prestige or money.  How many even discover something they love to work on? A few hundred thousand, perhaps, out of billions.It's hard to find work you love; it must be, if so few do.  So don't underestimate this task.  And don't feel bad if you haven't succeeded yet.",
      "  In fact, if you admit to yourself that you're discontented, you're a step ahead of most people, who are still in denial.  If you're surrounded by colleagues who claim to enjoy work that you find contemptible, odds are they're lying to themselves.  Not necessarily, but probably.Although doing great work takes less discipline than people think\u2014because the way to do great work is to find something you like so much that you don't have to force yourself to do it\u2014finding work you love does usually require discipline.   Some people are lucky enough to know what they want to do when they're 12, and just glide along as if they were on railroad tracks.  But this seems the exception.  More often people who do great things have careers with the trajectory of a ping-pong ball.  They go to school to study A, drop out and get a job doing B, and then become famous for C after taking it up on the side.Sometimes jumping from one sort of work to another is a sign of energy, and sometimes it's a sign of laziness.  Are you dropping out, or boldly carving a new path?  You often can't tell yourself. Plenty of people who will later do great things seem to be disappointments early on,",
      " when they're trying to find their niche.Is there some test you can use to keep yourself honest?  One is to try to do a good job at whatever you're doing, even if you don't like it.  Then at least you'll know you're not using dissatisfaction as an excuse for being lazy.  Perhaps more importantly, you'll get into the habit of doing things well.Another test you can use is: always produce.  For example, if you have a day job you don't take seriously because you plan to be a novelist, are you producing?  Are you writing pages of fiction, however bad?  As long as you're producing, you'll know you're not merely using the hazy vision of the grand novel you plan to write one day as an opiate.  The view of it will be obstructed by the all too palpably flawed one you're actually writing.\"Always produce\" is also a heuristic for finding the work you love. If you subject yourself to that constraint, it will automatically push you away from things you think you're supposed to work on, toward things you actually like.  \"Always produce\" will discover your life's work the way water, with the aid of gravity, finds the hole in your roof.Of course,",
      " figuring out what you like to work on doesn't mean you get to work on it.  That's a separate question.  And if you're ambitious you have to keep them separate: you have to make a conscious effort to keep your ideas about what you want from being contaminated by what seems possible.  [6]It's painful to keep them apart, because it's painful to observe the gap between them. So most people pre-emptively lower their expectations.  For example, if you asked random people on the street if they'd like to be able to draw like Leonardo, you'd find most would say something like \"Oh, I can't draw.\"  This is more a statement of intention than fact; it means, I'm not going to try.  Because the fact is, if you took a random person off the street and somehow got them to work as hard as they possibly could at drawing for the next twenty years, they'd get surprisingly far.  But it would require a great moral effort; it would mean staring failure in the eye every day for years.  And so to protect themselves people say \"I can't.\"Another related line you often hear is that not everyone can do work they love\u2014that someone has to do the unpleasant jobs.",
      "  Really? How do you make them?  In the US the only mechanism for forcing people to do unpleasant jobs is the draft, and that hasn't been invoked for over 30 years.  All we can do is encourage people to do unpleasant work, with money and prestige.If there's something people still won't do, it seems as if society just has to make do without.  That's what happened with domestic servants.  For millennia that was the canonical example of a job \"someone had to do.\"  And yet in the mid twentieth century servants practically disappeared in rich countries, and the rich have just had to do without.So while there may be some things someone has to do, there's a good chance anyone saying that about any particular job is mistaken. Most unpleasant jobs would either get automated or go undone if no one were willing to do them.Two RoutesThere's another sense of \"not everyone can do work they love\" that's all too true, however.  One has to make a living, and it's hard to get paid for doing work you love.  There are two routes to that destination:    The organic route: as you become more eminent, gradually to   increase the parts of your job that you like at the expense of   those you don't.The two-job route:",
      " to work at things you don't like to get money   to work on things you do.  The organic route is more common.  It happens naturally to anyone who does good work.  A young architect has to take whatever work he can get, but if he does well he'll gradually be in a position to pick and choose among projects.  The disadvantage of this route is that it's slow and uncertain.  Even tenure is not real freedom.The two-job route has several variants depending on how long you work for money at a time.  At one extreme is the \"day job,\" where you work regular hours at one job to make money, and work on what you love in your spare time.  At the other extreme you work at something till you make enough not to  have to work for money again.The two-job route is less common than the organic route, because it requires a deliberate choice.  It's also more dangerous.  Life tends to get more expensive as you get older, so it's easy to get sucked into working longer than you expected at the money job. Worse still, anything you work on changes you.  If you work too long on tedious stuff, it will rot your brain.  And the best paying jobs are most dangerous,",
      " because they require your full attention.The advantage of the two-job route is that it lets you jump over obstacles.  The landscape of possible jobs isn't flat; there are walls of varying heights between different kinds of work.  [7] The trick of maximizing the parts of your job that you like can get you from architecture to product design, but not, probably, to music. If you make money doing one thing and then work on another, you have more freedom of choice.Which route should you take?  That depends on how sure you are of what you want to do, how good you are at taking orders, how much risk you can stand, and the odds that anyone will pay (in your lifetime) for what you want to do.  If you're sure of the general area you want to work in and it's something people are likely to pay you for, then you should probably take the organic route.  But if you don't know what you want to work on, or don't like to take orders, you may want to take the two-job route, if you can stand the risk.Don't decide too soon.  Kids who know early what they want to do seem impressive, as if they got the answer to some math question before the other kids.",
      "  They have an answer, certainly, but odds are it's wrong.A friend of mine who is a quite successful doctor complains constantly about her job.  When people applying to medical school ask her for advice, she wants to shake them and yell \"Don't do it!\"  (But she never does.) How did she get into this fix?  In high school she already wanted to be a doctor.  And she is so ambitious and determined that she overcame every obstacle along the way\u2014including, unfortunately, not liking it.Now she has a life chosen for her by a high-school kid.When you're young, you're given the impression that you'll get enough information to make each choice before you need to make it. But this is certainly not so with work.  When you're deciding what to do, you have to operate on ridiculously incomplete information. Even in college you get little idea what various types of work are like.  At best you may have a couple internships, but not all jobs offer internships, and those that do don't teach you much more about the work than being a batboy teaches you about playing baseball.In the design of lives, as in the design of most other things, you get better results if you use flexible media.  So unless you're fairly sure what you want to do,",
      " your best bet may be to choose a type of work that could turn into either an organic or two-job career.  That was probably part of the reason I chose computers. You can be a professor, or make a lot of money, or morph it into any number of other kinds of work.It's also wise, early on, to seek jobs that let you do many different things, so you can learn faster what various kinds of work are like. Conversely, the extreme version of the two-job route is dangerous because it teaches you so little about what you like.  If you work hard at being a bond trader for ten years, thinking that you'll quit and write novels when you have enough money, what happens when you quit and then discover that you don't actually like writing novels?Most people would say, I'd take that problem.  Give me a million dollars and I'll figure out what to do.  But it's harder than it looks.  Constraints give your life shape.  Remove them and most people have no idea what to do: look at what happens to those who win lotteries or inherit money.  Much as everyone thinks they want financial security, the happiest people are not those who have it, but those who like what they do.",
      "  So a plan that promises freedom at the expense of knowing what to do with it may not be as good as it seems.Whichever route you take, expect a struggle.  Finding work you love is very difficult.  Most people fail.  Even if you succeed, it's rare to be free to work on what you want till your thirties or forties.  But if you have the destination in sight you'll be more likely to arrive at it.  If you know you can love work, you're in the home stretch, and if you know what work you love, you're practically there.Notes[1] Currently we do the opposite: when we make kids do boring work, like arithmetic drills, instead of admitting frankly that it's boring, we try to disguise it with superficial decorations.[2] One father told me about a related phenomenon: he found himself concealing from his family how much he liked his work.  When he wanted to go to work on a saturday, he found it easier to say that it was because he \"had to\" for some reason, rather than admitting he preferred to work than stay home with them.[3] Something similar happens with suburbs.  Parents move to suburbs to raise their kids in a safe environment,",
      " but suburbs are so dull and artificial that by the time they're fifteen the kids are convinced the whole world is boring.[4] I'm not saying friends should be the only audience for your work.  The more people you can help, the better.  But friends should be your compass.[5] Donald Hall said young would-be poets were mistaken to be so obsessed with being published.  But you can imagine what it would do for a 24 year old to get a poem published in The New Yorker. Now to people he meets at parties he's a real poet.  Actually he's no better or worse than he was before, but to a clueless audience like that, the approval of an official authority makes all the difference.   So it's a harder problem than Hall realizes.  The reason the young care so much about prestige is that the people they want to impress are not very discerning.[6] This is isomorphic to the principle that you should prevent your beliefs about how things are from being contaminated by how you wish they were.  Most people let them mix pretty promiscuously. The continuing popularity of religion is the most visible index of that.[7] A more accurate metaphor would be to say that the graph of jobs is not very well connected.Thanks to Trevor Blackwell,",
      " Dan Friedman, Sarah Harlin, Jessica Livingston, Jackie McDonough, Robert Morris, Peter Norvig,  David Sloo, and Aaron Swartz for reading drafts of this.July 2006 When I was in high school I spent a lot of time imitating bad writers.  What we studied in English classes was mostly fiction, so I assumed that was the highest form of writing.  Mistake number one.  The stories that seemed to be most admired were ones in which people suffered in complicated ways.  Anything funny or gripping was ipso facto suspect, unless it was old enough to be hard to understand, like Shakespeare or Chaucer.  Mistake number two.  The ideal medium seemed the short story, which I've since learned had quite a brief life, roughly coincident with the peak of magazine publishing.  But since their size made them perfect for use in high school classes, we read a lot of them, which gave us the impression the short story was flourishing.  Mistake number three. And because they were so short, nothing really had to happen; you could just show a randomly truncated slice of life, and that was considered advanced.  Mistake number four.  The result was that I wrote a lot of stories in which nothing happened except that someone was unhappy in a way that seemed deep.For most of college I was a philosophy major.",
      "  I was very impressed by the papers published in philosophy journals.  They were so beautifully typeset, and their tone was just captivating\u2014alternately casual and buffer-overflowingly technical.  A fellow would be walking along a street and suddenly modality qua modality would spring upon him.  I didn't ever quite understand these papers, but I figured I'd get around to that later, when I had time to reread them more closely.  In the meantime I tried my best to imitate them.  This was, I can now see, a doomed undertaking, because they weren't really saying anything.  No philosopher ever refuted another, for example, because no one said anything definite enough to refute. Needless to say, my imitations didn't say anything either.In grad school I was still wasting time imitating the wrong things. There was then a fashionable type of program called an expert system, at the core of which was something called an inference engine.  I looked at what these things did and thought \"I could write that in a thousand lines of code.\"  And yet eminent professors were writing books about them, and startups were selling them for a year's salary a copy.  What an opportunity, I thought; these impressive things seem easy to me;",
      " I must be pretty sharp.  Wrong.  It was simply a fad.  The books the professors wrote about expert systems are now ignored.  They were not even on a path to anything interesting. And the customers paying so much for them were largely the same government agencies that paid thousands for screwdrivers and toilet seats.How do you avoid copying the wrong things?  Copy only what you genuinely like.  That would have saved me in all three cases.  I didn't enjoy the short stories we had to read in English classes; I didn't learn anything from philosophy papers; I didn't use expert systems myself.  I believed these things were good because they were admired.It can be hard to separate the things you like from the things you're impressed with.  One trick is to ignore presentation.  Whenever I see a painting impressively hung in a museum, I ask myself: how much would I pay for this if I found it at a garage sale, dirty and frameless, and with no idea who painted it?  If you walk around a museum trying this experiment, you'll find you get some truly startling results.  Don't ignore this data point just because it's an outlier.Another way to figure out what you like is to look at what you enjoy as guilty pleasures.",
      "  Many things people like, especially if they're young and ambitious, they like largely for the feeling of virtue in liking them.  99% of people reading Ulysses are thinking \"I'm reading Ulysses\" as they do it. A guilty pleasure is at least a pure one.  What do you read when you don't feel up to being virtuous?  What kind of book do you read and feel sad that there's only half of it left, instead of being impressed that you're half way through?  That's what you really like.Even when you find genuinely good things to copy, there's another pitfall to be avoided.  Be careful to copy what makes them good, rather than their flaws.  It's easy to be drawn into imitating flaws, because they're easier to see, and of course easier to copy too.  For example, most painters in the eighteenth and nineteenth centuries used brownish colors.  They were imitating the great painters of the Renaissance, whose paintings by that time were brown with dirt.  Those paintings have since been cleaned, revealing brilliant colors; their imitators are of course still brown.It was painting, incidentally, that cured me of copying the wrong things.  Halfway through grad school I decided I wanted to try being a painter,",
      " and the art world was so manifestly corrupt that it snapped the leash of credulity.  These people made philosophy professors seem as scrupulous as mathematicians.  It was so clearly a choice of doing good work xor being an insider that I was forced to see the distinction.  It's there to some degree in almost every field, but I had till then managed to avoid facing it.That was one of the most valuable things I learned from painting: you have to figure out for yourself what's  good.  You can't trust authorities. They'll lie to you on this one.  Comment on this essay.July 2010What hard liquor, cigarettes, heroin, and crack have in common is that they're all more concentrated forms of less addictive predecessors. Most if not all the things we describe as addictive are.  And the scary thing is, the process that created them is accelerating.We wouldn't want to stop it.  It's the same process that cures diseases: technological progress.  Technological progress means making things do more of what we want.  When the thing we want is something we want to want, we consider technological progress good. If some new technique makes solar cells x% more efficient, that seems strictly better.  When progress concentrates something we don't want to want\u2014when it transforms opium into heroin\u2014it seems bad.",
      "  But it's the same process at work. [1]No one doubts this process is accelerating, which means increasing numbers of things we like will be transformed into things we like too much. [2]As far as I know there's no word for something we like too much. The closest is the colloquial sense of \"addictive.\" That usage has become increasingly common during my lifetime.  And it's clear why: there are an increasing number of things we need it for.  At the extreme end of the spectrum are crack and meth.  Food has been transformed by a combination of factory farming and innovations in food processing into something with way more immediate bang for the buck, and you can see the results in any town in America.  Checkers and solitaire have been replaced by World of Warcraft and FarmVille. TV has become much more engaging, and even so it can't compete with Facebook.The world is more addictive than it was 40 years ago.   And unless the forms of technological progress that produced these things are subject to different laws than technological progress in general, the world will get more addictive in the next 40 years than it did in the last 40.The next 40 years will bring us some wonderful things.  I don't mean to imply they're all to be avoided.",
      "  Alcohol is a dangerous drug, but I'd rather live in a world with wine than one without. Most people can coexist with alcohol; but you have to be careful. More things we like will mean more things we have to be careful about.Most people won't, unfortunately.  Which means that as the world becomes more addictive, the two senses in which one can live a normal life will be driven ever further apart.  One sense of \"normal\" is statistically normal: what everyone else does.  The other is the sense we mean when we talk about the normal operating range of a piece of machinery: what works best.These two senses are already quite far apart.  Already someone trying to live well would seem eccentrically abstemious in most of the US.  That phenomenon is only going to become more pronounced. You can probably take it as a rule of thumb from now on that if people don't think you're weird, you're living badly.Societies eventually develop antibodies to addictive new things. I've seen that happen with cigarettes.  When cigarettes first appeared, they spread the way an infectious disease spreads through a previously isolated population.  Smoking rapidly became a (statistically) normal thing.  There were ashtrays everywhere.  We had ashtrays in our house when I was a kid,",
      " even though neither of my parents smoked.  You had to for guests.As knowledge spread about the dangers of smoking, customs changed. In the last 20 years, smoking has been transformed from something that seemed totally normal into a rather seedy habit: from something movie stars did in publicity shots to something small huddles of addicts do outside the doors of office buildings.  A lot of the change was due to legislation, of course, but the legislation couldn't have happened if customs hadn't already changed.It took a while though\u2014on the order of 100 years.  And unless the rate at which social antibodies evolve can increase to match the accelerating rate at which technological progress throws off new addictions, we'll be increasingly unable to rely on customs to protect us. [3] Unless we want to be canaries in the coal mine of each new addiction\u2014the people whose sad example becomes a lesson to future generations\u2014we'll have to figure out for ourselves what to avoid and how.  It will actually become a reasonable strategy (or a more reasonable strategy) to suspect  everything new.In fact, even that won't be enough.  We'll have to worry not just about new things, but also about existing things becoming more addictive.  That's what bit me.  I've avoided most addictions,",
      " but the Internet got me because it became addictive while I was using it. [4]Most people I know have problems with Internet addiction.  We're all trying to figure out our own customs for getting free of it. That's why I don't have an iPhone, for example; the last thing I want is for the Internet to follow me out into the world. [5] My latest trick is taking long hikes.  I used to think running was a better form of exercise than hiking because it took less time.  Now the slowness of hiking seems an advantage, because the longer I spend on the trail, the longer I have to think without interruption.Sounds pretty eccentric, doesn't it?  It always will when you're trying to solve problems where there are no customs yet to guide you.  Maybe I can't plead Occam's razor; maybe I'm simply eccentric. But if I'm right about the acceleration of addictiveness, then this kind of lonely squirming to avoid it will increasingly be the fate of anyone who wants to get things done.  We'll increasingly be defined by what we say no to. Notes[1] Could you restrict technological progress to areas where you wanted it?  Only in a limited way, without becoming a police state.",
      " And even then your restrictions would have undesirable side effects. \"Good\" and \"bad\" technological progress aren't sharply differentiated, so you'd find you couldn't slow the latter without also slowing the former.  And in any case, as Prohibition and the \"war on drugs\" show, bans often do more harm than good.[2] Technology has always been accelerating.  By Paleolithic standards, technology evolved at a blistering pace in the Neolithic period.[3] Unless we mass produce social customs.  I suspect the recent resurgence of evangelical Christianity in the US is partly a reaction to drugs.  In desperation people reach for the sledgehammer; if their kids won't listen to them, maybe they'll listen to God.  But that solution has broader consequences than just getting kids to say no to drugs.  You end up saying no to  science as well. I worry we may be heading for a future in which only a few people plot their own itinerary through no-land, while everyone else books a package tour.  Or worse still, has one booked for them by the government.[4] People commonly use the word \"procrastination\" to describe what they do on the Internet.  It seems to me too mild to describe what's happening as merely not-doing-work.",
      "  We don't call it procrastination when someone gets drunk instead of working.[5] Several people have told me they like the iPad because it lets them bring the Internet into situations where a laptop would be too conspicuous.  In other words, it's a hip flask.  (This is true of the iPhone too, of course, but this advantage isn't as obvious because it reads as a phone, and everyone's used to those.)Thanks to Sam Altman, Patrick Collison, Jessica Livingston, and Robert Morris for reading drafts of this.May 2007People who worry about the increasing gap between rich and poor generally look back on the mid twentieth century as a golden age. In those days we had a large number of high-paying union manufacturing jobs that boosted the median income.  I wouldn't quite call the high-paying union job a myth, but I think people who dwell on it are reading too much into it.Oddly enough, it was working with startups that made me realize where the high-paying union job came from.  In a rapidly growing market, you don't worry too much about efficiency.  It's more important to grow fast.  If there's some mundane problem getting in your way, and there's a simple solution that's somewhat expensive,",
      " just take it and get on with more important things.  EBay didn't win by paying less for servers than their competitors.Difficult though it may be to imagine now, manufacturing was a growth industry in the mid twentieth century.  This was an era when small firms making everything from cars to candy were getting consolidated into a new kind of corporation with national reach and huge economies of scale.  You had to grow fast or die.  Workers were for these companies what servers are for an Internet startup. A reliable supply was more important than low cost.If you looked in the head of a 1950s auto executive, the attitude must have been: sure, give 'em whatever they ask for, so long as the new model isn't delayed.In other words, those workers were not paid what their work was worth.  Circumstances being what they were, companies would have been stupid to insist on paying them so little.If you want a less controversial example of this phenomenon, ask anyone who worked as a consultant building web sites during the Internet Bubble.  In the late nineties you could get paid huge sums of money for building the most trivial things.  And yet does anyone who was there have any expectation those days will ever return?  I doubt it.  Surely everyone realizes that was just a temporary aberration.The era of labor unions seems to have been the same kind of aberration,",
      "  just spread over a longer period, and mixed together with a lot of ideology that prevents people from viewing it with as cold an eye as they would something like consulting during the Bubble.Basically, unions were just Razorfish.People who think the labor movement was the creation of heroic union organizers have a problem to explain: why are unions shrinking now? The best they can do is fall back on the default explanation of people living in fallen civilizations.  Our ancestors were giants. The workers of the early twentieth century must have had a moral courage that's lacking today.In fact there's a simpler explanation.  The early twentieth century was just a fast-growing startup overpaying for infrastructure.  And we in the present are not a fallen people, who have abandoned whatever mysterious high-minded principles produced the high-paying union job.  We simply live in a time when the fast-growing companies overspend on different things.May 2021Noora Health, a nonprofit I've  supported for years, just launched a new NFT. It has a dramatic name, Save Thousands of Lives, because that's what the proceeds will do.Noora has been saving lives for 7 years. They run programs in hospitals in South Asia to teach new mothers how to take care of their babies once they get home.",
      " They're in 165 hospitals now. And because they know the numbers before and after they start at a new hospital, they can measure the impact they have. It is massive. For every 1000 live births, they save 9 babies.This number comes from a study of 133,733 families at 28 different hospitals that Noora conducted in collaboration with the Better Birth team at Ariadne Labs, a joint center for health systems innovation at Brigham and Women\u0092s Hospital and Harvard T.H. Chan School of Public Health.Noora is so effective that even if you measure their costs in the most conservative way, by dividing their entire budget by the number of lives saved, the cost of saving a life is the lowest I've seen. $1,235.For this NFT, they're going to issue a public report tracking how this specific tranche of money is spent, and estimating the number of lives saved as a result.NFTs are a new territory, and this way of using them is especially new, but I'm excited about its potential. And I'm excited to see what happens with this particular auction, because unlike an NFT representing something that has already happened, this NFT gets better as the price gets higher.The reserve price was about $2.",
      "5 million, because that's what it takes for the name to be accurate: that's what it costs to save 2000 lives. But the higher the price of this NFT goes, the more lives will be saved. What a sentence to be able to write.October 2015This will come as a surprise to a lot of people, but in some cases it's possible to detect bias in a selection process without knowing anything about the applicant pool.  Which is exciting because among other things it means third parties can use this technique to detect bias whether those doing the selecting want them to or not.You can use this technique whenever (a) you have at least a random sample of the applicants that were selected, (b) their subsequent performance is measured, and (c) the groups of applicants you're comparing have roughly equal distribution of ability.How does it work?  Think about what it means to be biased.  What it means for a selection process to be biased against applicants of type x is that it's harder for them to make it through.  Which means applicants of type x have to be better to get selected than applicants not of type x. [1] Which means applicants of type x who do make it through the selection process will outperform other successful applicants.",
      "  And if the performance of all the successful applicants is measured, you'll know if they do.Of course, the test you use to measure performance must be a valid one.  And in particular it must not be invalidated by the bias you're trying to measure. But there are some domains where performance can be measured, and in those detecting bias is straightforward. Want to know if the selection process was biased against some type of applicant?  Check whether they outperform the others.  This is not just a heuristic for detecting bias.  It's what bias means.For example, many suspect that venture capital firms are biased against female founders. This would be easy to detect: among their portfolio companies, do startups with female founders outperform those without?  A couple months ago, one VC firm (almost certainly unintentionally) published a study showing bias of this type. First Round Capital found that among its portfolio companies, startups with female founders outperformed those without by 63%.  [2]The reason I began by saying that this technique would come as a surprise to many people is that we so rarely see analyses of this type.  I'm sure it will come as a surprise to First Round that they performed one. I doubt anyone there realized that by limiting their sample to their own portfolio,",
      " they were producing a study not of startup trends but of their own biases when selecting companies.I predict we'll see this technique used more in the future.  The information needed to conduct such studies is increasingly available. Data about who applies for things is usually closely guarded by the organizations selecting them, but nowadays data about who gets selected is often publicly available to anyone who takes the trouble to aggregate it. Notes[1] This technique wouldn't work if the selection process looked for different things from different types of applicants\u2014for example, if an employer hired men based on their ability but women based on their appearance.[2] As Paul Buchheit points out, First Round excluded their most  successful investment, Uber, from the study.  And while it  makes sense to exclude outliers from some types of studies,  studies of returns from startup investing, which is all about  hitting outliers, are not one of them. Thanks to Sam Altman, Jessica Livingston, and Geoff Ralston for reading drafts of this.  Want to start a startup?  Get funded by Y Combinator.     October 2014(This essay is derived from a guest lecture in Sam Altman's startup class at Stanford.  It's intended for college students, but much of it is applicable to potential founders at other ages.)One of the advantages of having kids is that when you have to give advice,",
      " you can ask yourself \"what would I tell my own kids?\"  My kids are little, but I can imagine what I'd tell them about startups if they were in college, and that's what I'm going to tell you.Startups are very counterintuitive The best way to improve your vocabulary is by reading extensively and using new words in conversation..  I'm not sure why.  Maybe it's just because knowledge about them hasn't permeated our culture yet. But whatever the reason, starting a startup is a task where you can't always trust your instincts.It's like skiing in that way.  When you first try skiing and you want to slow down, your instinct is to lean back.  But if you lean back on skis you fly down the hill out of control.  So part of learning to ski is learning to suppress that impulse.  Eventually you get new habits, but at first it takes a conscious effort.  At first there's a list of things you're trying to remember as you start down the hill.Startups are as unnatural as skiing, so there's a similar list for startups. Here I'm going to give you the first part of it \u2014 the things to remember if you want to prepare yourself to start a startup. CounterintuitiveThe first item on it is the fact I already mentioned: that startups are so weird that if you trust your instincts,",
      " you'll make a lot of mistakes.  If you know nothing more than this, you may at least pause before making them.When I was running Y Combinator I used to joke that our function was to tell founders things they would ignore.  It's really true. Batch after batch, the YC partners warn founders about mistakes they're about to make, and the founders ignore them, and then come back a year later and say \"I wish we'd listened.\"Why do the founders ignore the partners' advice?  Well, that's the thing about counterintuitive ideas: they contradict your intuitions. They seem wrong.  So of course your first impulse is to disregard them.  And in fact my joking description is not merely the curse of Y Combinator but part of its raison d'etre. If founders' instincts already gave them the right answers, they wouldn't need us.  You only need other people to give you advice that surprises you. That's why there are a lot of ski instructors and not many running instructors. [1]You can, however, trust your instincts about people.  And in fact one of the most common mistakes young founders make is not to do that enough.  They get involved with people who seem impressive,",
      " but about whom they feel some misgivings personally.  Later when things blow up they say \"I knew there was something off about him, but I ignored it because he seemed so impressive.\"If you're thinking about getting involved with someone \u2014 as a cofounder, an employee, an investor, or an acquirer \u2014 and you have misgivings about them, trust your gut.  If someone seems slippery, or bogus, or a jerk, don't ignore it.This is one case where it pays to be self-indulgent. Work with people you genuinely like, and you've known long enough to be sure. ExpertiseThe second counterintuitive point is that it's not that important to know a lot about startups.  The way to succeed in a startup is not to be an expert on startups, but to be an expert on your users and the problem you're solving for them. Mark Zuckerberg didn't succeed because he was an expert on startups. He succeeded despite being a complete noob at startups, because he understood his users really well.If you don't know anything about, say, how to raise an angel round, don't feel bad on that account.  That sort of thing you can learn when you need to, and forget after you've done it.In fact,",
      " I worry it's not merely unnecessary to learn in great detail about the mechanics of startups, but possibly somewhat dangerous.  If I met an undergrad who knew all about convertible notes and employee agreements and (God forbid) class FF stock, I wouldn't think \"here is someone who is way ahead of their peers.\" It would set off alarms.  Because another of the characteristic mistakes of young founders is to go through the motions of starting a startup.  They make up some plausible-sounding idea, raise money at a good valuation, rent a cool office, hire a bunch of people. From the outside that seems like what startups do.  But the next step after rent a cool office and hire a bunch of people is: gradually realize how completely fucked they are, because while imitating all the outward forms of a startup they have neglected the one thing that's actually essential: making something people want. GameWe saw this happen so often that we made up a name for it: playing house.  Eventually I realized why it was happening.  The reason young founders go through the motions of starting a startup is because that's what they've been trained to do for their whole lives up to that point.  Think about what you have to do to get into college, for example.",
      "  Extracurricular activities, check.  Even in college classes most of the work is as artificial as running laps.I'm not attacking the educational system for being this way. There will always be a certain amount of fakeness in the work you do when you're being taught something, and if you measure their performance it's inevitable that people will exploit the difference to the point where much of what you're measuring is artifacts of the fakeness.I confess I did it myself in college. I found that in a lot of classes there might only be 20 or 30 ideas that were the right shape to make good exam questions.  The way I studied for exams in these classes was not (except incidentally) to master the material taught in the class, but to make a list of potential exam questions and work out the answers in advance. When I walked into the final, the main thing I'd be feeling was curiosity about which of my questions would turn up on the exam.  It was like a game.It's not surprising that after being trained for their whole lives to play such games, young founders' first impulse on starting a startup is to try to figure out the tricks for winning at this new game. Since fundraising appears to be the measure of success for startups (another classic noob mistake), they always want to know what the tricks are for convincing investors.",
      "  We tell them the best way to convince investors is to make a startup that's actually doing well, meaning growing fast, and then simply tell investors so.  Then they want to know what the tricks are for growing fast.  And we have to tell them the best way to do that is simply to make something people want.So many of the conversations YC partners have with young founders begin with the founder asking \"How do we...\" and the partner replying \"Just...\"Why do the founders always make things so complicated?  The reason, I realized, is that they're looking for the trick.So this is the third counterintuitive thing to remember about startups: starting a startup is where gaming the system stops working.  Gaming the system may continue to work if you go to work for a big company. Depending on how broken the company is, you can succeed by sucking up to the right people, giving the impression of productivity, and so on.  [2] But that doesn't work with startups. There is no boss to trick, only users, and all users care about is whether your product does what they want. Startups are as impersonal as physics.  You have to make something people want, and you prosper only to the extent you do.The dangerous thing is,",
      " faking does work to some degree on investors. If you're super good at sounding like you know what you're talking about, you can fool investors for at least one and perhaps even two rounds of funding.  But it's not in your interest to.  The company is ultimately doomed.  All you're doing is wasting your own time riding it down.So stop looking for the trick. There are tricks in startups, as there are in any domain, but they are an order of magnitude less important than solving the real problem. A founder who knows nothing about fundraising but has made something users love will have an easier time raising money than one who knows every trick in the book but has a flat usage graph. And more importantly, the founder who has made something users love is the one who will go on to succeed after raising the money.Though in a sense it's bad news in that you're deprived of one of your most powerful weapons, I think it's exciting that gaming the system stops working when you start a startup.  It's exciting that there even exist parts of the world where you win by doing good work.  Imagine how depressing the world would be if it were all like school and big companies, where you either have to spend a lot of time on bullshit things or lose to people who do.",
      " [3] I would have been delighted if I'd realized in college that there were parts of the real world where gaming the system mattered less than others, and a few where it hardly mattered at all.  But there are, and this variation is one of the most important things to consider when you're thinking about your future.  How do you win in each type of work, and what would you like to win by doing? [4] All-ConsumingThat brings us to our fourth counterintuitive point: startups are all-consuming.  If you start a startup, it will take over your life to a degree you cannot imagine.  And if your startup succeeds, it will take over your life for a long time: for several years at the very least, maybe for a decade, maybe for the rest of your working life.  So there is a real opportunity cost here.Larry Page may seem to have an enviable life, but there are aspects of it that are unenviable.  Basically at 25 he started running as fast as he could and it must seem to him that he hasn't stopped to catch his breath since.  Every day new shit happens in the Google empire that only the CEO can deal with, and he, as CEO,",
      " has to deal with it.  If he goes on vacation for even a week, a whole week's backlog of shit accumulates.  And he has to bear this uncomplainingly, partly because as the company's daddy he can never show fear or weakness, and partly because billionaires get less than zero sympathy if they talk about having difficult lives.  Which has the strange side effect that the difficulty of being a successful startup founder is concealed from almost everyone except those who've done it.Y Combinator has now funded several companies that can be called big successes, and in every single case the founders say the same thing.  It never gets any easier.  The nature of the problems change. You're worrying about construction delays at your London office instead of the broken air conditioner in your studio apartment. But the total volume of worry never decreases; if anything it increases.Starting a successful startup is similar to having kids in that it's like a button you push that changes your life irrevocably. And while it's truly wonderful having kids, there are a lot of things that are easier to do before you have them than after.  Many of which will make you a better parent when you do have kids. And since you can delay pushing the button for a while,",
      " most people in rich countries do.Yet when it comes to startups, a lot of people seem to think they're supposed to start them while they're still in college.  Are you crazy?  And what are the universities thinking?  They go out of their way to ensure their students are well supplied with contraceptives, and yet they're setting up entrepreneurship programs and startup incubators left and right.To be fair, the universities have their hand forced here.  A lot of incoming students are interested in startups.  Universities are, at least de facto, expected to prepare them for their careers.  So students who want to start startups hope universities can teach them about startups.  And whether universities can do this or not, there's some pressure to claim they can, lest they lose applicants to other universities that do.Can universities teach students about startups?  Yes and no.  They can teach students about startups, but as I explained before, this is not what you need to know.  What you need to learn about are the needs of your own users, and you can't do that until you actually start the company. [5] So starting a startup is intrinsically something you can only really learn by doing it.  And it's impossible to do that in college,",
      " for the reason I just explained: startups take over your life.  You can't start a startup for real as a student, because if you start a startup for real you're not a student anymore. You may be nominally a student for a bit, but you won't even be that for long. [6]Given this dichotomy, which of the two paths should you take?  Be a real student and not start a startup, or start a real startup and not be a student?  I can answer that one for you. Do not start a startup in college.  How to start a startup is just a subset of a bigger problem you're trying to solve: how to have a good life. And though starting a startup can be part of a good life for a lot of ambitious people, age 20 is not the optimal time to do it. Starting a startup is like a brutally fast depth-first search.  Most people should still be searching breadth-first at 20.You can do things in your early 20s that you can't do as well before or after, like plunge deeply into projects on a whim and travel super cheaply with no sense of a deadline.  For unambitious people, this sort of thing is the dreaded \"failure to launch,\" but for the ambitious ones it can be an incomparably valuable sort of exploration.",
      " If you start a startup at 20 and you're sufficiently successful, you'll never get to do it. [7]Mark Zuckerberg will never get to bum around a foreign country.  He can do other things most people can't, like charter jets to fly him to foreign countries. But success has taken a lot of the serendipity out of his life. Facebook is running him as much as he's running Facebook. And while it can be very cool to be in the grip of a project you consider your life's work, there are advantages to serendipity too, especially early in life.  Among other things it gives you more options to choose your life's work from.There's not even a tradeoff here. You're not sacrificing anything if you forgo starting a startup at 20, because you're more likely to succeed if you wait.  In the unlikely case that you're 20 and one of your side projects takes off like Facebook did, you'll face a choice of running with it or not, and it may be reasonable to run with it.  But the usual way startups take off is for the founders to make them take off, and it's gratuitously stupid to do that at 20. TryShould you do it at any age?",
      "  I realize I've made startups sound pretty hard.  If I haven't, let me try again: starting a startup is really hard.  What if it's too hard?  How can you tell if you're up to this challenge?The answer is the fifth counterintuitive point: you can't tell. Your life so far may have given you some idea what your prospects might be if you tried to become a mathematician, or a professional football player.  But unless you've had a very strange life you haven't done much that was like being a startup founder. Starting a startup will change you a lot.  So what you're trying to estimate is not just what you are, but what you could grow into, and who can do that?For the past 9 years it was my job to predict whether people would have what it took to start successful startups.  It was easy to tell how smart they were, and most people reading this will be over that threshold.  The hard part was predicting how tough and ambitious they would become.  There may be no one who has more experience at trying to predict that, so I can tell you how much an expert can know about it, and the answer is: not much.  I learned to keep a completely open mind about which of the startups in each batch would turn out to be the stars.The founders sometimes think they know.",
      " Some arrive feeling sure they will ace Y Combinator just as they've aced every one of the (few, artificial, easy) tests they've faced in life so far.  Others arrive wondering how they got in, and hoping YC doesn't discover whatever mistake caused it to accept them.  But there is little correlation between founders' initial attitudes and how well their companies do.I've read that the same is true in the military \u2014 that the swaggering recruits are no more likely to turn out to be really tough than the quiet ones. And probably for the same reason: that the tests involved are so different from the ones in their previous lives.If you're absolutely terrified of starting a startup, you probably shouldn't do it.  But if you're merely unsure whether you're up to it, the only way to find out is to try.  Just not now. IdeasSo if you want to start a startup one day, what should you do in college?  There are only two things you need initially: an idea and cofounders.  And the m.o. for getting both is the same.  Which leads to our sixth and last counterintuitive point: that the way to get startup ideas is not to try to think of startup ideas.I've written a whole essay on this,",
      " so I won't repeat it all here.  But the short version is that if you make a conscious effort to think of startup ideas, the ideas you come up with will not merely be bad, but bad and plausible-sounding, meaning you'll waste a lot of time on them before realizing they're bad.The way to come up with good startup ideas is to take a step back. Instead of making a conscious effort to think of startup ideas, turn your mind into the type that startup ideas form in without any conscious effort.  In fact, so unconsciously that you don't even realize at first that they're startup ideas.This is not only possible, it's how Apple, Yahoo, Google, and Facebook all got started.  None of these companies were even meant to be companies at first.  They were all just side projects.  The best startups almost have to start as side projects, because great ideas tend to be such outliers that your conscious mind would reject them as ideas for companies.Ok, so how do you turn your mind into the type that startup ideas form in unconsciously?  (1) Learn a lot about things that matter, then (2) work on problems that interest you (3) with people you like and respect.  The third part,",
      " incidentally, is how you get cofounders at the same time as the idea.The first time I wrote that paragraph, instead of \"learn a lot about things that matter,\" I wrote \"become good at some technology.\" But that prescription, though sufficient, is too narrow.  What was special about Brian Chesky and Joe Gebbia was not that they were experts in technology.  They were good at design, and perhaps even more importantly, they were good at organizing groups and making projects happen.  So you don't have to work on technology per se, so long as you work on problems demanding enough to stretch you.What kind of problems are those?  That is very hard to answer in the general case.  History is full of examples of young people who were working on important problems that no one else at the time thought were important, and in particular that their parents didn't think were important.  On the other hand, history is even fuller of examples of parents who thought their kids were wasting their time and who were right.  So how do you know when you're working on real stuff? [8]I know how I know.  Real problems are interesting, and I am self-indulgent in the sense that I always want to work on interesting things,",
      " even if no one else cares about them (in fact, especially if no one else cares about them), and find it very hard to make myself work on boring things, even if they're supposed to be important.My life is full of case after case where I worked on something just because it seemed interesting, and it turned out later to be useful in some worldly way.  Y Combinator itself was something I only did because it seemed interesting. So I seem to have some sort of internal compass that helps me out.  But I don't know what other people have in their heads. Maybe if I think more about this I can come up with heuristics for recognizing genuinely interesting problems, but for the moment the best I can offer is the hopelessly question-begging advice that if you have a taste for genuinely interesting problems, indulging it energetically is the best way to prepare yourself for a startup. And indeed, probably also the best way to live. [9]But although I can't explain in the general case what counts as an interesting problem, I can tell you about a large subset of them. If you think of technology as something that's spreading like a sort of fractal stain, every moving point on the edge represents an interesting problem.",
      "  So one guaranteed way to turn your mind into the type that has good startup ideas is to get yourself to the leading edge of some technology \u2014 to cause yourself, as Paul Buchheit put it, to \"live in the future.\" When you reach that point, ideas that will seem to other people uncannily prescient will seem obvious to you.  You may not realize they're startup ideas, but you'll know they're something that ought to exist.For example, back at Harvard in the mid 90s a fellow grad student of my friends Robert and Trevor wrote his own voice over IP software. He didn't mean it to be a startup, and he never tried to turn it into one.  He just wanted to talk to his girlfriend in Taiwan without paying for long distance calls, and since he was an expert on networks it seemed obvious to him that the way to do it was turn the sound into packets and ship it over the Internet. He never did any more with his software than talk to his girlfriend, but this is exactly the way the best startups get started.So strangely enough the optimal thing to do in college if you want to be a successful startup founder is not some sort of new, vocational version of college focused on \"entrepreneurship.\" It's the classic version of college as education for its own sake.",
      " If you want to start a startup after college, what you should do in college is learn powerful things.  And if you have genuine intellectual curiosity, that's what you'll naturally tend to do if you just follow your own inclinations. [10]The component of entrepreneurship that really matters is domain expertise.  The way to become Larry Page was to become an expert on search. And the way to become an expert on search was to be driven by genuine curiosity, not some ulterior motive.At its best, starting a startup is merely an ulterior motive for curiosity.  And you'll do it best if you introduce the ulterior motive toward the end of the process.So here is the ultimate advice for young would-be startup founders, boiled down to two words: just learn. Notes[1] Some founders listen more than others, and this tends to be a predictor of success. One of the things I remember about the Airbnbs during YC is how intently they listened.[2] In fact, this is one of the reasons startups are possible.  If big companies weren't plagued by internal inefficiencies, they'd be proportionately more effective, leaving less room for startups.[3] In a startup you have to spend a lot of time on schleps,",
      " but this sort of work is merely unglamorous, not bogus.[4] What should you do if your true calling is gaming the system? Management consulting.[5] The company may not be incorporated, but if you start to get significant numbers of users, you've started it, whether you realize it yet or not.[6] It shouldn't be that surprising that colleges can't teach students how to be good startup founders, because they can't teach them how to be good employees either.The way universities \"teach\" students how to be employees is to hand off the task to companies via internship programs.  But you couldn't do the equivalent thing for startups, because by definition if the students did well they would never come back.[7] Charles Darwin was 22 when he received an invitation to travel aboard the HMS Beagle as a naturalist.  It was only because he was otherwise unoccupied, to a degree that alarmed his family, that he could accept it. And yet if he hadn't we probably would not know his name.[8] Parents can sometimes be especially conservative in this department.  There are some whose definition of important problems includes only those on the critical path to med school.[9] I did manage to think of a heuristic for detecting whether you have a taste for interesting ideas:",
      " whether you find known boring ideas intolerable.  Could you endure studying literary theory, or working in middle management at a large company?[10] In fact, if your goal is to start a startup, you can stick even more closely to the ideal of a liberal education than past generations have. Back when students focused mainly on getting a job after college, they thought at least a little about how the courses they took might look to an employer.  And perhaps even worse, they might shy away from taking a difficult class lest they get a low grade, which would harm their all-important GPA.  Good news: users don't care what your GPA was.  And I've never heard of investors caring either.  Y Combinator certainly never asks what classes you took in college or what grades you got in them. Thanks to Sam Altman, Paul Buchheit, John Collison, Patrick Collison, Jessica Livingston, Robert Morris, Geoff Ralston, and Fred Wilson for reading drafts of this.September 2017The most valuable insights are both general and surprising.  F\u00a0=\u00a0ma for example. But general and surprising is a hard combination to achieve. That territory tends to be picked clean, precisely because those insights are so valuable.Ordinarily,",
      " the best that people can do is one without the other: either surprising without being general (e.g. gossip), or general without being surprising (e.g. platitudes).Where things get interesting is the moderately valuable insights.  You get those from small additions of whichever quality was missing.  The more common case is a small addition of generality: a piece of gossip that's more than just gossip, because it teaches something interesting about the world. But another less common approach is to focus on the most general ideas and see if you can find something new to say about them. Because these start out so general, you only need a small delta of novelty to produce a useful insight.A small delta of novelty is all you'll be able to get most of the time. Which means if you take this route, your ideas will seem a lot like ones that already exist. Sometimes you'll find you've merely rediscovered an idea that did already exist.  But don't be discouraged.  Remember the huge multiplier that kicks in when you do manage to think of something even a little new.Corollary: the more general the ideas you're talking about, the less you should worry about repeating yourself.  If you write enough, it's inevitable you will.  Your brain is much the same from year to year and so are the stimuli that hit it.",
      " I feel slightly bad when I find I've said something close to what I've said before, as if I were plagiarizing myself. But rationally one shouldn't.  You won't say something exactly the same way the second time, and that variation increases the chance you'll get that tiny but critical delta of novelty.And of course, ideas beget ideas.  (That sounds  familiar.) An idea with a small amount of novelty could lead to one with more. But only if you keep going. So it's doubly important not to let yourself be discouraged by people who say there's not much new about something you've discovered. \"Not much new\" is a real achievement when you're talking about the most general ideas. It's not true that there's nothing new under the sun.  There are some domains where there's almost nothing new.  But there's a big difference between nothing and almost nothing, when it's multiplied by the area under the sun. Thanks to Sam Altman, Patrick Collison, and Jessica Livingston for reading drafts of this.December 2001 (rev. May 2002)  (This article came about in response to some questions on the LL1 mailing list.  It is now incorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s,",
      " it was a radical departure from existing languages, the most important of which was Fortran.Lisp embodied nine new ideas: 1. Conditionals.  A conditional is an if-then-else construct.  We take these for granted now.  They were  invented by McCarthy in the course of developing Lisp.  (Fortran at that time only had a conditional goto, closely based on the branch instruction in the  underlying hardware.)  McCarthy, who was on the Algol committee, got conditionals into Algol, whence they spread to most other languages.2. A function type. In Lisp, functions are first class  objects-- they're a data type just like integers, strings, etc, and have a literal representation, can be stored in variables, can be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept before Lisp of course, but Lisp was the first programming language to support it.  (It's arguably implicit in making functions first class objects.)4. A new concept of variables.  In Lisp, all variables are effectively pointers. Values are what have types, not variables, and assigning or binding variables means copying pointers, not what they point to.5. Garbage-collection.6.",
      " Programs composed of expressions. Lisp programs are  trees of expressions, each of which returns a value.   (In some Lisps expressions can return multiple values.)  This is in contrast to Fortran and most succeeding languages, which distinguish between expressions and statements.It was natural to have this distinction in Fortran because (not surprisingly in a language where the input format was punched cards) the language was line-oriented.  You could not nest statements.  And so while you needed expressions for math to work, there was no point in making anything else return a value, because there could not be anything waiting for it.This limitation went away with the arrival of block-structured languages, but by then it was too late. The distinction between expressions and statements was entrenched.  It spread from  Fortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can compose expressions however you want.  You can say either (using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that you can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.",
      "   There is no real distinction between read-time, compile-time, and runtime. You can compile or run code while reading, read or run code while compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax; running code at compile-time is the basis of macros; compiling at runtime is the basis of Lisp's use as an extension language in programs like Emacs; and reading at runtime enables programs to communicate using s-expressions, an idea recently reinvented as XML. When Lisp was first invented, all these ideas were far removed from ordinary programming practice, which was dictated largely by the hardware available in the late 1950s.Over time, the default language, embodied in a succession of popular languages, has gradually evolved toward Lisp.  1-5 are now widespread. 6 is starting to appear in the mainstream. Python has a form of 7, though there doesn't seem to be any syntax for it.   8, which (with 9) is what makes Lisp macros possible, is so far still unique to Lisp, perhaps because (a) it requires those parens, or something  just as bad, and (b) if you add that final increment of power,  you can no  longer claim to have invented a new language,",
      " but only to have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's strange to describe Lisp in terms of its variation from the random expedients other languages adopted.  That was not, probably, how McCarthy thought of it.  Lisp wasn't designed to fix the mistakes in Fortran; it came about more as the byproduct of an attempt to axiomatize computation.  Want to start a startup?  Get funded by Y Combinator.     November 2005Does \"Web 2.0\" mean anything?  Till recently I thought it didn't, but the truth turns out to be more complicated.  Originally, yes, it was meaningless.  Now it seems to have acquired a meaning.  And yet those who dislike the term are probably right, because if it means what I think it does, we don't need it.I first heard the phrase \"Web 2.0\" in the name of the Web 2.0 conference in 2004.  At the time it was supposed to mean using \"the web as a platform,\" which I took to refer to web-based applications. [1]So I was surprised at a conference this summer when Tim O'Reilly led a session intended to figure out a definition of \"Web 2.",
      "0.\" Didn't it already mean using the web as a platform?  And if it didn't already mean something, why did we need the phrase at all?OriginsTim says the phrase \"Web 2.0\" first arose in \"a brainstorming session between O'Reilly and Medialive International.\" What is Medialive International? \"Producers of technology tradeshows and conferences,\" according to their site.  So presumably that's what this brainstorming session was about.  O'Reilly wanted to organize a conference about the web, and they were wondering what to call it.I don't think there was any deliberate plan to suggest there was a new version of the web.  They just wanted to make the point that the web mattered again.  It was a kind of semantic deficit spending: they knew new things were coming, and the \"2.0\" referred to whatever those might turn out to be.And they were right.  New things were coming.  But the new version number led to some awkwardness in the short term.  In the process of developing the pitch for the first conference, someone must have decided they'd better take a stab at explaining what that \"2.0\" referred to.  Whatever it meant, \"the web as a platform\"",
      " was at least not too constricting.The story about \"Web 2.0\" meaning the web as a platform didn't live much past the first conference.  By the second conference, what \"Web 2.0\" seemed to mean was something about democracy.  At least, it did when people wrote about it online.  The conference itself didn't seem very grassroots.  It cost $2800, so the only people who could afford to go were VCs and people from big companies.And yet, oddly enough, Ryan Singel's article about the conference in Wired News spoke of \"throngs of geeks.\"  When a friend of mine asked Ryan about this, it was news to him.  He said he'd originally written something like \"throngs of VCs and biz dev guys\" but had later shortened it just to \"throngs,\" and that this must have in turn been expanded by the editors into \"throngs of geeks.\"  After all, a Web 2.0 conference would presumably be full of geeks, right?Well, no.  There were about 7.  Even Tim O'Reilly was wearing a    suit, a sight so alien I couldn't parse it at first.  I saw him walk by and said to one of the O'Reilly people \"that guy looks just like Tim.\"\"Oh,",
      " that's Tim.  He bought a suit.\" I ran after him, and sure enough, it was.  He explained that he'd just bought it in Thailand.The 2005 Web 2.0 conference reminded me of Internet trade shows during the Bubble, full of prowling VCs looking for the next hot startup.  There was that same odd atmosphere created by a large   number of people determined not to miss out.  Miss out on what? They didn't know.  Whatever was going to happen\u2014whatever Web 2.0 turned out to be.I wouldn't quite call it \"Bubble 2.0\" just because VCs are eager to invest again.  The Internet is a genuinely big deal.  The bust was as much an overreaction as the boom.  It's to be expected that once we started to pull out of the bust, there would be a lot of growth in this area, just as there was in the industries that spiked the sharpest before the Depression.The reason this won't turn into a second Bubble is that the IPO market is gone.  Venture investors are driven by exit strategies.  The reason they were funding all   those laughable startups during the late 90s was that they hoped to sell them to gullible retail investors;",
      " they hoped to be laughing all the way to the bank.  Now that route is closed.  Now the default exit strategy is to get bought, and acquirers are less prone to irrational exuberance than IPO investors.  The closest you'll get  to Bubble valuations is Rupert Murdoch paying $580 million for    Myspace.  That's only off by a factor of 10 or so.1. AjaxDoes \"Web 2.0\" mean anything more than the name of a conference yet?  I don't like to admit it, but it's starting to.  When people say \"Web 2.0\" now, I have some idea what they mean.  And the fact that I both despise the phrase and understand it is the surest proof that it has started to mean something.One ingredient of its meaning is certainly Ajax, which I can still only just bear to use without scare quotes.  Basically, what \"Ajax\" means is \"Javascript now works.\"  And that in turn means that web-based applications can now be made to work much more like desktop ones.As you read this, a whole new generation of software is being written to take advantage of Ajax.  There hasn't been such a wave of new applications since microcomputers first appeared.",
      "  Even Microsoft sees it, but it's too late for them to do anything more than leak \"internal\"   documents designed to give the impression they're on top of this new trend.In fact the new generation of software is being written way too fast for Microsoft even to channel it, let alone write their own in house.  Their only hope now is to buy all the best Ajax startups before Google does.  And even that's going to be hard, because Google has as big a head start in buying microstartups as it did in search a few years ago.  After all, Google Maps, the canonical Ajax application, was the result of a startup they bought.So ironically the original description of the Web 2.0 conference turned out to be partially right: web-based applications are a big component of Web 2.0.  But I'm convinced they got this right by  accident.  The Ajax boom didn't start till early 2005, when Google Maps appeared and the term \"Ajax\" was coined.2. DemocracyThe second big element of Web 2.0 is democracy.  We now have several examples to prove that amateurs can    surpass professionals, when they have the right kind of system to  channel their efforts.  Wikipedia may be the most famous.",
      "  Experts have given Wikipedia middling reviews, but they miss the critical point: it's good enough.  And    it's free, which means people actually read it.  On the web, articles you have to pay for might as well not exist.  Even if you were     willing to pay to read them yourself, you can't link to them.     They're not part of the conversation.Another place democracy seems to win is in deciding what counts as news.  I never look at any news site now except Reddit. [2]  I know if something major happens, or someone writes a particularly interesting article, it    will show up there.  Why bother checking the front page of any specific paper or magazine?  Reddit's like an RSS feed for the whole web, with a filter for quality.  Similar sites include Digg, a technology news site that's rapidly approaching Slashdot in popularity, and del.icio.us, the collaborative bookmarking network that set off the \"tagging\" movement.  And whereas Wikipedia's main appeal is that it's good enough and free, these sites suggest that voters do a significantly better job than human editors.The most dramatic example of Web 2.0 democracy is not in the selection of ideas, but their production.",
      "   I've noticed for a while that the stuff I read on individual people's sites is as good as or better than the stuff I read in newspapers and magazines.  And now I have independent evidence: the top links on Reddit are generally links to individual people's sites rather   than to magazine articles or news stories.My experience of writing for magazines suggests an explanation.  Editors.  They control the topics you can write about, and they can generally rewrite whatever you produce.  The result is to damp extremes.  Editing yields 95th percentile writing\u201495% of articles are improved by it, but 5% are dragged down.  5% of the time you get \"throngs of geeks.\"On the web, people can publish whatever they want.  Nearly all of it falls short of the editor-damped writing in print publications. But the pool of writers is very, very large.  If it's large enough, the lack of damping means the best writing online should surpass   the best in print. [3]   And now that the web has evolved mechanisms for selecting good stuff, the web wins net.  Selection beats damping, for the same reason market economies beat centrally planned ones.Even the startups are different this time around.  They are to the   startups of the Bubble what bloggers are to the print media.",
      "  During the Bubble, a startup meant a company headed by an MBA that was    blowing through several million dollars of VC money to \"get big fast\" in the most literal sense.  Now it means a smaller, younger, more technical group that just       decided to make something great.  They'll decide later if they want   to raise VC-scale funding, and if they take it, they'll take it on their terms.3. Don't Maltreat UsersI think everyone would agree that democracy and Ajax are elements of \"Web 2.0.\"  I also see a third: not to maltreat users.  During the Bubble a lot of popular sites were quite high-handed with users. And not just in obvious ways, like making them register, or subjecting them to annoying ads.  The very design of the average site in the    late 90s was an abuse.  Many of the most popular sites were loaded with obtrusive branding that made them slow to load and sent the user the message: this is our site, not yours.  (There's a physical analog in the Intel and Microsoft stickers that come on some laptops.)I think the root of the problem was that sites felt they were giving something away for free, and till recently a company giving anything away for free could be pretty high-handed about it.",
      "  Sometimes it reached the point of economic sadism: site owners assumed that the more pain they caused the user, the more benefit it must be to them.   The most dramatic remnant of this model may be at salon.com, where    you can read the beginning of a story, but to get the rest you have sit through a movie.At Y Combinator we advise all the startups we fund never to lord it over users.  Never make users register, unless you need to in order to store something for them.  If you do make users register,    never make them wait for a confirmation link in an email; in fact, don't even ask for their email address unless you need it for some reason.  Don't ask them any unnecessary questions.  Never send them email unless they explicitly ask for it.  Never frame pages you link to, or open them in new windows.  If you have a free version  and a pay version, don't make the free version too restricted.  And if you find yourself asking \"should we allow users to do x?\" just  answer \"yes\" whenever you're unsure.  Err on the side of generosity.In How to Start a Startup I advised startups never to let anyone fly under them, meaning never to let any other company offer a cheaper,",
      " easier solution.  Another way to fly low  is to give users more power.  Let users do what they want.  If you  don't and a competitor does, you're in trouble.iTunes is Web 2.0ish in this sense.  Finally you can buy individual songs instead of having to buy whole albums.  The recording industry hated the idea and resisted it as long as possible.  But it was obvious what users wanted, so Apple flew under the labels. [4] Though really it might be better to describe iTunes as Web 1.5.      Web 2.0 applied to music would probably mean individual bands giving away DRMless songs for free.The ultimate way to be nice to users is to give them something for free that competitors charge for.  During the 90s a lot of people    probably thought we'd have some working system for micropayments      by now.  In fact things have gone in the other direction.  The most    successful sites are the ones that figure out new ways to give stuff away for free.  Craigslist has largely destroyed the classified ad sites of the 90s, and OkCupid looks likely to do the same to the previous generation of dating sites.Serving web pages is very,",
      " very cheap.  If you can make even a    fraction of a cent per page view, you can make a profit.  And technology for targeting ads continues to improve.  I wouldn't be surprised if ten years from now eBay had been supplanted by an       ad-supported freeBay (or, more likely, gBay).Odd as it might sound, we tell startups that they should try to make as little money as possible.  If you can figure out a way to turn a billion dollar industry into a fifty million dollar industry, so much the better, if all fifty million go to you.  Though indeed, making things cheaper often turns out to generate more money in the end, just as automating things often turns out to generate more jobs.The ultimate target is Microsoft.  What a bang that balloon is going to make when someone pops it by offering a free web-based alternative  to MS Office. [5] Who will?  Google?  They seem to be taking their time.  I suspect the pin will be wielded by a couple of 20 year old hackers who are too naive to be intimidated by the idea.  (How hard can it be?)The Common ThreadAjax, democracy, and not dissing users.  What do they all have in   common?",
      "  I didn't realize they had anything in common till recently, which is one of the reasons I disliked the term \"Web 2.0\" so much. It seemed that it was being used as a label for whatever happened to be new\u2014that it didn't predict anything.But there is a common thread.  Web 2.0 means using the web the way it's meant to be used.  The \"trends\" we're seeing now are simply the inherent nature of the web emerging from under the broken models that got imposed on it during the Bubble.I realized this when I read an  interview with Joe Kraus, the co-founder of Excite. [6]    Excite really never got the business model right at all.  We fell    into the classic problem of how when a new medium comes out it   adopts the practices, the content, the business models of the old   medium\u2014which fails, and then the more appropriate models get   figured out.  It may have seemed as if not much was happening during the years after the Bubble burst.  But in retrospect, something was happening: the web was finding its natural angle of repose.  The democracy  component, for example\u2014that's not an innovation, in the sense of something someone made happen.",
      "  That's what the web naturally tends to produce.Ditto for the idea of delivering desktop-like applications over the web.  That idea is almost as old as the web.  But the first time     around it was co-opted by Sun, and we got Java applets.  Java has since been remade into a generic replacement for C++, but in 1996 the story about Java was that it represented a new model of software. Instead of desktop applications, you'd run Java \"applets\" delivered from a server.This plan collapsed under its own weight. Microsoft helped kill it, but it would have died anyway.  There was no uptake among hackers. When you find PR firms promoting something as the next development platform, you can be sure it's not.  If it were, you wouldn't need PR firms to tell you, because    hackers would already be writing stuff on top of it, the way sites     like Busmonster used Google Maps as a platform before Google even meant it to be one.The proof that Ajax is the next hot platform is that thousands of   hackers have spontaneously started building things on top of it.  Mikey likes it.There's another thing all three components of Web 2.0 have in common. Here's a clue.",
      "  Suppose you approached investors with the following idea for a Web 2.0 startup:    Sites like del.icio.us and flickr allow users to \"tag\" content   with descriptive tokens.  But there is also huge source of   implicit tags that they ignore: the text within web links.   Moreover, these links represent a social network connecting the      individuals and organizations who created the pages, and by using   graph theory we can compute from this network an estimate of the   reputation of each member.  We plan to mine the web for these    implicit tags, and use them together with the reputation hierarchy   they embody to enhance web searches.  How long do you think it would take them on average to realize that it was a description of Google?Google was a pioneer in all three components of Web 2.0: their core business sounds crushingly hip when described in Web 2.0 terms,  \"Don't maltreat users\" is a subset of \"Don't be evil,\" and of course Google set off the whole Ajax boom with Google Maps.Web 2.0 means using the web as it was meant to be used, and Google does.  That's their secret.    They're sailing with the wind, instead of sitting   becalmed praying for a business model,",
      " like the print media, or    trying to tack upwind by suing their customers, like Microsoft and  the record labels. [7]Google doesn't try to force things to happen their way.  They try    to figure out what's going to happen, and arrange to be standing  there when it does.  That's the way to approach technology\u2014and  as business includes an ever larger technological component, the right way to do business.The fact that Google is a \"Web 2.0\" company shows that, while meaningful, the term is also rather bogus.  It's like the word \"allopathic.\"  It just means doing things right, and it's a bad    sign when you have a special word for that. Notes[1] From the conference site, June 2004: \"While the first wave of the Web was closely   tied to the browser, the second wave extends applications across     the web and enables a new generation of services and business opportunities.\"  To the extent this means anything, it seems to be about  web-based applications.[2] Disclosure: Reddit was funded by  Y Combinator.  But although I started using it out of loyalty to the home team, I've become a genuine addict.  While we're at it,",
      " I'm also an investor in!MSFT, having sold all my shares earlier this year.[3] I'm not against editing. I spend more time editing than writing, and I have a group of picky friends who proofread almost everything I write.  What I dislike is editing done after the fact   by someone else.[4] Obvious is an understatement.  Users had been climbing in through   the window for years before Apple finally moved the door.[5] Hint: the way to create a web-based alternative to Office may not be to write every component yourself, but to establish a protocol for web-based apps to share a virtual home directory spread across multiple servers.  Or it may be to write it all yourself.[6] In Jessica Livingston's Founders at Work.[7] Microsoft didn't sue their customers directly, but they seem  to have done all they could to help SCO sue them.Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, Peter Norvig, Aaron Swartz, and Jeff Weiner for reading drafts of this, and to the guys at O'Reilly and Adaptive Path for answering my questions.November 2005In the next few years, venture capital funds will find themselves squeezed from four directions.  They're already stuck with a seller's market,",
      " because of the huge amounts they raised at the end of the Bubble and still haven't invested.  This by itself is not the end of the world.  In fact, it's just a more extreme version of the norm in the VC business: too much money chasing too few deals.Unfortunately, those few deals now want less and less money, because it's getting so cheap to start a startup.  The four causes: open source, which makes software free; Moore's law, which makes hardware geometrically closer to free; the Web, which makes promotion free if you're good; and better languages, which make development a lot cheaper.When we started our startup in 1995, the first three were our biggest expenses.  We had to pay $5000 for the Netscape Commerce Server, the only software that then supported secure http connections.  We paid $3000 for a server with a 90 MHz processor and 32 meg of memory.  And we paid a PR firm about $30,000 to promote our launch.Now you could get all three for nothing.  You can get the software for free; people throw away computers more powerful than our first server; and if you make something good you can generate ten times as much traffic by word of mouth online than our first PR firm got through the print media.And of course another big change for the average startup is that programming languages have improved-- or rather,",
      " the median language has.  At most startups ten years ago, software development meant ten programmers writing code in C++.  Now the same work might be done by one or two using Python or Ruby.During the Bubble, a lot of people predicted that startups would outsource their development to India.  I think a better model for the future is David Heinemeier Hansson, who outsourced his development to a more powerful language instead.  A lot of well-known applications are now, like BaseCamp, written by just one programmer.  And one guy is more than 10x cheaper than ten, because (a) he won't waste any time in meetings, and (b) since he's probably a founder, he can pay himself nothing.Because starting a startup is so cheap, venture capitalists now often want to give startups more money than the startups want to take.  VCs like to invest several million at a time.  But as one VC told me after a startup he funded would only take about half a million, \"I don't know what we're going to do.  Maybe we'll just have to give some of it back.\" Meaning give some of the fund back to the institutional investors who supplied it, because it wasn't going to be possible to invest it all.Into this already bad situation comes the third problem:",
      " Sarbanes-Oxley. Sarbanes-Oxley is a law, passed after the Bubble, that drastically increases the regulatory burden on public companies. And in addition to the cost of compliance, which is at least two million dollars a year, the law introduces frightening legal exposure for corporate officers.  An experienced CFO I know said flatly: \"I would not want to be CFO of a public company now.\"You might think that responsible corporate governance is an area where you can't go too far.  But you can go too far in any law, and this remark convinced me that Sarbanes-Oxley must have.  This CFO is both the smartest and the most upstanding money guy I know.  If Sarbanes-Oxley deters people like him from being CFOs of public   companies, that's proof enough that it's broken.Largely because of Sarbanes-Oxley, few startups go public now.  For all practical purposes, succeeding now equals getting bought.  Which means VCs are now in the business of finding promising little 2-3 man startups and pumping them up into companies that cost $100 million to acquire.   They didn't mean to be in this business; it's just what their business has evolved into.Hence the fourth problem:",
      " the acquirers have begun to realize they can buy wholesale.  Why should they wait for VCs to make the startups they want more expensive?  Most of what the VCs add, acquirers don't want anyway.  The acquirers already have brand recognition and HR departments.  What they really want is the software and the developers, and that's what the startup is in the early phase: concentrated software and developers.Google, typically, seems to have been the first to figure this out. \"Bring us your startups early,\" said Google's speaker at the Startup School.  They're quite explicit about it: they like to acquire startups at just the point where they would do a Series A round.  (The Series A round is the first round of real VC funding; it usually happens in the first year.) It is a brilliant strategy, and one that other big technology companies will no doubt try to duplicate.  Unless they want to have  still more of their lunch eaten by Google.Of course, Google has an advantage in buying startups: a lot of the people there are rich, or expect to be when their options vest. Ordinary employees find it very hard to recommend an acquisition; it's just too annoying to see a bunch of twenty year olds get rich when you're still working for salary.",
      "  Even if it's the right thing    for your company to do.The Solution(s)Bad as things look now, there is a way for VCs to save themselves. They need to do two things, one of which won't surprise them, and   another that will seem an anathema.Let's start with the obvious one: lobby to get Sarbanes-Oxley   loosened.  This law was created to prevent future Enrons, not to destroy the IPO market.  Since the IPO market was practically dead when it passed, few saw what bad effects it would have.  But now  that technology has recovered from the last bust, we can see clearly what a bottleneck Sarbanes-Oxley has become.Startups are fragile plants\u2014seedlings, in fact.  These seedlings are worth protecting, because they grow into the trees of the economy.  Much of the economy's growth is their growth.  I think most politicians realize that.  But they don't realize just how    fragile startups are, and how easily they can become collateral damage of laws meant to fix some other problem.Still more dangerously, when you destroy startups, they make very little noise.  If you step on the toes of the coal industry, you'll hear about it.",
      "  But if you inadvertantly squash the startup industry, all that happens is that the founders of the next Google stay in  grad school instead of starting a company.My second suggestion will seem shocking to VCs: let founders cash   out partially in the Series A round.  At the moment, when VCs invest in a startup, all the stock they get is newly issued and all the  money goes to the company.  They could buy some stock directly from the founders as well.Most VCs have an almost religious rule against doing this.  They don't want founders to get a penny till the company is sold or goes public.  VCs are obsessed with control, and they worry that they'll have less leverage over the founders if the founders have any money.This is a dumb plan.  In fact, letting the founders sell a little stock early would generally be better for the company, because it would cause the founders' attitudes toward risk to be aligned with the VCs'.  As things currently work, their attitudes toward risk tend to be diametrically opposed: the founders, who have nothing, would prefer a 100% chance of $1 million to a 20% chance of $10 million, while the VCs can afford to be \"rational\"",
      " and prefer the latter.Whatever they say, the reason founders are selling their companies early instead of doing Series A rounds is that they get paid up front.  That first million is just worth so much more than the subsequent ones.  If founders could sell a little stock early, they'd be happy to take VC money and bet the rest on a bigger outcome.So why not let the founders have that first million, or at least half million?  The VCs would get same number of shares for the    money.  So what if some of the money would go to the   founders instead of the company?Some VCs will say this is unthinkable\u2014that they want all their money to be put to work growing the company.  But the fact is, the huge size of current VC investments is dictated by the structure of VC funds, not the needs of startups.  Often as not these large   investments go to work destroying the company rather than growing it.The angel investors who funded our startup let the founders sell some stock directly to them, and it was a good deal for everyone.  The angels made a huge return on that investment, so they're happy. And for us founders it blunted the terrifying all-or-nothingness of a startup, which in its raw form is more a distraction than a motivator.If VCs are frightened at the idea of letting founders partially cash out,",
      " let me tell them something still more frightening: you are now competing directly with Google. Thanks to Trevor Blackwell, Sarah Harlin, Jessica Livingston, and Robert Morris for reading drafts of this.November 2022Since I was about 9 I've been puzzled by the apparent contradiction between being made of matter that behaves in a predictable way, and the feeling that I could choose to do whatever I wanted. At the time I had a self-interested motive for exploring the question. At that age (like most succeeding ages) I was always in trouble with the authorities, and it seemed to me that there might possibly be some way to get out of trouble by arguing that I wasn't responsible for my actions. I gradually lost hope of that, but the puzzle remained: How do you reconcile being a machine made of matter with the feeling that you're free to choose what you do? [1]The best way to explain the answer may be to start with a slightly wrong version, and then fix it. The wrong version is: You can do what you want, but you can't want what you want. Yes, you can control what you do, but you'll do what you want, and you can't control that.The reason this is mistaken is that people do sometimes change what they want.",
      " People who don't want to want something \u2014 drug addicts, for example \u2014 can sometimes make themselves stop wanting it. And people who want to want something \u2014 who want to like classical music, or broccoli \u2014 sometimes succeed.So we modify our initial statement: You can do what you want, but you can't want to want what you want.That's still not quite true. It's possible to change what you want to want. I can imagine someone saying \"I decided to stop wanting to like classical music.\" But we're getting closer to the truth. It's rare for people to change what they want to want, and the more \"want to\"s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more \"want to\"s in much the same way we can get arbitrarily close to 1 by adding more 9s to a string of 9s following a decimal point. In practice three or four \"want to\"s must surely be enough. It's hard even to envision what it would mean to change what you want to want to want to want, let alone actually do it.So one way to express the correct answer is to use a regular expression. You can do what you want, but there's some statement of the form \"you can't (want to)*",
      " want what you want\" that's true. Ultimately you get back to a want that you don't control. [2] Notes[1] I didn't know when I was 9 that matter might behave randomly, but I don't think it affects the problem much. Randomness destroys the ghost in the machine as effectively as determinism.[2] If you don't like using an expression, you can make the same point using higher-order desires: There is some n such that you don't control your nth-order desires. Thanks to Trevor Blackwell, Jessica Livingston, Robert Morris, and Michael Nielsen for reading drafts of this.January 2015Corporate Development, aka corp dev, is the group within companies that buys other companies. If you're talking to someone from corp dev, that's why, whether you realize it yet or not.It's usually a mistake to talk to corp dev unless (a) you want to sell your company right now and (b) you're sufficiently likely to get an offer at an acceptable price.  In practice that means startups should only talk to corp dev when they're either doing really well or really badly.  If you're doing really badly, meaning the company is about to die, you may as well talk to them, because you have nothing to lose.",
      " And if you're doing really well, you can safely talk to them, because you both know the price will have to be high, and if they show the slightest sign of wasting your time, you'll be confident enough to tell them to get lost.The danger is to companies in the middle.  Particularly to young companies that are growing fast, but haven't been doing it for long enough to have grown big yet.  It's usually a mistake for a promising company less than a year old even to talk to corp dev.But it's a mistake founders constantly make.  When someone from corp dev wants to meet, the founders tell themselves they should at least find out what they want.  Besides, they don't want to offend Big Company by refusing to meet.Well, I'll tell you what they want.  They want to talk about buying you.  That's what the title \"corp dev\" means.   So before agreeing to meet with someone from corp dev, ask yourselves, \"Do we want to sell the company right now?\"  And if the answer is no, tell them \"Sorry, but we're focusing on growing the company.\"  They won't be offended.  And certainly the founders of Big Company won't be offended. If anything they'll think more highly of you.",
      "  You'll remind them of themselves.  They didn't sell either; that's why they're in a position now to buy other companies. [1]Most founders who get contacted by corp dev already know what it means.  And yet even when they know what corp dev does and know they don't want to sell, they take the meeting.  Why do they do it? The same mix of denial and wishful thinking that underlies most mistakes founders make. It's flattering to talk to someone who wants to buy you.  And who knows, maybe their offer will be surprisingly high.  You should at least see what it is, right?No.  If they were going to send you an offer immediately by email, sure, you might as well open it.  But that is not how conversations with corp dev work.  If you get an offer at all, it will be at the end of a long and unbelievably distracting process.  And if the offer is surprising, it will be surprisingly low.Distractions are the thing you can least afford in a startup.  And conversations with corp dev are the worst sort of distraction, because as well as consuming your attention they undermine your morale.  One of the tricks to surviving a grueling process is not to stop and think how tired you are.",
      "  Instead you get into a sort of flow.  [2] Imagine what it would do to you if at mile 20 of a marathon, someone ran up beside you and said \"You must feel really tired.  Would you like to stop and take a rest?\"  Conversations with corp dev are like that but worse, because the suggestion of stopping gets combined in your mind with the imaginary high price you think they'll offer.And then you're really in trouble.  If they can, corp dev people like to turn the tables on you. They like to get you to the point where you're trying to convince them to buy instead of them trying to convince you to sell.  And surprisingly often they succeed.This is a very slippery slope, greased with some of the most powerful forces that can work on founders' minds, and attended by an experienced professional whose full time job is to push you down it.Their tactics in pushing you down that slope are usually fairly brutal. Corp dev people's whole job is to buy companies, and they don't even get to choose which.  The only way their performance is measured is by how cheaply they can buy you, and the more ambitious ones will stop at nothing to achieve that. For example, they'll almost always start with a lowball offer,",
      " just to see if you'll take it. Even if you don't, a low initial offer will demoralize you and make you easier to manipulate.And that is the most innocent of their tactics. Just wait till you've agreed on a price and think you have a done deal, and then they come back and say their boss has vetoed the deal and won't do it for more than half the agreed upon price. Happens all the time. If you think investors can behave badly, it's nothing compared to what corp dev people can do.  Even corp dev people at companies that are otherwise benevolent.I remember once complaining to a friend at Google about some nasty trick their corp dev people had pulled on a YC startup.\"What happened to Don't be Evil?\" I asked.\"I don't think corp dev got the memo,\" he replied.The tactics you encounter in M&A conversations can be like nothing you've experienced in the otherwise comparatively  upstanding world of Silicon Valley.  It's as if a chunk of genetic material from the old-fashioned robber baron business world got incorporated into the startup world. [3]The simplest way to protect yourself is to use the trick that John D. Rockefeller, whose grandfather was an alcoholic, used to protect himself from becoming one.",
      "  He once told a Sunday school class    Boys, do you know why I never became a drunkard?  Because I never   took the first drink.  Do you want to sell your company right now?  Not eventually, right now.  If not, just don't take the first meeting.  They won't be offended.  And you in turn will be guaranteed to be spared one of the worst experiences that can happen to a startup.If you do want to sell, there's another set of  techniques  for doing that.  But the biggest mistake founders make in dealing with corp dev is not doing a bad job of talking to them when they're ready to, but talking to them before they are.  So if you remember only the title of this essay, you already know most of what you need to know about M&A in the first year.Notes[1] I'm not saying you should never sell.  I'm saying you should be clear in your own mind about whether you want to sell or not, and not be led by manipulation or wishful thinking into trying to sell earlier than you otherwise would have.[2] In a startup, as in most competitive sports, the task at hand almost does this for you; you're too busy to feel tired.",
      "  But when you lose that protection, e.g. at the final whistle, the fatigue hits you like a wave.  To talk to corp dev is to let yourself feel it mid-game.[3] To be fair, the apparent misdeeds of corp dev people are magnified by the fact that they function as the face of a large organization that often doesn't know its own mind.  Acquirers can be surprisingly indecisive about acquisitions, and their flakiness is indistinguishable from dishonesty by the time it filters down to you.Thanks to Marc Andreessen, Jessica Livingston, Geoff Ralston, and Qasar Younis for reading drafts of this.  Want to start a startup?  Get funded by Y Combinator.     October 2010  (I wrote this for Forbes, who asked me to write something about the qualities we look for in founders.  In print they had to cut the last item because they didn't have room.)1. DeterminationThis has turned out to be the most important quality in startup founders.  We thought when we started Y Combinator that the most important quality would be intelligence.  That's the myth in the Valley. And certainly you don't want founders to be stupid.  But as long as you're over a certain threshold of intelligence,",
      " what matters most is determination.  You're going to hit a lot of"
    ]
  }
]